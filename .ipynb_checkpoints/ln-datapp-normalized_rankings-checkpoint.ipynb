{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LN - Data Pre-Processing - Normalized rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import itertools\n",
    "#import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import io\n",
    "import random\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "#from tqdm.notebook import trange\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "from time import sleep\n",
    "\n",
    "from dask_cloudprovider import FargateCluster\n",
    "from dask.distributed import Client\n",
    "import dask.array as da\n",
    "import dask\n",
    "dask.config.set({'distributed.scheduler.allowed-failures': 50}) \n",
    "\n",
    "\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "\n",
    "bucket='ln-strategy-data'\n",
    "extraction_id=1587447789\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate s3 resource\n",
    "\n",
    "session = boto3.session.Session()\n",
    "s3 = session.resource('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load objects form S3\n",
    "# Dataframe\n",
    "\n",
    "decisions_load = s3.Object(bucket_name=bucket, key='decisions_df.csv').get()\n",
    "decisions_df=pd.read_csv(io.BytesIO(decisions_load['Body'].read()))\n",
    "\n",
    "# Channel closures\n",
    "closure_file = s3.Object(bucket_name=bucket, key='channel_closures.p').get()\n",
    "channel_closures = pickle.loads(closure_file['Body'].read())\n",
    "    \n",
    "    \n",
    "# Channel openings \n",
    "opens_file = s3.Object(bucket_name=bucket, key='channel_opens.p').get()\n",
    "channel_opens = pickle.loads(opens_file['Body'].read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--OPENS---\n",
      "[(505149, [5314, 6038]), (506402, [934, 3023]), (506847, [576, 3452]), (508075, [3436, 3310]), (508090, [2378, 4223]), (508320, [1912, 422]), (508400, [1912, 5154]), (508447, [6656, 6595, 2120, 4688, 4119]), (508503, [422, 2953, 5426, 7059, 3957, 7478, 2518, 5725]), (508666, [422, 5294])]\n",
      "--CLOSURES---\n",
      "[(505149, []), (506402, []), (506847, []), (508075, []), (508090, []), (508320, []), (508400, []), (508447, []), (508503, []), (508666, [])]\n"
     ]
    }
   ],
   "source": [
    "# Transform data: Create list of .items with nodes involved in opens/closures per block\n",
    "open_nodes_list=[(opens[0],list(set([i for t in opens[1] for i in t[:2]]))) for opens in sorted(list(channel_opens.items()))]\n",
    "closure_nodes_list=[(closes[0],list(set([i for t in closes[1] for i in t[:2]]))) for closes in sorted(list(channel_closures.items()))]\n",
    "print('--OPENS---')\n",
    "print(open_list_sets[:10])\n",
    "print('--CLOSURES---')\n",
    "print(closure_list_sets[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_keys = [obj.key \n",
    "        for obj in s3.Bucket(name=bucket).objects.all()\n",
    "        if re.match(\".*\"+str(extraction_id)+\"_connected/.*\\.gpickle\",obj.key)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base lists to be populated\n",
    "blocks=[]\n",
    "base_ix=6 # From this index onward the connected component has more than 3 items. \n",
    "final_ix=1000\n",
    "#final_ix=len(graph_keys)\n",
    "extract_keys=graph_keys[base_ix:final_ix] # Blocks below 6th index are <3 and affect some graph metrics\n",
    "\n",
    "for key in extract_keys: \n",
    "    \n",
    "    # Create block list from file_names\n",
    "    block_i=int(key.split(\".\")[0].split(\"/\")[-1]) \n",
    "    blocks.append(block_i)\n",
    "    \n",
    "# Update node lists\n",
    "open_nodes=open_nodes_list[base_ix:final_ix]\n",
    "closure_nodes=closure_nodes_list[base_ix:final_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = FargateCluster(n_workers=10,scheduler_timeout='20 minutes',image='dsrincon/dask-graph:nx-scipy-v1',scheduler_cpu=4096,scheduler_mem=16384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4ee87c9afc4f279c4eeeb496b1b2ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>FargateCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n  â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/distributed/client.py:1079: VersionMismatchWarning: Mismatched versions found\n",
      "\n",
      "python\n",
      "+---------------------------+---------------+\n",
      "|                           | version       |\n",
      "+---------------------------+---------------+\n",
      "| client                    | 3.7.3.final.0 |\n",
      "| scheduler                 | 3.7.4.final.0 |\n",
      "| tcp://172.31.22.145:42769 | 3.7.4.final.0 |\n",
      "| tcp://172.31.38.7:37551   | 3.7.4.final.0 |\n",
      "| tcp://172.31.44.211:40181 | 3.7.4.final.0 |\n",
      "| tcp://172.31.52.161:46465 | 3.7.4.final.0 |\n",
      "| tcp://172.31.56.17:42725  | 3.7.4.final.0 |\n",
      "| tcp://172.31.76.24:34171  | 3.7.4.final.0 |\n",
      "| tcp://172.31.83.221:43709 | 3.7.4.final.0 |\n",
      "| tcp://172.31.87.105:44503 | 3.7.4.final.0 |\n",
      "| tcp://172.31.89.169:34801 | 3.7.4.final.0 |\n",
      "| tcp://172.31.92.34:40195  | 3.7.4.final.0 |\n",
      "+---------------------------+---------------+\n",
      "\n",
      "tornado\n",
      "+---------------------------+---------+\n",
      "|                           | version |\n",
      "+---------------------------+---------+\n",
      "| client                    | 6.0.3   |\n",
      "| scheduler                 | 6.0.4   |\n",
      "| tcp://172.31.22.145:42769 | 6.0.4   |\n",
      "| tcp://172.31.38.7:37551   | 6.0.4   |\n",
      "| tcp://172.31.44.211:40181 | 6.0.4   |\n",
      "| tcp://172.31.52.161:46465 | 6.0.4   |\n",
      "| tcp://172.31.56.17:42725  | 6.0.4   |\n",
      "| tcp://172.31.76.24:34171  | 6.0.4   |\n",
      "| tcp://172.31.83.221:43709 | 6.0.4   |\n",
      "| tcp://172.31.87.105:44503 | 6.0.4   |\n",
      "| tcp://172.31.89.169:34801 | 6.0.4   |\n",
      "| tcp://172.31.92.34:40195  | 6.0.4   |\n",
      "+---------------------------+---------+\n",
      "  warnings.warn(version_module.VersionMismatchWarning(msg[0][\"warning\"]))\n"
     ]
    }
   ],
   "source": [
    "client=Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def graph_ranking(input_tuple):\n",
    "    \n",
    "    # Unpacking input\n",
    "    block_num=input_tuple[0]\n",
    "    measurement=input_tuple[1]\n",
    "    extraction_id=input_tuple[2]\n",
    "    key_rawscore=input_tuple[3]\n",
    "    bucket=input_tuple[4]\n",
    "    \n",
    "    \n",
    "    # Retrieve snapshot from S3\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.resource('s3')\n",
    "    response = s3.Object(bucket_name=bucket, key=key_rawscore).get()\n",
    "    snapshot=pickle.loads(response['Body'].read())\n",
    "    \n",
    "    \n",
    "    # Calculate ranking for snapshot\n",
    "    max_value = max(snapshot.values())\n",
    "    norm_rank = {k: v / max_value for k, v in snapshot.items()}\n",
    "    \n",
    "    \n",
    "    # Write output into S3\n",
    "    key_out='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/norm_rank/'+str(block_num)+'.pkl'\n",
    "    pickle_byte_obj = pickle.dumps(norm_rank)\n",
    "    response=s3.Object(bucket,key_out).put(Body=pickle_byte_obj)['ResponseMetadata']['HTTPStatusCode']\n",
    "    \n",
    "    \n",
    "    return response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST calculate_ranking \n",
    "\n",
    "test_block=516790\n",
    "measurement='channels'\n",
    "input_tuple=(test_block,measurement,extraction_id)\n",
    "response_test=calculate_ranking(input_tuple)\n",
    "\n",
    "# Test if function saved result correctly and download result\n",
    "if response_test==200:\n",
    "    key_test='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/norm_rank/'+str(test_block)+'.pkl'\n",
    "    g_rank_test_load = s3.Object(bucket_name=bucket, key=key_test).get()\n",
    "    g_rank_test = pickle.loads(g_rank_test_load['Body'].read())\n",
    "    g_rank_values=sorted([v for k,v in g_rank_test.items()])\n",
    "    #print(g_rank_values)\n",
    "    #print('The dic saved has these first items: {}'.format(list(g_rank_test.items())))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collection_ranking(extraction_id,blocks,measurement,bucket):\n",
    "\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.resource('s3')\n",
    "    \n",
    "  \n",
    "    \n",
    "\n",
    "    delayed_responses=[]\n",
    "    with tqdm(total=len(blocks)) as pbar:\n",
    "        for block_num in blocks:\n",
    "\n",
    "            \n",
    "            # Create key\n",
    "            key='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/raw_score/'+str(block_num)+'.pkl'\n",
    "            \n",
    "            # Create input tuple\n",
    "            input_tuple=(block_num,measurement,extraction_id,key,bucket)\n",
    "            \n",
    "            # Run delayed function using dask\n",
    "            response=dask.delayed(graph_ranking)(input_tuple)\n",
    "            delayed_responses.append(response)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Collect futures\n",
    "    futures = dask.persist(*delayed_responses)\n",
    "\n",
    "    # Run parallel computations\n",
    "    start=time.time()\n",
    "    final_responses = dask.compute(*futures)\n",
    "    end=time.time()\n",
    "    print('Compute in seconds: {}'.format(end-start))\n",
    "\n",
    "    return final_responses\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c71a31f18f134527bc270a15deb99ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=36536.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compute in seconds: 591.6712484359741\n"
     ]
    }
   ],
   "source": [
    "# Test collection_ranking\n",
    "test_responses=collection_ranking(extraction_id,blocks,'channels',bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200)\n"
     ]
    }
   ],
   "source": [
    "print(test_responses[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate normalized rankings for measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Age ranking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "751ac66a51ee4a59b4928515db7f8955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=36536.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compute in seconds: 384.95358204841614\n"
     ]
    }
   ],
   "source": [
    "age_responses=collection_ranking(extraction_id,blocks,'age',bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Capacity ranking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d04043f743b496bb579857a5a5bcb31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=36536.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compute in seconds: 360.1695795059204\n"
     ]
    }
   ],
   "source": [
    "capacity_responses=collection_ranking(extraction_id,blocks,'capacity',bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Betweeness ranking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47df99dc334c49dbb74b0e195c3d6986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=36536.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compute in seconds: 363.8422577381134\n"
     ]
    }
   ],
   "source": [
    "betweeness_responses=collection_ranking(extraction_id,blocks,'betweeness_curr_aprox',bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Growth ranking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0733546dd6547459c203b632ddcaff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=36536.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compute in seconds: 408.5428283214569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n",
      "_GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=CancelledError()>\n",
      "concurrent.futures._base.CancelledError\n"
     ]
    }
   ],
   "source": [
    "growth_responses=collection_ranking(extraction_id,blocks,'capacity_growth',bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Channels ranking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_responses=collection_ranking(extraction_id,blocks,'channels',bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TETS: Test correct norm_ranking creation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5314, 2.7010283123980915e-05), (934, 4.241744824033501e-05), (3023, 7.752735612136513e-15), (3452, 1.0746104212680437e-05), (576, 3.408679201523418e-16), (3436, 0.02993702469278996), (3310, 1.2395197096448792e-15), (4223, 0.0009586206254005652), (422, 0.029096542491107023), (1912, 3.51576918762022e-05)]\n",
      "[(5314, 0.000191980840699471), (934, 0.00030149026339808336), (3023, 5.510407624040229e-14), (3452, 7.638002576740317e-05), (576, 2.422785039977094e-15), (3436, 0.21278322563971583), (3310, 8.810127418098523e-15), (4223, 0.006813582543044772), (422, 0.20680933492071135), (1912, 0.00024989013304542766)]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "test_block=blocks[-100]\n",
    "measurement='betweeness_curr_aprox'\n",
    "key_raw='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/raw_score/'+str(test_block)+'.pkl'\n",
    "key_norm='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/norm_rank/'+str(test_block)+'.pkl'\n",
    "\n",
    "\n",
    "raw_load = s3.Object(bucket_name=bucket, key=key_raw).get()\n",
    "dic_raw = pickle.loads(raw_load['Body'].read())\n",
    "items_raw=list(dic_raw.items())\n",
    "\n",
    "norm_load = s3.Object(bucket_name=bucket, key=key_norm).get()\n",
    "dic_norm = pickle.loads(norm_load['Body'].read())\n",
    "items_norm=list(dic_norm.items())\n",
    "\n",
    "\n",
    "print(items_raw[:10])\n",
    "print(items_norm[:10])\n",
    "print(len(items_raw)==len(items_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add normalized rankings to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_values(input_tuple):\n",
    "    \n",
    "    # Unpack input tuple\n",
    "    \n",
    "    bucket=input_tuple[0]\n",
    "    extraction_id=input_tuple[1]\n",
    "    measurement=input_tuple[2]\n",
    "    score_type=input_tuple[3]\n",
    "    prev_block=input_tuple[4]\n",
    "    act_block=input_tuple[5]\n",
    "    nodes_lists=input_tuple[6]\n",
    "    decision_type=input_tuple[7]\n",
    "    \n",
    "    \n",
    "    # Initialize list of lists to return\n",
    "    values_lists=[]\n",
    "    \n",
    "    # Start S3 session\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.resource('s3')\n",
    "    \n",
    "    # Create keys\n",
    "    dic_key='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/'+score_type+'/'+str(prev_block)+'.pkl'\n",
    "    g_key='graph_snapshots/'+str(extraction_id)+'_connected/'+str(prev_block)+'.gpickle'\n",
    "    \n",
    "    \n",
    "    # Load dic\n",
    "    dic_load = s3.Object(bucket_name=bucket, key=dic_key).get()\n",
    "    dic = pickle.loads(dic_load['Body'].read())\n",
    "                       \n",
    "    # Load graph\n",
    "    g_load = s3.Object(bucket_name=bucket, key=g_key).get()\n",
    "    g = pickle.loads(g_load['Body'].read())\n",
    "                     \n",
    "                       \n",
    "    # Extract relevant values\n",
    "    for nodes in nodes_lists:\n",
    "        # Define list to return\n",
    "        values_i=[]\n",
    "        \n",
    "        # loop over nodes in lists\n",
    "        for node in nodes:\n",
    "            # Check if node is in graph and retrieve metric from dic, else set to 0\n",
    "            if g.has_node(node):         \n",
    "                values_i.append(dic[node])\n",
    "            else:\n",
    "                values_i.append(0)   \n",
    "        \n",
    "        # Update value list in global return list\n",
    "        values_lists.append(values_i)\n",
    "        \n",
    "    # Save value to S3\n",
    "    key_out='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/'+decision_type+'_'+score_type+'/'+str(act_block)+'.pkl'\n",
    "    pickle_byte_obj = pickle.dumps(values_lists)\n",
    "    response=s3.Object(bucket,key_out).put(Body=pickle_byte_obj)['ResponseMetadata']['HTTPStatusCode']\n",
    "                     \n",
    "    return response\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Block:615886\n",
      "Test Nodes:[[2940, 668, 847, 6748], [5321, 4517]]\n",
      "Elapsed time seconds:0.2798001766204834\n",
      "Values:[[0, 0.15827338129496402, 0.17176258992805754, 0.4433453237410072], [0.7437050359712231, 0.00539568345323741]]\n"
     ]
    }
   ],
   "source": [
    "# TEST extract_values function\n",
    "\n",
    "# set parameters\n",
    "test_ix=-1005\n",
    "test_act_block=blocks[test_ix]\n",
    "test_prev_block=blocks[test_ix-1]\n",
    "test_nodes=[open_nodes[test_ix][1],closure_nodes[test_ix][1]]\n",
    "measurement='channels'\n",
    "score_type='norm_rank'\n",
    "decision_type='open'\n",
    "\n",
    "print('Test Block:{}'.format(test_block))\n",
    "print('Test Nodes:{}'.format(test_nodes))\n",
    "\n",
    "# run function and print results\n",
    "start=time.time()\n",
    "test_input_tuple=(bucket,extraction_id,measurement,score_type,test_prev_block,test_act_block,test_nodes,decision_type)\n",
    "response=extract_values(test_input_tuple)\n",
    "\n",
    "end=time.time()\n",
    "\n",
    "\n",
    "if response==200:\n",
    "    \n",
    "    # Check value recorded to s3\n",
    "    values_key='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/'+decision_type+'_'+score_type+'/'+str(test_act_block)+'.pkl'\n",
    "    test_load = s3.Object(bucket_name=bucket, key=values_key).get()\n",
    "    values = pickle.loads(test_load['Body'].read())\n",
    "    \n",
    "    # Print results\n",
    "    print('Elapsed time seconds:{}'.format(end-start))\n",
    "    print('Values:{}'.format(values))\n",
    "    \n",
    "else:\n",
    "    print('Did not save to S3 correctly')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collection_extract_values(decisions_df,blocks,decision_type,measurement,score_type):\n",
    "    \n",
    "    # Define dictonary to return\n",
    "    delayed_responses=[]\n",
    "    \n",
    "    # Set list of blocks and column names\n",
    "    if decision_type=='open':\n",
    "        column_name='open_block'\n",
    "        dec_blocks=[b for b,o in open_nodes if len(o)>0]\n",
    "        \n",
    "    if decision_type=='close':\n",
    "        column_name='close_block'\n",
    "        dec_blocks=[b for b,o in closure_nodes if len(o)>0]\n",
    "    \n",
    "    # Loop over blocks in decision type, starting from the 2nd one\n",
    "    print(len(dec_blocks))\n",
    "    with tqdm(total=len(range(1,len(dec_blocks)))) as pbar:\n",
    "        for i in range(1,len(dec_blocks)):\n",
    "\n",
    "            # Define blocks to look at decisions and prev block to query measurments\n",
    "            dec_block_i=dec_blocks[i]\n",
    "            prev_block=blocks[blocks.index(dec_block_i)-1]\n",
    "\n",
    "            # Select nodes in node0,node1 and create list of lists\n",
    "            node0_nodes=decisions_df[decisions_df[column_name]==dec_block_i]['node0_id'].tolist()\n",
    "            node1_nodes=decisions_df[decisions_df[column_name]==dec_block_i]['node1_id'].tolist()\n",
    "            nodes_lists=[node0_nodes,node1_nodes]\n",
    "\n",
    "            # Run delayed function to extract values for node0 and node1 in prev_block\n",
    "            input_tuple=(bucket,extraction_id,measurement,score_type,prev_block,dec_block_i,nodes_lists,decision_type)\n",
    "            response=dask.delayed(extract_values)(input_tuple)\n",
    "            delayed_responses.append(response)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.update(1)\n",
    "        \n",
    "\n",
    "        # Collect futures\n",
    "        futures = dask.persist(*delayed_responses)\n",
    "\n",
    "        # Run parallel computations\n",
    "        start=time.time()\n",
    "        final_responses = dask.compute(*futures)\n",
    "        end=time.time()\n",
    "        print('Compute in seconds: {}'.format(end-start))\n",
    "    \n",
    "    \n",
    "    return final_responses\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Channels: DF update extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "994\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af5437a43414bd78a64b813bd7fc2fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=993.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute in seconds: 15.531179904937744\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n",
      "_GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=CancelledError()>\n",
      "concurrent.futures._base.CancelledError\n"
     ]
    }
   ],
   "source": [
    "open_channelsrank_responses=collection_extract_values(decisions_df,blocks,'open','age','norm_rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "994"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test open_channelsrank_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_channelsrank_responses=collection_extract_values(decisions_df,blocks,'close','age','norm_rank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add normalized rankings to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_dataframe(s3,decisions_df,decison_blocks,node_values,decision_type,measurement,score_type):\n",
    "    \n",
    "    for block in decision_blocks:\n",
    "        \n",
    "        # Extract measurement values for block from S3:\n",
    "        values_key='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/'+decision_type+'_'+score_type+'/'+str(block)+'.pkl'\n",
    "        values_load = s3.Object(bucket_name=bucket, key=values_key).get()\n",
    "        values = pickle.loads(values_load['Body'].read())\n",
    "        \n",
    "        for \n",
    "        \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
