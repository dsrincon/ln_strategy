{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LN - Data PP - Stability and efficiency calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import itertools\n",
    "#import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import io\n",
    "import random\n",
    "from itertools import islice\n",
    "import math\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "#from tqdm.notebook import trange\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "from time import sleep\n",
    "\n",
    "from dask_cloudprovider import FargateCluster\n",
    "from dask.distributed import Client\n",
    "import dask.array as da\n",
    "import dask\n",
    "dask.config.set({'distributed.scheduler.allowed-failures': 50}) \n",
    "\n",
    "\n",
    "import boto3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "\n",
    "bucket='ln-strategy-data'\n",
    "extraction_id=1587447789\n",
    "#extraction_id=1585344554"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection to AWS Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate s3 resource\n",
    "\n",
    "session = boto3.session.Session()\n",
    "s3 = session.resource('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fargate Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster = FargateCluster(n_workers=100,scheduler_timeout='20 minutes',image='dsrincon/dask-graph:nx-scipy-v1',scheduler_cpu=4096,scheduler_mem=16384,worker_mem=32768)\n",
    "cluster = FargateCluster(n_workers=60,scheduler_timeout='10 minutes',image='dsrincon/dask-graph:nx-scipy-v1',scheduler_cpu=4096,scheduler_mem=16384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "801b544825c7403c8d8525250c9be20d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>FargateCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n  â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/distributed/client.py:1079: VersionMismatchWarning: Mismatched versions found\n",
      "\n",
      "python\n",
      "+---------------------------+---------------+\n",
      "|                           | version       |\n",
      "+---------------------------+---------------+\n",
      "| client                    | 3.7.3.final.0 |\n",
      "| scheduler                 | 3.7.4.final.0 |\n",
      "| tcp://172.31.0.195:35081  | 3.7.4.final.0 |\n",
      "| tcp://172.31.14.163:34909 | 3.7.4.final.0 |\n",
      "| tcp://172.31.14.55:36647  | 3.7.4.final.0 |\n",
      "| tcp://172.31.15.132:37669 | 3.7.4.final.0 |\n",
      "| tcp://172.31.16.121:40997 | 3.7.4.final.0 |\n",
      "| tcp://172.31.17.224:34133 | 3.7.4.final.0 |\n",
      "| tcp://172.31.18.106:40207 | 3.7.4.final.0 |\n",
      "| tcp://172.31.18.231:33677 | 3.7.4.final.0 |\n",
      "| tcp://172.31.22.216:45263 | 3.7.4.final.0 |\n",
      "| tcp://172.31.22.25:40657  | 3.7.4.final.0 |\n",
      "| tcp://172.31.23.114:38071 | 3.7.4.final.0 |\n",
      "| tcp://172.31.24.238:42373 | 3.7.4.final.0 |\n",
      "| tcp://172.31.31.206:36863 | 3.7.4.final.0 |\n",
      "| tcp://172.31.31.74:40949  | 3.7.4.final.0 |\n",
      "| tcp://172.31.32.10:37897  | 3.7.4.final.0 |\n",
      "| tcp://172.31.32.85:40017  | 3.7.4.final.0 |\n",
      "| tcp://172.31.34.208:43373 | 3.7.4.final.0 |\n",
      "| tcp://172.31.37.155:36297 | 3.7.4.final.0 |\n",
      "| tcp://172.31.39.1:39575   | 3.7.4.final.0 |\n",
      "| tcp://172.31.39.57:44677  | 3.7.4.final.0 |\n",
      "| tcp://172.31.4.253:45103  | 3.7.4.final.0 |\n",
      "| tcp://172.31.40.90:36785  | 3.7.4.final.0 |\n",
      "| tcp://172.31.41.203:33947 | 3.7.4.final.0 |\n",
      "| tcp://172.31.41.255:46249 | 3.7.4.final.0 |\n",
      "| tcp://172.31.42.10:37065  | 3.7.4.final.0 |\n",
      "| tcp://172.31.50.197:45139 | 3.7.4.final.0 |\n",
      "| tcp://172.31.51.233:42777 | 3.7.4.final.0 |\n",
      "| tcp://172.31.51.91:39141  | 3.7.4.final.0 |\n",
      "| tcp://172.31.54.34:46225  | 3.7.4.final.0 |\n",
      "| tcp://172.31.54.46:43241  | 3.7.4.final.0 |\n",
      "| tcp://172.31.59.162:41569 | 3.7.4.final.0 |\n",
      "| tcp://172.31.60.21:44569  | 3.7.4.final.0 |\n",
      "| tcp://172.31.60.58:43991  | 3.7.4.final.0 |\n",
      "| tcp://172.31.62.46:46227  | 3.7.4.final.0 |\n",
      "| tcp://172.31.63.44:43171  | 3.7.4.final.0 |\n",
      "| tcp://172.31.64.11:35547  | 3.7.4.final.0 |\n",
      "| tcp://172.31.64.4:39509   | 3.7.4.final.0 |\n",
      "| tcp://172.31.65.133:37151 | 3.7.4.final.0 |\n",
      "| tcp://172.31.68.188:44405 | 3.7.4.final.0 |\n",
      "| tcp://172.31.69.63:41163  | 3.7.4.final.0 |\n",
      "| tcp://172.31.7.156:42513  | 3.7.4.final.0 |\n",
      "| tcp://172.31.72.147:33333 | 3.7.4.final.0 |\n",
      "| tcp://172.31.72.158:41029 | 3.7.4.final.0 |\n",
      "| tcp://172.31.73.89:44839  | 3.7.4.final.0 |\n",
      "| tcp://172.31.74.45:34949  | 3.7.4.final.0 |\n",
      "| tcp://172.31.77.154:43787 | 3.7.4.final.0 |\n",
      "| tcp://172.31.80.220:32871 | 3.7.4.final.0 |\n",
      "| tcp://172.31.81.183:39199 | 3.7.4.final.0 |\n",
      "| tcp://172.31.81.211:39295 | 3.7.4.final.0 |\n",
      "| tcp://172.31.81.29:41867  | 3.7.4.final.0 |\n",
      "| tcp://172.31.82.214:36955 | 3.7.4.final.0 |\n",
      "| tcp://172.31.82.219:33937 | 3.7.4.final.0 |\n",
      "| tcp://172.31.82.236:44757 | 3.7.4.final.0 |\n",
      "| tcp://172.31.88.5:45471   | 3.7.4.final.0 |\n",
      "| tcp://172.31.9.113:41123  | 3.7.4.final.0 |\n",
      "| tcp://172.31.9.194:37845  | 3.7.4.final.0 |\n",
      "| tcp://172.31.9.210:42393  | 3.7.4.final.0 |\n",
      "| tcp://172.31.9.26:36741   | 3.7.4.final.0 |\n",
      "| tcp://172.31.92.189:44335 | 3.7.4.final.0 |\n",
      "| tcp://172.31.95.242:42007 | 3.7.4.final.0 |\n",
      "+---------------------------+---------------+\n",
      "\n",
      "tornado\n",
      "+---------------------------+---------+\n",
      "|                           | version |\n",
      "+---------------------------+---------+\n",
      "| client                    | 6.0.3   |\n",
      "| scheduler                 | 6.0.4   |\n",
      "| tcp://172.31.0.195:35081  | 6.0.4   |\n",
      "| tcp://172.31.14.163:34909 | 6.0.4   |\n",
      "| tcp://172.31.14.55:36647  | 6.0.4   |\n",
      "| tcp://172.31.15.132:37669 | 6.0.4   |\n",
      "| tcp://172.31.16.121:40997 | 6.0.4   |\n",
      "| tcp://172.31.17.224:34133 | 6.0.4   |\n",
      "| tcp://172.31.18.106:40207 | 6.0.4   |\n",
      "| tcp://172.31.18.231:33677 | 6.0.4   |\n",
      "| tcp://172.31.22.216:45263 | 6.0.4   |\n",
      "| tcp://172.31.22.25:40657  | 6.0.4   |\n",
      "| tcp://172.31.23.114:38071 | 6.0.4   |\n",
      "| tcp://172.31.24.238:42373 | 6.0.4   |\n",
      "| tcp://172.31.31.206:36863 | 6.0.4   |\n",
      "| tcp://172.31.31.74:40949  | 6.0.4   |\n",
      "| tcp://172.31.32.10:37897  | 6.0.4   |\n",
      "| tcp://172.31.32.85:40017  | 6.0.4   |\n",
      "| tcp://172.31.34.208:43373 | 6.0.4   |\n",
      "| tcp://172.31.37.155:36297 | 6.0.4   |\n",
      "| tcp://172.31.39.1:39575   | 6.0.4   |\n",
      "| tcp://172.31.39.57:44677  | 6.0.4   |\n",
      "| tcp://172.31.4.253:45103  | 6.0.4   |\n",
      "| tcp://172.31.40.90:36785  | 6.0.4   |\n",
      "| tcp://172.31.41.203:33947 | 6.0.4   |\n",
      "| tcp://172.31.41.255:46249 | 6.0.4   |\n",
      "| tcp://172.31.42.10:37065  | 6.0.4   |\n",
      "| tcp://172.31.50.197:45139 | 6.0.4   |\n",
      "| tcp://172.31.51.233:42777 | 6.0.4   |\n",
      "| tcp://172.31.51.91:39141  | 6.0.4   |\n",
      "| tcp://172.31.54.34:46225  | 6.0.4   |\n",
      "| tcp://172.31.54.46:43241  | 6.0.4   |\n",
      "| tcp://172.31.59.162:41569 | 6.0.4   |\n",
      "| tcp://172.31.60.21:44569  | 6.0.4   |\n",
      "| tcp://172.31.60.58:43991  | 6.0.4   |\n",
      "| tcp://172.31.62.46:46227  | 6.0.4   |\n",
      "| tcp://172.31.63.44:43171  | 6.0.4   |\n",
      "| tcp://172.31.64.11:35547  | 6.0.4   |\n",
      "| tcp://172.31.64.4:39509   | 6.0.4   |\n",
      "| tcp://172.31.65.133:37151 | 6.0.4   |\n",
      "| tcp://172.31.68.188:44405 | 6.0.4   |\n",
      "| tcp://172.31.69.63:41163  | 6.0.4   |\n",
      "| tcp://172.31.7.156:42513  | 6.0.4   |\n",
      "| tcp://172.31.72.147:33333 | 6.0.4   |\n",
      "| tcp://172.31.72.158:41029 | 6.0.4   |\n",
      "| tcp://172.31.73.89:44839  | 6.0.4   |\n",
      "| tcp://172.31.74.45:34949  | 6.0.4   |\n",
      "| tcp://172.31.77.154:43787 | 6.0.4   |\n",
      "| tcp://172.31.80.220:32871 | 6.0.4   |\n",
      "| tcp://172.31.81.183:39199 | 6.0.4   |\n",
      "| tcp://172.31.81.211:39295 | 6.0.4   |\n",
      "| tcp://172.31.81.29:41867  | 6.0.4   |\n",
      "| tcp://172.31.82.214:36955 | 6.0.4   |\n",
      "| tcp://172.31.82.219:33937 | 6.0.4   |\n",
      "| tcp://172.31.82.236:44757 | 6.0.4   |\n",
      "| tcp://172.31.88.5:45471   | 6.0.4   |\n",
      "| tcp://172.31.9.113:41123  | 6.0.4   |\n",
      "| tcp://172.31.9.194:37845  | 6.0.4   |\n",
      "| tcp://172.31.9.210:42393  | 6.0.4   |\n",
      "| tcp://172.31.9.26:36741   | 6.0.4   |\n",
      "| tcp://172.31.92.189:44335 | 6.0.4   |\n",
      "| tcp://172.31.95.242:42007 | 6.0.4   |\n",
      "+---------------------------+---------+\n",
      "  warnings.warn(version_module.VersionMismatchWarning(msg[0][\"warning\"]))\n"
     ]
    }
   ],
   "source": [
    "client = Client(cluster)\n",
    "#cluster=Client('tcp://54.85.207.8:8786')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Write output to DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function write output to DataFrame\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "add_columns\n",
    "    Function that takes an output from a decision comparisson computation and adds it's results for nodes 1 and 0 in the main DataFrame\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "output_diclist : list\n",
    "    Dictionary of the form (node0_dic_i,node1_dic_i) where i runs for all of the blocks being compared. \n",
    "\n",
    "original_df: Pandas DataFrame\n",
    "    Original DataFrame containing the opening and closure information for each channel, with a column named 'short_channel_id' to denote \n",
    "    id of channel. \n",
    "\n",
    "column_name_node0: string\n",
    "    Name for column in dataframe where the results will be stored for node 0\n",
    "    \n",
    "column_name_node1: string\n",
    "    Name for column in dataframe where the results will be stored for node 1\n",
    "\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "no_changes: list\n",
    "    List with the 'short_channel_id' of the channels edited. \n",
    " \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def add_columns(output_diclist,original_df,column_name_node0,column_name_node1):\n",
    "\n",
    "\n",
    "    # Merge individual dictionaries into one for each node\n",
    "    node0_dic={}\n",
    "    node1_dic={}\n",
    "    for dic_tuple in output_diclist:\n",
    "        node0_dic.update(dic_tuple[0])\n",
    "        node1_dic.update(dic_tuple[1])\n",
    "    \n",
    "    # Add to DataFrame\n",
    "\n",
    "    # Create empty columns\n",
    "    original_df[column_name_node0]=np.nan\n",
    "    original_df[column_name_node1]=np.nan\n",
    "\n",
    "    # Populate df with values\n",
    "    original_df[column_name_node0]=original_df['short_channel_id'].map(node0_dic)\n",
    "    original_df[column_name_node1]=original_df['short_channel_id'].map(node1_dic)\n",
    "    \n",
    "    # Calculate values changed\n",
    "    rows_edited=(original_df[original_df[column_name_node0].notnull()]['short_channel_id']).tolist()\n",
    "    \n",
    "    return rows_edited\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Save python object to S3 using pickle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to write pickle data to S3 bucket\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "pickle_save_s3\n",
    "    Saves Python object to S3 as pickle object\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "obj : <any>\n",
    "    Python Object\n",
    "\n",
    "blocks: list\n",
    "    List of extracted blocks\n",
    "\n",
    "extraction_id: int\n",
    "    Number of block extraction\n",
    "    \n",
    "name: string\n",
    "    Name of object to add to filename in S3\n",
    "\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "response: int\n",
    "    HTTP response code from S3 \n",
    " \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def pickle_save_s3(obj,blocks,extraction_id,name):\n",
    "\n",
    "\n",
    "    # Define number of blocks\n",
    "    start_block=np.min(np.array(blocks))\n",
    "    end_block=np.max(np.array(blocks))\n",
    "    no_blocks=len(blocks)\n",
    "\n",
    "    # Load S3 and bucket details\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.resource('s3')\n",
    "\n",
    "    # File path and name ([extraction_id][name]-[no_blocks]-[start_block]-[end_block])\n",
    "    key='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+str(extraction_id)+name+'-'+str(no_blocks)+'-'+str(start_block)+'-'+str(end_block)+'.pkl'\n",
    "\n",
    "    # Create pickle object and send to S3\n",
    "    pickle_byte_obj = pickle.dumps(obj) \n",
    "    response=s3.Object(bucket,key).put(Body=pickle_byte_obj)\n",
    "    \n",
    "    return response['ResponseMetadata']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Load single graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "load_graph\n",
    "    Loads networkX (pickle serialized) object from S3\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "key : str\n",
    "    Path in S3 bucket for individual pickled serialized networkX graph object \n",
    "\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "response: networkX graph\n",
    "    Graph object\n",
    " \n",
    "\"\"\"\n",
    "\n",
    "def load_graph(key):\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.resource('s3')\n",
    "    response = s3.Object(bucket_name=bucket, key=key).get()\n",
    "    G=pickle.loads(response['Body'].read())\n",
    "    \n",
    "    return G\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Run graph measurement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "load_graph_measurement:\n",
    "    Runs graph measurement for every node in a specific block and loads the created dictionary from S3\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "extraction_id: int\n",
    "    Timestamp of block extraction\n",
    "\n",
    "test_ix: int\n",
    "    Index of block to test. Can be negative to move backwards in array\n",
    "\n",
    "measurement: str\n",
    "    Type of graph measurement to perform\n",
    "\n",
    "weight: str\n",
    "    Node attribute to use for weighted calculations\n",
    "\n",
    "blocks: list\n",
    "    List of (ints) blocks extracted \n",
    "\n",
    "graph_keys: list\n",
    "    List of (str) graph paths in S3\n",
    "\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "g_test: NetworkX graph\n",
    "    NetworkX graph extracted for the given ix\n",
    "\n",
    "nodes_test: list\n",
    "    List of nodes in g_test\n",
    "\n",
    "g_dic_test: dict\n",
    "    Dictionary of graph measurment for each node in g_test\n",
    "\n",
    "block: int\n",
    "    Block selected for test\n",
    " \n",
    "\"\"\"\n",
    "\n",
    "def load_graph_measurement(extraction_id,measurement,weight,blocks,graph_keys,test_ix=None):\n",
    "\n",
    "    if test_ix==None: # If no index provided choose one at random\n",
    "        test_ix=random.choice(range(len(blocks)))\n",
    "        \n",
    "        \n",
    "    \n",
    "    # Define block and graph key\n",
    "    test_block=blocks[test_ix]\n",
    "    g_key=graph_keys[test_ix]\n",
    "    print('Block selected:{}'.format(test_block))\n",
    "    \n",
    "    # Load graph and nodes\n",
    "    response = s3.Object(bucket_name=bucket, key=g_key).get()\n",
    "    g_test=pickle.loads(response['Body'].read())\n",
    "    nodes_test=list(g_test.nodes())\n",
    "\n",
    "    # Run function\n",
    "    block,response_test=graph_measurement((g_key,measurement,weight,bucket))\n",
    "\n",
    "\n",
    "    if response_test==200:\n",
    "        # Load created dictionary with calculation from S3\n",
    "        \n",
    "        key_test='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/raw_score/'+str(test_block)+'.pkl'\n",
    "        g__test_load = s3.Object(bucket_name=bucket, key=key_test).get()\n",
    "        g_dic_test = pickle.loads(g__test_load['Body'].read())\n",
    "        print('Dic was saved correctly. Sample below:')\n",
    "        print(list(g_dic_test.items())[:10])\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        print('Measurement was not saved correctly')\n",
    "        return\n",
    "    \n",
    "    return g_test,nodes_test,g_dic_test,test_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load objects form S3\n",
    "# Dataframe\n",
    "\n",
    "decisions_load = s3.Object(bucket_name=bucket, key='decisions_df.csv').get()\n",
    "decisions_df=pd.read_csv(io.BytesIO(decisions_load['Body'].read()))\n",
    "\n",
    "# Channel closures\n",
    "closure_file = s3.Object(bucket_name=bucket, key='channel_closures.p').get()\n",
    "channel_closures = pickle.loads(closure_file['Body'].read())\n",
    "    \n",
    "    \n",
    "# Channel openings \n",
    "opens_file = s3.Object(bucket_name=bucket, key='channel_opens.p').get()\n",
    "channel_opens = pickle.loads(opens_file['Body'].read())\n",
    "\n",
    "    \n",
    "\n",
    "# Create list with graph keys\n",
    "\n",
    "#TODO: Save graphs as numpy array in single H5 file to reduce. Test if creating graphs takes longer than reading from S3\n",
    "\n",
    "# graph_dir='./data/graph_snapshots' - For local tests\n",
    "\n",
    "\n",
    "graph_keys = [obj.key \n",
    "        for obj in s3.Bucket(name=bucket).objects.all()\n",
    "        if re.match(\".*\"+str(extraction_id)+\"_connected/.*\\.gpickle\",obj.key)]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Blocks to be extracted and define graph\n",
    "\n",
    "\n",
    "# Base lists to be populated\n",
    "graph_snapshots=[]\n",
    "blocks=[]\n",
    "base_ix=6\n",
    "\n",
    "\n",
    "extract_keys=graph_keys[base_ix:] # Blocks below 6th index are <3 and affect some graph metrics\n",
    "\n",
    "for key in extract_keys: # Change to [700:] for full range\n",
    "    \n",
    "    # Create block list from file_names\n",
    "    block_i=int(key.split(\".\")[0].split(\"/\")[-1]) \n",
    "    blocks.append(block_i)\n",
    "    \n",
    "    #Extract graphs - UNCOMMENT TO have them out of function\n",
    "    #G=dask.delayed(load_graph)(key)\n",
    "    #graph_snapshots.append(G)\n",
    "    \n",
    "   \n",
    "start_block=np.min(np.array(blocks))\n",
    "end_block=np.max(np.array(blocks))\n",
    "no_blocks=len(blocks)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Extract nodes and calculate age of nodes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29bb147ec1c34b8292dda6e8a059fd81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7735.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Details extracted and save to S3 succesful\n"
     ]
    }
   ],
   "source": [
    "# Define nodes and calculate details\n",
    "\n",
    "nodes=list(set(decisions_df['node0_id'].tolist()).union(set(decisions_df['node1_id'].tolist())))\n",
    "\n",
    "# DETAIL_1:Calculate birth block\n",
    "\n",
    "# Create list of sets (node pairs )\n",
    "opens_list=sorted(list(channel_opens.items()))\n",
    "open_list_sets=[(opens[0],[{t[0],t[1]} for t in opens[1]]) for opens in opens_list]\n",
    "\n",
    "# Dic to store details per node \n",
    "node_details={}\n",
    "\n",
    "\n",
    "with tqdm(total=len(nodes)) as pbar:\n",
    "    for node in nodes:\n",
    "\n",
    "        for opens in open_list_sets:\n",
    "            if opens[1] and node in set.union(*opens[1]):\n",
    "                birth_block=opens[0]\n",
    "                node_details[node]={'birth_block':birth_block}\n",
    "                break\n",
    "    \n",
    "        pbar.update(1)\n",
    "        \n",
    "\n",
    "# SAVE to S3\n",
    "\n",
    "key='node_details.p'\n",
    "pickle_byte_obj = pickle.dumps(node_details) \n",
    "response=s3.Object(bucket,key).put(Body=pickle_byte_obj)['ResponseMetadata']\n",
    "\n",
    "if response['HTTPStatusCode']==200:\n",
    "    print('Details extracted and save to S3 succesful')\n",
    "else: \n",
    "    print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details extracted and save to S3 succesful\n"
     ]
    }
   ],
   "source": [
    "# TESTS\n",
    "len_nodes=len(nodes)\n",
    "len_details=len(list(node_details.items()))\n",
    "\n",
    "\n",
    "if len_nodes==len_details and response['HTTPStatusCode']==200:\n",
    "    print('Details extracted and save to S3 succesful')\n",
    "else:\n",
    "    print('Details for {} nodes were not extracted'.format(len_nodes-len_details))   \n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total graph keys:36542\n",
      "Number of blocks to be processed:36536\n",
      "---Sample graph keys---\n",
      "graph_snapshots/1587447789_connected/505149.gpickle\n",
      "---Sample channel opens---\n",
      "[(2378, 4223, {'capacity': 400000, 'open_fee': 4557, 'dec_id': 58766, 'channel_id': '508090x1515x1', 'no_channels': 0})]\n",
      "---Sample channel closures---\n",
      "[(2643, 6038, {'close_type': 'force', 'dec_id': 26620, 'channel_id': '570913x720x1', 'capacity': 300000}), (6038, 5314, {'close_type': 'mutual', 'dec_id': 0, 'channel_id': '505149x622x0', 'capacity': 300000})]\n"
     ]
    }
   ],
   "source": [
    "# Test: extracted formats\n",
    "print(\"Number of total graph keys:{}\".format(len(graph_keys)))\n",
    "print(\"Number of blocks to be processed:{}\".format(len(extract_keys)))\n",
    "print(\"---Sample graph keys---\")\n",
    "print(graph_keys[0])\n",
    "print(\"---Sample channel opens---\")\n",
    "print(channel_opens[508090])\n",
    "print(\"---Sample channel closures---\")\n",
    "print(channel_closures[592638])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of DataFrame in Memory:64821774\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>short_channel_id</th>\n",
       "      <th>open_block</th>\n",
       "      <th>open_transaction</th>\n",
       "      <th>address</th>\n",
       "      <th>close_block</th>\n",
       "      <th>close_transaction</th>\n",
       "      <th>node0</th>\n",
       "      <th>node1</th>\n",
       "      <th>satoshis</th>\n",
       "      <th>...</th>\n",
       "      <th>close_fee</th>\n",
       "      <th>last_update</th>\n",
       "      <th>close_type</th>\n",
       "      <th>close_htlc_count</th>\n",
       "      <th>close_balance_a</th>\n",
       "      <th>close_balance_b</th>\n",
       "      <th>dec_id</th>\n",
       "      <th>node0_id</th>\n",
       "      <th>node1_id</th>\n",
       "      <th>node_pair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72475</th>\n",
       "      <td>0</td>\n",
       "      <td>505149x622x0</td>\n",
       "      <td>505149</td>\n",
       "      <td>f6bc767df9148ebf76d2b9baf4eb46e3230712c2bf5a51...</td>\n",
       "      <td>bc1qjmg6ev344fenh3zhg0yjl6hyvxpxluw6x9nn2a5lv4...</td>\n",
       "      <td>592638.0</td>\n",
       "      <td>82cb2ea2a06c8c453d8b9ca08e17bbefe87225aa380b2d...</td>\n",
       "      <td>0250373555232cec757ea141273e75381c84cc3ab22f1e...</td>\n",
       "      <td>02ef61a252f9504a42fc264a28476f44cea0711a44b2da...</td>\n",
       "      <td>300000</td>\n",
       "      <td>...</td>\n",
       "      <td>184.0</td>\n",
       "      <td>1.563172e+09</td>\n",
       "      <td>mutual</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3570.0</td>\n",
       "      <td>296246.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6038</td>\n",
       "      <td>5314</td>\n",
       "      <td>32085932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71541</th>\n",
       "      <td>36324</td>\n",
       "      <td>525150x3092x0</td>\n",
       "      <td>525150</td>\n",
       "      <td>d51eed0501cfb3ed1a6f0d1ee7a9fe85037eb489fd8211...</td>\n",
       "      <td>bc1qxy09e7wmkdm6g699zpjdtxepym5l4wqfc422ks3a8w...</td>\n",
       "      <td>592652.0</td>\n",
       "      <td>e7b02ed49b97b0ed4cca822edcc02313534f2c266cb074...</td>\n",
       "      <td>0250373555232cec757ea141273e75381c84cc3ab22f1e...</td>\n",
       "      <td>03bf7528f412ef621ac4898e4815a7c94d427e719eefba...</td>\n",
       "      <td>10000</td>\n",
       "      <td>...</td>\n",
       "      <td>8762.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unused</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1238.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36324</td>\n",
       "      <td>6038</td>\n",
       "      <td>1241</td>\n",
       "      <td>7493158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70663</th>\n",
       "      <td>36396</td>\n",
       "      <td>534878x239x0</td>\n",
       "      <td>534878</td>\n",
       "      <td>a7b2e480775f3baf2602e18abfde05dfcb4fc72719087a...</td>\n",
       "      <td>bc1qky60ee6h0l2n8rz2rz7dvpp3ez3eazmxtv0phhw5w9...</td>\n",
       "      <td>592652.0</td>\n",
       "      <td>8f470554f7fe76cfb33e15319387451aef97cbc04a6110...</td>\n",
       "      <td>0250373555232cec757ea141273e75381c84cc3ab22f1e...</td>\n",
       "      <td>032e04b67641c00444af1d83145c0b63bac8316a6afb8f...</td>\n",
       "      <td>50000</td>\n",
       "      <td>...</td>\n",
       "      <td>3806.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unused</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46194.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36396</td>\n",
       "      <td>6038</td>\n",
       "      <td>5515</td>\n",
       "      <td>33299570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67488</th>\n",
       "      <td>3758</td>\n",
       "      <td>551133x1292x0</td>\n",
       "      <td>551133</td>\n",
       "      <td>7c021f4fe8fd67f7619910958c8ad0b9d89dde8dd8436d...</td>\n",
       "      <td>bc1qr7m6r2uweu3jjwjwth7umwnjn3zyajdp5qf8fuy8kp...</td>\n",
       "      <td>592641.0</td>\n",
       "      <td>9149e4ff28e61e8169d3eeac97ec8c2444660642e38d35...</td>\n",
       "      <td>0250373555232cec757ea141273e75381c84cc3ab22f1e...</td>\n",
       "      <td>02ad6fb8d693dc1e4569bcedefadf5f72a931ae027dc0f...</td>\n",
       "      <td>100000</td>\n",
       "      <td>...</td>\n",
       "      <td>1417.0</td>\n",
       "      <td>1.563172e+09</td>\n",
       "      <td>mutual</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2399.0</td>\n",
       "      <td>96184.0</td>\n",
       "      <td>3758</td>\n",
       "      <td>6038</td>\n",
       "      <td>4998</td>\n",
       "      <td>30177924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65967</th>\n",
       "      <td>5143</td>\n",
       "      <td>554406x1810x0</td>\n",
       "      <td>554406</td>\n",
       "      <td>2684410b0c4aa5a33b6ba315fce2ff5af20c57a83b267d...</td>\n",
       "      <td>bc1qev2wed4sk02g3rmh9ptpalnk3lz020jkv2m8hu0rn6...</td>\n",
       "      <td>592641.0</td>\n",
       "      <td>1b44f65c02d5030b9e622919a92dba176c30e115845d93...</td>\n",
       "      <td>0250373555232cec757ea141273e75381c84cc3ab22f1e...</td>\n",
       "      <td>0279c22ed7a068d10dc1a38ae66d2d6461e269226c6025...</td>\n",
       "      <td>5000001</td>\n",
       "      <td>...</td>\n",
       "      <td>1157.0</td>\n",
       "      <td>1.563475e+09</td>\n",
       "      <td>mutual</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9882.0</td>\n",
       "      <td>4988962.0</td>\n",
       "      <td>5143</td>\n",
       "      <td>6038</td>\n",
       "      <td>2056</td>\n",
       "      <td>12414128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43523</th>\n",
       "      <td>38270</td>\n",
       "      <td>570910x2312x1</td>\n",
       "      <td>570910</td>\n",
       "      <td>80c2d7b8c5954a6da799aa9d2fc9b850b08813e6d0003e...</td>\n",
       "      <td>bc1qtv35eslch42v8zmmmxkgft4le477alh5pffd23rsnm...</td>\n",
       "      <td>592652.0</td>\n",
       "      <td>8a5bcde5ed26379f74137e67ff3d9726a865b36e3b5a46...</td>\n",
       "      <td>0250373555232cec757ea141273e75381c84cc3ab22f1e...</td>\n",
       "      <td>03bd932f13a27ea286da6f7d1e2bbb19d85e75bc2d83cb...</td>\n",
       "      <td>150000</td>\n",
       "      <td>...</td>\n",
       "      <td>13681.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unused</td>\n",
       "      <td>0.0</td>\n",
       "      <td>136319.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38270</td>\n",
       "      <td>6038</td>\n",
       "      <td>3019</td>\n",
       "      <td>18228722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43142</th>\n",
       "      <td>38275</td>\n",
       "      <td>571235x1611x0</td>\n",
       "      <td>571235</td>\n",
       "      <td>a8739298f3316e97700138e4d2fd1e820c04b9fe8658e6...</td>\n",
       "      <td>bc1qpyj4xx2hn2s3re0pcyt5056qxh6st6r0ktrzj6gvap...</td>\n",
       "      <td>592640.0</td>\n",
       "      <td>5849ebf6e79e53bedcc929ee73373cfa96a396ae2ace41...</td>\n",
       "      <td>0250373555232cec757ea141273e75381c84cc3ab22f1e...</td>\n",
       "      <td>03bceda62f96db8ca4deb9156e97542c079c0747d53578...</td>\n",
       "      <td>433758</td>\n",
       "      <td>...</td>\n",
       "      <td>16445.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unused</td>\n",
       "      <td>0.0</td>\n",
       "      <td>417313.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38275</td>\n",
       "      <td>6038</td>\n",
       "      <td>5120</td>\n",
       "      <td>30914560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42400</th>\n",
       "      <td>27710</td>\n",
       "      <td>572058x154x0</td>\n",
       "      <td>572058</td>\n",
       "      <td>b84ff5ba05fd81d495c6f0c0d700473dcc1b7f18232180...</td>\n",
       "      <td>bc1q0n066dazjdckzajzrhsf54hugxyym0fk6uaspucpmh...</td>\n",
       "      <td>592640.0</td>\n",
       "      <td>712fa0d6825ab1f6d0d95e0d2d03f02559c8084c1ef408...</td>\n",
       "      <td>0250373555232cec757ea141273e75381c84cc3ab22f1e...</td>\n",
       "      <td>03a503d8e30f2ff407096d235b5db63b4fcf3f89a653ac...</td>\n",
       "      <td>125000</td>\n",
       "      <td>...</td>\n",
       "      <td>5923.0</td>\n",
       "      <td>1.563172e+09</td>\n",
       "      <td>force</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2598.0</td>\n",
       "      <td>2598.0</td>\n",
       "      <td>27710</td>\n",
       "      <td>6038</td>\n",
       "      <td>1232</td>\n",
       "      <td>7438816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36664</th>\n",
       "      <td>33409</td>\n",
       "      <td>575386x1941x0</td>\n",
       "      <td>575386</td>\n",
       "      <td>cde60e2b0ac91198ad752e5b2663b507308977dea201b0...</td>\n",
       "      <td>bc1q4ckmcprfkz5khgm53f84whn4rl0xmysg77m622zwzf...</td>\n",
       "      <td>591787.0</td>\n",
       "      <td>4d78495878f817f35fe7cd2b79e4e6d7784cad95e376b7...</td>\n",
       "      <td>0250373555232cec757ea141273e75381c84cc3ab22f1e...</td>\n",
       "      <td>0322877910cd911c22dd95eb1e3ebc1b0923e32c47ba88...</td>\n",
       "      <td>202976</td>\n",
       "      <td>...</td>\n",
       "      <td>4412.0</td>\n",
       "      <td>1.564604e+09</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33409</td>\n",
       "      <td>6038</td>\n",
       "      <td>1501</td>\n",
       "      <td>9063038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32092</th>\n",
       "      <td>39570</td>\n",
       "      <td>580010x1282x0</td>\n",
       "      <td>580010</td>\n",
       "      <td>8fb25ff585f7accff65894d39ba4d70e584062fb290ce2...</td>\n",
       "      <td>bc1qnf3geqtr2fgumx2wucxcuwyzym73l4h9u88qwn0302...</td>\n",
       "      <td>592653.0</td>\n",
       "      <td>418814bd87be82a3056abbb0478c6152fb387f7399fff3...</td>\n",
       "      <td>0250373555232cec757ea141273e75381c84cc3ab22f1e...</td>\n",
       "      <td>03e61366e8a7e9e1300d2211200d27f295b921ebddd9f1...</td>\n",
       "      <td>50000</td>\n",
       "      <td>...</td>\n",
       "      <td>6392.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39570</td>\n",
       "      <td>6038</td>\n",
       "      <td>4109</td>\n",
       "      <td>24810142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29267</th>\n",
       "      <td>43188</td>\n",
       "      <td>583325x1896x1</td>\n",
       "      <td>583325</td>\n",
       "      <td>53d8641c35f4f7013ebc3303020b835d56fba7d64d63ab...</td>\n",
       "      <td>bc1qp8trq0ml6shj042c95lpdtntxwjnhnq3z2xzhr5qcz...</td>\n",
       "      <td>592640.0</td>\n",
       "      <td>c482413c88d8bbb818f555a34cada238e89991e4b41f9f...</td>\n",
       "      <td>0250373555232cec757ea141273e75381c84cc3ab22f1e...</td>\n",
       "      <td>0316baa4dbda59fa6d392ba0804d616d0fc36db0cf7285...</td>\n",
       "      <td>600000</td>\n",
       "      <td>...</td>\n",
       "      <td>5921.0</td>\n",
       "      <td>1.563172e+09</td>\n",
       "      <td>unused</td>\n",
       "      <td>0.0</td>\n",
       "      <td>594079.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43188</td>\n",
       "      <td>6038</td>\n",
       "      <td>6068</td>\n",
       "      <td>36638584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0 short_channel_id  open_block  \\\n",
       "72475           0     505149x622x0      505149   \n",
       "71541       36324    525150x3092x0      525150   \n",
       "70663       36396     534878x239x0      534878   \n",
       "67488        3758    551133x1292x0      551133   \n",
       "65967        5143    554406x1810x0      554406   \n",
       "43523       38270    570910x2312x1      570910   \n",
       "43142       38275    571235x1611x0      571235   \n",
       "42400       27710     572058x154x0      572058   \n",
       "36664       33409    575386x1941x0      575386   \n",
       "32092       39570    580010x1282x0      580010   \n",
       "29267       43188    583325x1896x1      583325   \n",
       "\n",
       "                                        open_transaction  \\\n",
       "72475  f6bc767df9148ebf76d2b9baf4eb46e3230712c2bf5a51...   \n",
       "71541  d51eed0501cfb3ed1a6f0d1ee7a9fe85037eb489fd8211...   \n",
       "70663  a7b2e480775f3baf2602e18abfde05dfcb4fc72719087a...   \n",
       "67488  7c021f4fe8fd67f7619910958c8ad0b9d89dde8dd8436d...   \n",
       "65967  2684410b0c4aa5a33b6ba315fce2ff5af20c57a83b267d...   \n",
       "43523  80c2d7b8c5954a6da799aa9d2fc9b850b08813e6d0003e...   \n",
       "43142  a8739298f3316e97700138e4d2fd1e820c04b9fe8658e6...   \n",
       "42400  b84ff5ba05fd81d495c6f0c0d700473dcc1b7f18232180...   \n",
       "36664  cde60e2b0ac91198ad752e5b2663b507308977dea201b0...   \n",
       "32092  8fb25ff585f7accff65894d39ba4d70e584062fb290ce2...   \n",
       "29267  53d8641c35f4f7013ebc3303020b835d56fba7d64d63ab...   \n",
       "\n",
       "                                                 address  close_block  \\\n",
       "72475  bc1qjmg6ev344fenh3zhg0yjl6hyvxpxluw6x9nn2a5lv4...     592638.0   \n",
       "71541  bc1qxy09e7wmkdm6g699zpjdtxepym5l4wqfc422ks3a8w...     592652.0   \n",
       "70663  bc1qky60ee6h0l2n8rz2rz7dvpp3ez3eazmxtv0phhw5w9...     592652.0   \n",
       "67488  bc1qr7m6r2uweu3jjwjwth7umwnjn3zyajdp5qf8fuy8kp...     592641.0   \n",
       "65967  bc1qev2wed4sk02g3rmh9ptpalnk3lz020jkv2m8hu0rn6...     592641.0   \n",
       "43523  bc1qtv35eslch42v8zmmmxkgft4le477alh5pffd23rsnm...     592652.0   \n",
       "43142  bc1qpyj4xx2hn2s3re0pcyt5056qxh6st6r0ktrzj6gvap...     592640.0   \n",
       "42400  bc1q0n066dazjdckzajzrhsf54hugxyym0fk6uaspucpmh...     592640.0   \n",
       "36664  bc1q4ckmcprfkz5khgm53f84whn4rl0xmysg77m622zwzf...     591787.0   \n",
       "32092  bc1qnf3geqtr2fgumx2wucxcuwyzym73l4h9u88qwn0302...     592653.0   \n",
       "29267  bc1qp8trq0ml6shj042c95lpdtntxwjnhnq3z2xzhr5qcz...     592640.0   \n",
       "\n",
       "                                       close_transaction  \\\n",
       "72475  82cb2ea2a06c8c453d8b9ca08e17bbefe87225aa380b2d...   \n",
       "71541  e7b02ed49b97b0ed4cca822edcc02313534f2c266cb074...   \n",
       "70663  8f470554f7fe76cfb33e15319387451aef97cbc04a6110...   \n",
       "67488  9149e4ff28e61e8169d3eeac97ec8c2444660642e38d35...   \n",
       "65967  1b44f65c02d5030b9e622919a92dba176c30e115845d93...   \n",
       "43523  8a5bcde5ed26379f74137e67ff3d9726a865b36e3b5a46...   \n",
       "43142  5849ebf6e79e53bedcc929ee73373cfa96a396ae2ace41...   \n",
       "42400  712fa0d6825ab1f6d0d95e0d2d03f02559c8084c1ef408...   \n",
       "36664  4d78495878f817f35fe7cd2b79e4e6d7784cad95e376b7...   \n",
       "32092  418814bd87be82a3056abbb0478c6152fb387f7399fff3...   \n",
       "29267  c482413c88d8bbb818f555a34cada238e89991e4b41f9f...   \n",
       "\n",
       "                                                   node0  \\\n",
       "72475  0250373555232cec757ea141273e75381c84cc3ab22f1e...   \n",
       "71541  0250373555232cec757ea141273e75381c84cc3ab22f1e...   \n",
       "70663  0250373555232cec757ea141273e75381c84cc3ab22f1e...   \n",
       "67488  0250373555232cec757ea141273e75381c84cc3ab22f1e...   \n",
       "65967  0250373555232cec757ea141273e75381c84cc3ab22f1e...   \n",
       "43523  0250373555232cec757ea141273e75381c84cc3ab22f1e...   \n",
       "43142  0250373555232cec757ea141273e75381c84cc3ab22f1e...   \n",
       "42400  0250373555232cec757ea141273e75381c84cc3ab22f1e...   \n",
       "36664  0250373555232cec757ea141273e75381c84cc3ab22f1e...   \n",
       "32092  0250373555232cec757ea141273e75381c84cc3ab22f1e...   \n",
       "29267  0250373555232cec757ea141273e75381c84cc3ab22f1e...   \n",
       "\n",
       "                                                   node1  satoshis  ...  \\\n",
       "72475  02ef61a252f9504a42fc264a28476f44cea0711a44b2da...    300000  ...   \n",
       "71541  03bf7528f412ef621ac4898e4815a7c94d427e719eefba...     10000  ...   \n",
       "70663  032e04b67641c00444af1d83145c0b63bac8316a6afb8f...     50000  ...   \n",
       "67488  02ad6fb8d693dc1e4569bcedefadf5f72a931ae027dc0f...    100000  ...   \n",
       "65967  0279c22ed7a068d10dc1a38ae66d2d6461e269226c6025...   5000001  ...   \n",
       "43523  03bd932f13a27ea286da6f7d1e2bbb19d85e75bc2d83cb...    150000  ...   \n",
       "43142  03bceda62f96db8ca4deb9156e97542c079c0747d53578...    433758  ...   \n",
       "42400  03a503d8e30f2ff407096d235b5db63b4fcf3f89a653ac...    125000  ...   \n",
       "36664  0322877910cd911c22dd95eb1e3ebc1b0923e32c47ba88...    202976  ...   \n",
       "32092  03e61366e8a7e9e1300d2211200d27f295b921ebddd9f1...     50000  ...   \n",
       "29267  0316baa4dbda59fa6d392ba0804d616d0fc36db0cf7285...    600000  ...   \n",
       "\n",
       "      close_fee   last_update  close_type  close_htlc_count  close_balance_a  \\\n",
       "72475     184.0  1.563172e+09      mutual               0.0           3570.0   \n",
       "71541    8762.0           NaN      unused               0.0           1238.0   \n",
       "70663    3806.0           NaN      unused               0.0          46194.0   \n",
       "67488    1417.0  1.563172e+09      mutual               0.0           2399.0   \n",
       "65967    1157.0  1.563475e+09      mutual               0.0           9882.0   \n",
       "43523   13681.0           NaN      unused               0.0         136319.0   \n",
       "43142   16445.0           NaN      unused               0.0         417313.0   \n",
       "42400    5923.0  1.563172e+09       force               0.0           2598.0   \n",
       "36664    4412.0  1.564604e+09     unknown               0.0              0.0   \n",
       "32092    6392.0           NaN     unknown               0.0              0.0   \n",
       "29267    5921.0  1.563172e+09      unused               0.0         594079.0   \n",
       "\n",
       "       close_balance_b dec_id  node0_id  node1_id  node_pair  \n",
       "72475         296246.0      0      6038      5314   32085932  \n",
       "71541              0.0  36324      6038      1241    7493158  \n",
       "70663              0.0  36396      6038      5515   33299570  \n",
       "67488          96184.0   3758      6038      4998   30177924  \n",
       "65967        4988962.0   5143      6038      2056   12414128  \n",
       "43523              0.0  38270      6038      3019   18228722  \n",
       "43142              0.0  38275      6038      5120   30914560  \n",
       "42400           2598.0  27710      6038      1232    7438816  \n",
       "36664              0.0  33409      6038      1501    9063038  \n",
       "32092              0.0  39570      6038      4109   24810142  \n",
       "29267              0.0  43188      6038      6068   36638584  \n",
       "\n",
       "[11 rows x 24 columns]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort and visualize DataFrame\n",
    "\n",
    "decisions_df.sort_values(by=['open_block'],inplace=True,ascending=True)\n",
    "print('Size of DataFrame in Memory:{}'.format(sys.getsizeof(decisions_df)))\n",
    "# Check specific channel id\n",
    "#decisions_df[decisions_df['short_channel_id']=='513675x2245x0'].head()\n",
    "\n",
    "decisions_df[decisions_df['node0_id']==6038].sort_values(by=['open_block'],ascending=True).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TESTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Lazy Graph extract\n",
    "blocks_att=[]\n",
    "for i in range(len(graph_snapshots)):\n",
    "    graph_i=dask.compute(graph_snapshots[i])\n",
    "    block=graph_i.graph['block']\n",
    "    blocks_att.append(block)\n",
    "\n",
    "print(blocks_att)\n",
    "\n",
    "#graph_snapshots=dask.compute(*graph_snapshots)\n",
    "#block=graph_snapshots[0].graph['block']\n",
    "    \n",
    "#print(len(graph_snapshots[5]))\n",
    "#print(graph_snapshots[3].graph['block'])\n",
    "\n",
    "# Delayed testing\n",
    "#results = dask.compute(*futures)\n",
    "#graphs=dask.compute(*graph_snapshots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Comparative Analysis\n",
    "\n",
    "In order to understand the potential motivations behind each decision we analyze each decission (opening or closure of a channel) independently from the perspective of each of the participants in the decission, which we'll call the node under analysis. For each decission we extract or compute the following information: \n",
    "\n",
    "Betweenness centrality measures how central is a network to the flow of information in a network. In the case of the Lightning Network the higher the betweenness centrality of a node, the more transactions (messages) that are routed through it. In particular, we will use a measure of betweenness centrality defined in (Brandes and Fleischer 2005 - https://link.springer.com/chapter/10.1007/978-3-540-31856-9_44) that models infomation through a network, as electric current, efficiently and not only considering shortest path. This allows us to account for the fact that not all transactions travel through shortes path given that there are fee and capacity considerations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Measurments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Property measurement for a collection of graphs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEW\n",
    "# Function to calculate base measurement for every graph in snapshot\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "collection_measure\n",
    "    Iterates over graph snapshots and calculates measurement for every node in each of the graphs.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "g_snapshots : list\n",
    "    List of delayed nx graph elements contianing graph snapshots\n",
    "    \n",
    "\n",
    "measurement: string\n",
    "    Name of the type of measurment that will applied to the graph. (See graph_measurement function for options)\n",
    "\n",
    "\n",
    "\n",
    "Returns\n",
    "-------\n",
    "snapshot_mes_dic: dic\n",
    "    Dictionary with blocks as keys and dic of measurements as values\n",
    "\n",
    "\"\"\"\n",
    "def collection_measure(bucket,graph_keys,measurement):\n",
    "    \n",
    "    snapshot_mes_list=[]\n",
    "    \n",
    "    \n",
    "    \n",
    "    with tqdm(total=len(graph_keys)) as pbar:\n",
    "        for key in graph_keys:\n",
    "\n",
    "\n",
    "            measurement_input=(key,measurement,'capacity',bucket)\n",
    "\n",
    "            b_g_tuple=dask.delayed(graph_measurement)(measurement_input)\n",
    "            snapshot_mes_list.append(b_g_tuple)\n",
    "            pbar.update(1)\n",
    "\n",
    "    futures = dask.persist(*snapshot_mes_list)\n",
    "    \n",
    "    start=time.time()\n",
    "    snapshot_mes_list = dask.compute(*futures)\n",
    "    #snapshot_mes_list = dask.compute(*snapshot_mes_list)\n",
    "    snapshot_mes_dic={record[0]:record[1] for record in snapshot_mes_list}\n",
    "    end=time.time()\n",
    "    print('Compute in seconds: {}'.format(end-start))\n",
    "    \n",
    "    return snapshot_mes_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate base measurement for every graph in snapshot\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "collection_measure\n",
    "    Iterates over graph snapshots and calculates measurement for every node in each of the graphs.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "g_snapshots : list\n",
    "    List of delayed nx graph elements contianing graph snapshots\n",
    "    \n",
    "\n",
    "measurement: string\n",
    "    Name of the type of measurment that will applied to the graph. (See graph_measurement function for options)\n",
    "\n",
    "\n",
    "\n",
    "Returns\n",
    "-------\n",
    "snapshot_mes_dic: dic\n",
    "    Dictionary with blocks as keys and dic of measurements as values\n",
    "\n",
    "\n",
    "def collection_measure(g_snapshots,measurement):\n",
    "    \n",
    "    snapshot_mes_list=[]\n",
    "   \n",
    "    for g in graph_snapshots:\n",
    "      \n",
    "        \n",
    "        measurement_input=(g,measurement,'capacity')\n",
    "        \n",
    "        b_g_tuple=dask.delayed(graph_measurement)(measurement_input)\n",
    "        snapshot_mes_list.append(b_g_tuple)\n",
    "        \n",
    "\n",
    "    #futures = dask.persist(*snapshot_mes_list)\n",
    "    \n",
    "    start=time.time()\n",
    "    #snapshot_mes_list = dask.compute(*futures)\n",
    "    snapshot_mes_list = dask.compute(*snapshot_mes_list)\n",
    "    snapshot_mes_dic={record[0]:record[1] for record in snapshot_mes_list}\n",
    "    end=time.time()\n",
    "    print('Compute in seconds: {}'.format(end-start))\n",
    "    \n",
    "    return snapshot_mes_dic\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Property measurment for a single graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "graph_measurement\n",
    "    Performs specific graph measurement \n",
    "\n",
    "Parameters\n",
    "----------\n",
    "measurment_input: tuple\n",
    "    g : nx graph\n",
    "        NetworkX graph object over which measurment will be calculated for each node\n",
    "\n",
    "    measurment: string\n",
    "        Type of measurement to be performed on graph\n",
    "        \n",
    "    weight: string\n",
    "        Edge attribute to be used as weight\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "g_dir: dir\n",
    "    Dictionary with measurment values for each node\n",
    "\"\"\"\n",
    "\n",
    "def graph_measurement(measurment_input):\n",
    "    \n",
    "    # Extract inputs\n",
    "    key=measurment_input[0]\n",
    "    measurement=measurment_input[1]\n",
    "    weight=measurment_input[2]\n",
    "    bucket=measurment_input[3]\n",
    "    \n",
    "    # Download graph\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.resource('s3')\n",
    "    response = s3.Object(bucket_name=bucket, key=key).get()\n",
    "    g=pickle.loads(response['Body'].read())\n",
    "    \n",
    "    # Extract Block\n",
    "    \n",
    "    block=g.graph['block']\n",
    "   \n",
    "    \n",
    "    if measurement=='current_betweeness_full':\n",
    "        g_dir=nx.algorithms.centrality.current_flow_betweenness_centrality(g,weight=weight)\n",
    "    \n",
    "    elif measurement=='current_betweeness':\n",
    "        g_dir=nx.algorithms.centrality.approximate_current_flow_betweenness_centrality(g,weight=weight)\n",
    "    \n",
    "    elif measurement=='current_closeness':\n",
    "        g_dir=nx.algorithms.centrality.current_flow_closeness_centrality(g,weight=weight)\n",
    "    \n",
    "    elif measurement=='closeness':\n",
    "        g_dir=nx.closeness_centrality(g)\n",
    "        \n",
    "    elif measurement=='clustering':\n",
    "        g_dir=nx.clustering(g,weight=weight)\n",
    "        \n",
    "    elif measurement=='node_count':\n",
    "        g_dir=len(g.nodes())\n",
    "        \n",
    "    elif measurement=='channels':\n",
    "        g_dir=dict(list(g.degree(g.nodes())))\n",
    "    \n",
    "    elif measurement=='capacity':\n",
    "        g_dir=dict(list(g.degree(g.nodes(),weight=weight)))\n",
    "        \n",
    "    elif measurement=='age': \n",
    "        \n",
    "        # Get node_details from S3 \n",
    "        opens_file = s3.Object(bucket_name=bucket, key='node_details.p').get()\n",
    "        node_details = pickle.loads(opens_file['Body'].read())\n",
    "        \n",
    "        # Create dic with node's age in blocks\n",
    "        g_dir={node:block-node_details[node]['birth_block'] for node in list(g.nodes())} \n",
    "        \n",
    "        \n",
    "    elif measurement=='capacity_growth':  \n",
    "        g_dir=capacity_growth (weight,bucket,g,block,s3,block_frame=3600)\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Safe graph processing to S3\n",
    "    \n",
    "    extraction_id=key.split('/')[1].split('_')[0]\n",
    "    block=key.split('/')[2].split('.')[0]\n",
    "    key_out='graph_snapshots/'+extraction_id+'_connected/.data_transformations/'+measurement+'/raw_score/'+block+'.pkl'\n",
    "    pickle_byte_obj = pickle.dumps(g_dir) \n",
    "    response=s3.Object(bucket,key_out).put(Body=pickle_byte_obj)['ResponseMetadata']['HTTPStatusCode']\n",
    "    \n",
    "    \n",
    "    \n",
    "    return (block,response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "capacity_growth\n",
    "    Calculates how much has capacity grown (or decreased) for all nodes in a graph\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "weight: str\n",
    "    Node property that will be used to weight the calculation.\n",
    "    \n",
    "bucket: str\n",
    "    S3 bucket where data is stored\n",
    "\n",
    "g: NetworkX graph\n",
    "    Graph for which the calculation will be computed\n",
    "    \n",
    "block: int\n",
    "    Block number corresponding to the selected graph\n",
    "\n",
    "s3: S3 session object\n",
    "    S3 session object for the boto3 api\n",
    "    \n",
    "block_frame: int\n",
    "    The amount of blocks into the past that will be considered to calculate growth\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "g_dir: dir\n",
    "    Dictionary with measurment values for each node\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def capacity_growth (weight,bucket,g,block,s3,block_frame=3600):\n",
    "    \n",
    "    # Initialize g_dir items and min_block\n",
    "    \n",
    "    g_dir={}\n",
    "    min_block=block-block_frame\n",
    "    \n",
    "    \n",
    "    # Get graph nodes\n",
    "    nodes=list(g.nodes())\n",
    "    \n",
    "    # Load decisions DataFrame\n",
    "    \n",
    "    decisions_load = s3.Object(bucket_name=bucket, key='decisions_df.csv').get()\n",
    "    decisions_df=pd.read_csv(io.BytesIO(decisions_load['Body'].read()))\n",
    "    \n",
    "    with tqdm(total=len(nodes),disable=True) as pbar:\n",
    "        \n",
    "        for node in nodes:\n",
    "\n",
    "            # Find all channel creations and closerues in block frame\n",
    "            opens_blockframe_node0=decisions_df[(decisions_df['node0_id']==node) & (decisions_df['open_block']>=min_block) & (decisions_df['open_block']<=block)]['satoshis']\n",
    "            opens_blockframe_node1=decisions_df[(decisions_df['node1_id']==node) & (decisions_df['open_block']>=min_block) & (decisions_df['open_block']<=block)]['satoshis']\n",
    "            closes_blockframe_node0=decisions_df[(decisions_df['node0_id']==node) & (decisions_df['close_block']>=min_block) & (decisions_df['close_block']<=block)]['satoshis']\n",
    "            closes_blockframe_node1=decisions_df[(decisions_df['node1_id']==node) & (decisions_df['close_block']>=min_block) & (decisions_df['close_block']<=block)]['satoshis']\n",
    "\n",
    "            # Calculate growth by adding capacity created in block frame and subtracting capacity lost\n",
    "            if weight==1: #Unweighted calculation\n",
    "                gain=opens_blockframe_node0.count()+opens_blockframe_node1.count()\n",
    "                loss=closes_blockframe_node0.count()+closes_blockframe_node1.count()\n",
    "\n",
    "            else:\n",
    "                gain=opens_blockframe_node0.sum()+opens_blockframe_node1.sum()\n",
    "                loss=closes_blockframe_node0.sum()+closes_blockframe_node1.sum()\n",
    "\n",
    "            # Calculate growth and save to dir\n",
    "            net_growth=gain-loss\n",
    "            g_dir[node]=net_growth\n",
    "            pbar.update(1)\n",
    "        \n",
    "\n",
    "    return g_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEST: Weighted capacity function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block selected:532022\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8fb351046764ca6a674e682973588e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=683.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dic was saved correctly. Sample below:\n",
      "[(6038, 0), (5314, 0), (934, 0), (3023, 0), (3436, 3131000), (3310, 0), (422, 0), (1912, 0), (5154, 0), (4688, 0)]\n"
     ]
    }
   ],
   "source": [
    "# Define test parameters\n",
    "\n",
    "\n",
    "measurement='capacity_growth'\n",
    "weight='capacity'\n",
    "g_test,nodes_test,g_dic_test,block=load_graph_measurement(extraction_id,measurement,weight,blocks,extract_keys,test_ix=-1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3436, 3131000), (1331, 5500000), (6156, 520000), (4998, 4485183), (4527, 200000), (2757, 1677721), (3065, 1200000), (346, 19061712), (2724, 1491661), (2460, 100000), (2476, 9564725), (1893, 300000), (6418, 15737856), (6599, 93799), (4534, 805272), (5641, 97000), (2295, 1200000), (6832, 792562), (7711, 1500000), (1410, 2700000), (1514, 89141), (6296, 1267024), (7608, 2541273), (7631, 182031), (3271, 21417), (2674, 9193000), (3382, 1872962), (1220, 20000), (4580, 500000), (6215, 11315423), (6363, 10000), (4620, 7124495), (7673, 520550), (4639, 20000), (4426, 553341), (5738, 1319859), (7259, 4490000), (6924, 343070), (448, 174997), (227, 675364), (326, 2000), (5372, 77107), (4490, 10000), (2739, 50000), (2881, 20000), (4819, 20000), (4427, 500000), (1172, 80000), (6378, 100000), (1120, 1777721), (5634, 5000), (2300, 500000), (2973, 34003), (6249, 60000), (3, 27529), (7606, 743866), (415, 389026), (5601, 3390969), (5406, 1500000), (5495, 600000), (1257, 500000), (2512, 350000), (7073, 443002), (1082, 318000), (595, 200000), (7379, 800000), (7566, 132765), (6400, 2000000), (3846, 682031), (3283, 6681303), (6975, 40928), (3635, 900000), (1350, 28764), (6646, 740000), (869, 77130), (4259, 165826), (99, 500000), (3351, 1677721), (5242, 669000), (3107, 200000), (7533, 357886), (6465, 1646300), (6435, 60000), (5063, 656735), (1469, 8974725), (5797, 86000), (764, 188831), (5016, 500000), (4175, 1000000), (6748, 2000000), (5321, 7455151), (275, 2000000), (6553, 1000000), (4950, 172552), (4571, 1200000), (3224, 200000), (1182, 6379999), (4701, 1014003), (3030, 20000), (5976, 500000), (6657, 1950000), (2604, 300000), (1901, 1999999), (2686, 300000), (3181, 1400000), (285, 1581273), (2247, 810629), (5796, 579000), (1131, 143366), (3841, 420177), (2789, 400000), (3904, 2293799), (6757, 2000000), (4896, 550000), (2785, 80800), (4516, 7930183), (1239, 666741), (4297, 600000), (180, 400000), (6303, 1133341), (62, 1227139), (5235, 400000), (174, 20000), (3979, 182519), (7194, 100000), (5311, 500000), (3796, 165826), (3882, 2700000), (953, 540000), (4409, 100000), (7039, 20000), (6160, 7400000), (7250, 616350), (5429, 122962), (5583, 400000), (7696, 21000), (252, 10000000), (4192, 3000000), (5005, 400000), (5066, 384510), (5899, 9193000), (5124, 480000), (3996, 40928), (4899, 26591), (1910, 176716), (6787, 500000), (1631, 222273), (4043, 150000), (3589, 50000), (424, 100000), (2696, 700000), (6408, 2175926), (1603, 1097000), (2769, 500000), (4720, 6750000), (795, 100000), (5209, 1600000), (5393, 220000), (3844, 169884), (1813, 482976), (2806, 20000), (4474, 100000), (6112, 240000), (6568, 2350000), (1503, 134510), (619, 150000), (5654, 491000), (4723, 150705), (4928, 55381), (6075, 86000), (7205, 500000), (3808, 1000000), (7125, 2226296), (2654, 22334), (4813, 500000), (5227, 872816), (6063, 852625), (6023, 70000), (1714, 140000), (1333, 280000), (5599, 16637691), (5933, 100000), (420, 318000), (6267, 295300), (4776, 1677721), (2466, 120000), (2994, 500000), (4735, 500000), (3474, 900000), (7734, 614869), (6061, 140863)]\n"
     ]
    }
   ],
   "source": [
    "positive=[t for t in list(g_dic_test.items()) if t[1]>0]\n",
    "print(positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_growth(g_test,g_dic_test,block,channel_opens,channel_closures):\n",
    "\n",
    "    test_list=[]\n",
    "    error_nodes=[]\n",
    "\n",
    "\n",
    "    min_block=block-3600\n",
    "\n",
    "    relevant_opens=[t[0] for t in list(channel_opens.items()) if (t[0]>=min_block and t[0]<=block)]\n",
    "    relevant_closures=[t[0] for t in list(channel_closures.items()) if (t[0]>=min_block and t[0]<=block)]\n",
    "    relevant_blocks=sorted(list(set(relevant_opens).union(set(relevant_closures))))\n",
    "\n",
    "\n",
    "    # Get graph nodes\n",
    "    nodes=list(g_test.nodes())\n",
    "\n",
    "        # Calculate net change for each node in each of the relevant blocks\n",
    "    with tqdm(total=len(nodes)) as pbar:\n",
    "        for node in nodes:\n",
    "            net_change=0\n",
    "            for block in relevant_blocks:\n",
    "\n",
    "                # Create list of nodes involved in channel opens and count how many times a node appears\n",
    "\n",
    "                if weight==1:\n",
    "                    list_block_opens=[[t[0],t[1]] for t in channel_opens[block] if (t[0]==node or t[1]==node)]\n",
    "                    total_weights=len(list_block_opens)\n",
    "\n",
    "                else:\n",
    "                    list_block_opens=[[t[0],t[1],t[2][weight]] for t in channel_opens[block] if (t[0]==node or t[1]==node)]\n",
    "                    total_weights=np.array([l[2] for l in list_block_opens]).sum()\n",
    "\n",
    "\n",
    "                net_change+=total_weights\n",
    "\n",
    "\n",
    "                # Create list of nodes involved in channel closures and count how many times a node appears\n",
    "\n",
    "                if weight==1:\n",
    "                    list_block_closures=[[t[0],t[1]] for t in channel_closures[block] if (t[0]==node or t[1]==node)]\n",
    "                    total_weights=len(list_block_opens)\n",
    "\n",
    "                else:\n",
    "                    list_block_closures=[[t[0],t[1],t[2][weight]] for t in channel_closures[block] if (t[0]==node or t[1]==node)]\n",
    "                    total_weights=np.array([l[2] for l in list_block_closures]).sum()\n",
    "\n",
    "\n",
    "                net_change-=total_weights\n",
    "\n",
    "            #Retrieve recorded growth for node\n",
    "\n",
    "            recorded_growth=g_dic_test[node] \n",
    "\n",
    "            # Check if growth match and populate test_list accordingly\n",
    "            if net_change==recorded_growth:\n",
    "                test_list.append(1)\n",
    "            else:\n",
    "                test_list.append(0)\n",
    "                error_nodes.append((node,recorded_growth,net_growth))\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "        # Add up all passed tests\n",
    "        tests_passed=np.array(test_list).sum()\n",
    "\n",
    "        # Print out statements based on test results\n",
    "    if tests_passed==len(nodes):\n",
    "        print('Growth calculated correctly for all nodes')\n",
    "\n",
    "\n",
    "    else: \n",
    "        print('Growth failed to be correctly calculated for {} nodes'.format(len(list(g_test.nodes()))-tests_passed))\n",
    "        if len(error_nodes)>10:\n",
    "            print('Some nodes with errors (node,recorded,actual)')\n",
    "            print(error_nodes[:10])\n",
    "\n",
    "        else:\n",
    "            print('Some nodes with errors (node,recorded,actual)')\n",
    "            print(error_nodes[:len(error_nodes)])\n",
    "        \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe37b5c85394f6193b009e7a0b12ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=683.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Growth calculated correctly for all nodes\n"
     ]
    }
   ],
   "source": [
    "test_growth(g_test,g_dic_test,block,channel_opens,channel_closures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEST: Test for age calculation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bafd82b8e064c6caebc03e31e9cdd89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5745.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Age calculated correctly for all nodes\n",
      "Example age for nodes in block 617169\n",
      "[(5314, 112020), (934, 110767), (3023, 110767), (3452, 110322), (576, 110322), (3436, 109094), (3310, 109094), (4223, 109079), (422, 108849), (1912, 108849)]\n"
     ]
    }
   ],
   "source": [
    "test_ix=-100\n",
    "test_block=blocks[test_ix]\n",
    "measurement='age'\n",
    "g_key=graph_keys[test_ix]\n",
    "block,response_test=graph_measurement((g_key,'age',None,bucket))\n",
    "\n",
    "#Load graph\n",
    "\n",
    "response = s3.Object(bucket_name=bucket, key=g_key).get()\n",
    "g_test=pickle.loads(response['Body'].read())\n",
    "nodes_test=list(g_test.nodes())\n",
    "\n",
    "if response_test==200:\n",
    "    key_test='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/raw_score/'+str(test_block)+'.pkl'\n",
    "    g_age_test_load = s3.Object(bucket_name=bucket, key=key_test).get()\n",
    "    g_age_test = pickle.loads(g_age_test_load['Body'].read())\n",
    "    \n",
    "    test_list=[]\n",
    "    error_nodes=[]\n",
    "    # Test that each node age is calculated correctly by looking at the decisions_df\n",
    "    with tqdm(total=len(nodes_test)) as pbar:\n",
    "        \n",
    "        for n in nodes_test:\n",
    "            \n",
    "            # Find creation block by looking at node0 and node1 columns\n",
    "            firstseenas_node0=decisions_df[decisions_df['node0_id']==n]['open_block'].min()\n",
    "            firstseenas_node1=decisions_df[decisions_df['node1_id']==n]['open_block'].min()\n",
    "            \n",
    "            # Correct for nan values, in case node is not present in either column (make it infinite)\n",
    "            fs_list=[firstseenas_node0,firstseenas_node1]\n",
    "            fs_list=[np.inf if np.isnan(i) else i for i in fs_list]\n",
    "            \n",
    "            # Calculate age\n",
    "            actual_age=block-min(fs_list[0],fs_list[1])\n",
    "            recorded_age=g_age_test[n]\n",
    "            \n",
    "            # Check if ages match and populate test_list accordingly\n",
    "            if recorded_age==actual_age:\n",
    "                test_list.append(1)\n",
    "            else:\n",
    "                test_list.append(0)\n",
    "                error_nodes.append((n,recorded_age,actual_age))\n",
    "                \n",
    "            pbar.update(1)\n",
    "            \n",
    "        # Add up all passed tests\n",
    "        tests_passed=np.array(test_list).sum()\n",
    "\n",
    "    \n",
    "# Print out statements based on test results\n",
    "    if tests_passed==len(nodes_test):\n",
    "        print('Age calculated correctly for all nodes')\n",
    "   \n",
    "    \n",
    "    else: \n",
    "        print('Age failed to be correctly calculated for {} nodes'.format(len(list(g_test.nodes()))-tests_passed))\n",
    "        if len(error_nodes)>10:\n",
    "            print('Some nodes with errors (node,recorded_age,actual_age)')\n",
    "            print(error_nodes[:10])\n",
    "        \n",
    "        else:\n",
    "            print('Some nodes with errors (node,recorded_age,actual_age)')\n",
    "            print(error_nodes[:len(error_nodes)])\n",
    "        \n",
    "    \n",
    "    print('Example age for nodes in block {}'.format(test_block))\n",
    "    print(list(g_age_test.items())[:10])\n",
    "    \n",
    "\n",
    "else:\n",
    "    print('Age was not saved correctly')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_closeness_g (bucket,key,block,nodes,p=13.38,estimate_sample=50):\n",
    "                             \n",
    "   \n",
    "    \n",
    "    # Download graph and calculate all nodes\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.resource('s3')\n",
    "    response = s3.Object(bucket_name=bucket, key=key).get()\n",
    "    g=pickle.loads(response['Body'].read())\n",
    "    g_nodes=list(g.nodes())\n",
    "                             \n",
    "    # Estimate c_mid for graph\n",
    "                             \n",
    "    # Calculate estimated rank for each node\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5, 1]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample([1,2,3,4,5,6], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Property measurment for a couple of nodes in a graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "node_measurement\n",
    "    Performs selected graph measurment on specific nodes in graph\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "g : nx graph\n",
    "    NetworkX graph object over which the measurment will be performed\n",
    "\n",
    "measurement: string\n",
    "    Type of measurement to be performend in graph\n",
    "    \n",
    "node0: int\n",
    "    Node id for node 0\n",
    "\n",
    "node1: int\n",
    "    Node id for node 1\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "node_tuple: tuple\n",
    "    Tuple of the form (node0_mes,node1_mes)\n",
    "    \n",
    "    node0_mes: float\n",
    "        Graph measurement for node0\n",
    "    node1_mes: float\n",
    "        Graph measurement for node1\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "def node_measurement(g,measurement,node0,node1):\n",
    "    \n",
    "    measurement_input=(g,measurement,'capacity')\n",
    "    block,g_dir=graph_measurement(measurement_input)\n",
    "    \n",
    "    node0_mes=g_dir[node0]\n",
    "    node1_mes=g_dir[node1]\n",
    "        \n",
    "    # Update marginal values for node0 and node1\n",
    "        \n",
    "    if (g.has_node(node0)): #If connected component of marginal graph contains node0 find betweeness\n",
    "        node0_mes=g_dir[node0]\n",
    "    else: # else update with fixed value\n",
    "        node0_mes=0\n",
    "            \n",
    "    if (g.has_node(node1)): #If connected component of marginal graph contains node1 find betweeness\n",
    "        node1_mes=g_dir[node1]\n",
    "    else: # else update with fixed value\n",
    "        node1_mes=0\n",
    "    \n",
    "    return (node0_mes,node1_mes)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incremental_closeness(bucket,graph_keys,blocks,start_point):\n",
    "    \n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.resource('s3')\n",
    "    counter=0\n",
    "    extraction_id=graph_keys[0].split('/')[1].split('_')[0]\n",
    "    responses=[]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Channel closures\n",
    "    closure_file = s3.Object(bucket_name=bucket, key='channel_closures.p').get()\n",
    "    channel_closures = pickle.loads(closure_file['Body'].read())\n",
    "    \n",
    "    \n",
    "    # Channel openings \n",
    "    opens_file = s3.Object(bucket_name=bucket, key='channel_opens.p').get()\n",
    "    channel_opens = pickle.loads(opens_file['Body'].read())\n",
    "    snapshot_clo={}\n",
    "    \n",
    "    #Initialize graph with all nodes \n",
    "    lastgraph_block=blocks[-1]\n",
    "    last_key='graph_snapshots/'+str(extraction_id)+'/'+str(lastgraph_block)+'.gpickle'\n",
    "    response = s3.Object(bucket_name=bucket, key=last_key).get()\n",
    "    G_final=pickle.loads(response['Body'].read())\n",
    "    nodes_final=list(G_final.nodes())\n",
    "    G=nx.Graph()\n",
    "    G.add_nodes_from(nodes_final)\n",
    "    prev_clo=None\n",
    "    \n",
    "    if start_point>0:\n",
    "        # Get previous graph\n",
    "        inigraph_block=blocks[start_point-1]\n",
    "        ini_key='graph_snapshots/'+str(extraction_id)+'/'+str(inigraph_block)+'.gpickle'\n",
    "        response = s3.Object(bucket_name=bucket, key=ini_key).get()\n",
    "        G_ini=pickle.loads(response['Body'].read())\n",
    "        \n",
    "        # Get previous closeness centrality\n",
    "        measurement='incremental_closeness'\n",
    "        key_clo='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/raw_score/'+str(inigraph_block)+'.pkl'\n",
    "        response = s3.Object(bucket_name=bucket, key=key_clo).get()\n",
    "        prev_clo=pickle.loads(response['Body'].read())\n",
    "      \n",
    "        G.add_edges_from(list(G_ini.edges(data=True)))\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    with tqdm(total=len(range(start_point,len(graph_keys)))) as pbar:\n",
    "        for i  in range(start_point,len(graph_keys)):\n",
    "\n",
    "            \n",
    "            block=blocks[i]\n",
    "            new_edges=channel_opens[block]\n",
    "            closed_edges=channel_closures[block]\n",
    "        \n",
    "\n",
    "            # Incremental closeness calculation for OPENS\n",
    "\n",
    "            with tqdm(total=len(new_edges),disable=True) as pbar1:\n",
    "                for edge in new_edges:\n",
    "\n",
    "                    #If edge exists in previous graph closeness doesn't change\n",
    "                    if G.has_edge(edge[0],edge[1]):\n",
    "                        new_clo=prev_clo\n",
    "                        G.edges[edge[0],edge[1]]['no_channels']+=1\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        new_clo=nx.incremental_closeness_centrality(G,(edge[0],edge[1]),prev_cc=prev_clo,insertion=True)\n",
    "                        G.add_edges_from([edge])\n",
    "                \n",
    "\n",
    "                    prev_clo=new_clo\n",
    "                    pbar1.update(1)\n",
    "                \n",
    "            \n",
    "\n",
    "            # Incremental closeness calculation for CLOSES\n",
    "\n",
    "            with tqdm(total=len(closed_edges),disable=True) as pbar2:\n",
    "                for edge in closed_edges:\n",
    "\n",
    "                    # Verify if existing edges result from multiple channels, if so, only reduce capacity otherwise remove edge\n",
    "                    no_channels=G.edges[edge[0],edge[1]]['no_channels']\n",
    "\n",
    "                    #If edge exists in previous graph closeness doesn't change\n",
    "                    if no_channels>1:\n",
    "                        new_clo=prev_clo\n",
    "                        G.edges[edge[0],edge[1]]['no_channels']-=1\n",
    "\n",
    "                    else:                                    \n",
    "                        new_clo=nx.incremental_closeness_centrality(G,(edge[0],edge[1]),prev_cc=prev_clo,insertion=False)                   \n",
    "                        G.remove_edge(edge[0],edge[1])\n",
    "\n",
    "                    prev_clo=new_clo\n",
    "                    pbar2.update(1)\n",
    "                \n",
    "\n",
    "\n",
    "            # Safe outcome\n",
    "            g_dir=new_clo\n",
    "\n",
    "\n",
    "            # Safe graph processing to S3\n",
    "            \n",
    "            measurement='incremental_closeness'\n",
    "            key_out='graph_snapshots/'+extraction_id+'_connected/.data_transformations/'+measurement+'/raw_score/'+str(block)+'.pkl'\n",
    "            pickle_byte_obj = pickle.dumps(g_dir) \n",
    "            response=s3.Object(bucket,key_out).put(Body=pickle_byte_obj)['ResponseMetadata']['HTTPStatusCode']\n",
    "            #print((block,response))\n",
    "\n",
    "\n",
    "            # Loop updates\n",
    "            pbar1.close()\n",
    "            pbar2.close()\n",
    "            pbar.update(1)\n",
    "            responses.append(response)\n",
    "            \n",
    "        \n",
    "        output={b:res for b,res in zip(blocks,responses)}\n",
    "\n",
    "           \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit sigmoid for p by looking at sample of graphs\n",
    "# estimate mid closeness by samplig closeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sp_closeness(G):\n",
    "    \n",
    "    nodes=list(G.nodes())\n",
    "    n=len(nodes)\n",
    "    sp_matrix=np.zeros((len(nodes),len(nodes)))\n",
    "    sp_matrix=sp_matrix.astype(object)\n",
    "    print(sp_matrix.dtype)\n",
    "    \n",
    "    dic_clo={}\n",
    "    \n",
    "    \n",
    "    for i in range(n):\n",
    "\n",
    "        for j in range(i,n):\n",
    "            # Calculate delayed shortest path\n",
    "            input_tuple=(G,nodes[i],nodes[j])\n",
    "            sp=dask.delayed(len_shortest_path)(input_tuple)\n",
    "            #print(type(sp))\n",
    "            sp_matrix[i][j]=sp\n",
    "            sp_matrix[j][i]=sp\n",
    "        \n",
    "        sp_sum=np.array(dask.compute(*sp_matrix[i].tolist())).sum()\n",
    "        clo_i=n-1/sp_sum\n",
    "        dic_clo[i]=clo_i\n",
    "    return dic_clo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "start=time.time()\n",
    "dic_clo_test=sp_closeness(G_test)\n",
    "end=time.time()\n",
    "print(sp)\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print(list(dic_clo_test.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With aproximate betweeness: 3.782655715942383s \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nmeasurement_input_2=(extract_keys[10000],'current_betweeness_full','capacity',bucket)\\n\\nstart=time.time()\\ntest_full = graph_measurement(measurement_input_2)\\nend=time.time()\\n\\nprint('With full betweeness: {}s '.format(end-start))\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute in seconds: 10.438627481460571\n"
     ]
    }
   ],
   "source": [
    "snapshot_nodes=collection_measure(bucket,extract_keys,'node_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(snapshot_nodes.items())[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Baseline Current Betweeness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute in seconds: 8009.403836488724\n"
     ]
    }
   ],
   "source": [
    "# Calculate Baseline betweeness\n",
    "snapshot_bet=collection_measure(bucket,extract_keys,'current_betweeness')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save to S3 succesful\n"
     ]
    }
   ],
   "source": [
    "# Save baseline current betweeness to S3\n",
    "response=pickle_save_s3(snapshot_bet,blocks,extraction_id,'snapshot_bet')\n",
    "if response['HTTPStatusCode']==200:\n",
    "    print('Save to S3 succesful')\n",
    "else:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Baseline Current Closeness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Baseline current closeness\n",
    "#snapshot_clo=collection_measure(bucket,extract_keys,'current_closeness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate incremental closeness\n",
    "snapshot_clo=incremental_closeness(bucket,extract_keys,blocks,1425+3073)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graph_snapshots/1587447789_connected/535029.gpickle'"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_keys[1402]\n",
    "\n",
    "#channel_opens[1407]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "key='graph_snapshots/1587447789_connected/'+str(blocks[-100])+'.gpickle'\n",
    "response = s3.Object(bucket_name=bucket, key=key).get()\n",
    "G_test=pickle.loads(response['Body'].read())\n",
    "\n",
    "node1=random.choice(list(G.nodes()))\n",
    "node2=random.choice(list(G.nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6306, 2470)\n",
      "0\n",
      "Compute in seconds: 9.965896606445312e-05\n"
     ]
    }
   ],
   "source": [
    "print((node1,node2))\n",
    "#len(G_test.nodes())\n",
    "start=time.time()\n",
    "sp=nx.shortest_path_length(G_test, source=node1, target=node1)\n",
    "end=time.time()\n",
    "print(sp)\n",
    "print('Compute in seconds: {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0.2, 2: 0.2, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0}\n",
      "[(1, 2)]\n",
      "{1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2, 5: 0.0, 6: 0.0}\n",
      "[(1, 2), (3, 4)]\n",
      "{1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2, 5: 0.2, 6: 0.2}\n",
      "[(1, 2), (3, 4), (5, 6)]\n",
      "{1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2, 5: 0.0, 6: 0.0}\n",
      "[(1, 2), (3, 4)]\n"
     ]
    }
   ],
   "source": [
    "G=nx.Graph()\n",
    "G.add_nodes_from([1,2,3,4,5,6])\n",
    "#G.add_edge(2,1)\n",
    "new_clo=nx.incremental_closeness_centrality(G,(2,1),prev_cc=None,insertion=True)\n",
    "G.add_edge(2,1)\n",
    "print(new_clo)\n",
    "print(G.edges())\n",
    "\n",
    "\n",
    "new_clo=nx.incremental_closeness_centrality(G,(3,4),prev_cc=new_clo,insertion=True)\n",
    "G.add_edge(3,4)\n",
    "print(new_clo)\n",
    "print(G.edges())\n",
    "\n",
    "\n",
    "new_clo=nx.incremental_closeness_centrality(G,(5,6),prev_cc=new_clo,insertion=True)\n",
    "G.add_edge(5,6)\n",
    "print(new_clo)\n",
    "print(G.edges())\n",
    "\n",
    "\n",
    "new_clo=nx.incremental_closeness_centrality(G,(5,6),prev_cc=new_clo,insertion=False)\n",
    "G.remove_edge(5,6)\n",
    "\n",
    "print(new_clo)\n",
    "print(G.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save to S3 succesful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec1dca6cf3d443abad6f463e8fad8fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1400.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "% of nodes with positive closeness per Block : 0.9888914970559514\n",
      "Total Blocks: 1400\n"
     ]
    }
   ],
   "source": [
    "# TEST all saved correctly\n",
    "responses=np.array([r for b,r in snapshot_clo.items()])\n",
    "\n",
    "if np.amax(responses)==np.amin(responses): #and len(responses) == len(extract_keys):\n",
    "    print('Save to S3 succesful')\n",
    "else:\n",
    "    print('Error saving to S3')\n",
    "\n",
    "range_test=[]\n",
    "no_blocks=1400\n",
    "    \n",
    "with tqdm(total=no_blocks) as pbar:\n",
    "    for i in range(no_blocks):\n",
    "\n",
    "        measurement='incremental_closeness'    \n",
    "        #rand_block=str(random.choice(blocks[:1200]))    \n",
    "        rand_block=str(blocks[i])\n",
    "\n",
    "        test_key='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/raw_score/'+rand_block+'.pkl'\n",
    "        test_file = s3.Object(bucket_name=bucket, key=test_key).get()\n",
    "        test_object = pickle.loads(test_file['Body'].read())\n",
    "\n",
    "        graph_key='graph_snapshots/'+str(extraction_id)+'_connected/'+str(rand_block)+'.gpickle'\n",
    "        graph_response = s3.Object(bucket_name=bucket, key=graph_key).get()\n",
    "        G=pickle.loads(graph_response['Body'].read())\n",
    "\n",
    "\n",
    "        #print('Example of {} saved for block {}:'.format(measurement,rand_block))\n",
    "        #print(list(test_object.items())[:100]) \n",
    "        #print('Total nodes graph: {}'.format(len(G.nodes())))\n",
    "\n",
    "        connected_nodes=[(b,cent) for b,cent in list(test_object.items()) if cent>0]\n",
    "        #print('Total entries: {}'.format(len(connected_nodes)))\n",
    "\n",
    "        node_cons=[]\n",
    "        for node in list(G.nodes()):\n",
    "\n",
    "            node_con=test_object[node]\n",
    "            \n",
    "          \n",
    "            if node_con>0:\n",
    "                node_cons.append(1)\n",
    "            else:\n",
    "                node_cons.append(0)\n",
    "            \n",
    "            \n",
    "\n",
    "        block_test=np.array(node_cons).sum()/len(G.nodes)          \n",
    "\n",
    "        '''\n",
    "        if block_test==1:\n",
    "            test=1\n",
    "        else:\n",
    "            test=0\n",
    "        '''\n",
    "\n",
    "        range_test.append(block_test)\n",
    "        #range_test.append(test)\n",
    "        pbar.update(1)\n",
    "\n",
    "print('% of nodes with positive closeness per Block : {}'.format(np.array(range_test).mean()))\n",
    "#print('Blocks with closeness for all nodes: {}'.format(np.array(range_test).sum()))\n",
    "print('Total Blocks: {}'.format(no_blocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9950617283950617"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(range_test)[1399]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Baseline Channels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute in seconds: 186.33308386802673\n"
     ]
    }
   ],
   "source": [
    "# Calculate Baseline degree\n",
    "snapshot_channels=collection_measure(bucket,extract_keys,'channels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST all saved correctly\n",
    "responses=np.array([r for b,r in snapshot_channels.items()])\n",
    "\n",
    "if np.amax(responses)==np.amin(responses) and len(responses) == len(extract_keys):\n",
    "    print('Save to S3 succesful')\n",
    "else:\n",
    "    print('Error saving to S3')\n",
    "    \n",
    "measurement='channels'    \n",
    "rand_block=str(random.choice(blocks))    \n",
    "test_key='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/raw_score/'+rand_block+'.pkl'\n",
    "test_file = s3.Object(bucket_name=bucket, key=test_key).get()\n",
    "test_object = pickle.loads(test_file['Body'].read())\n",
    "print('Example of {} saved for block {}:'.format(measurement,rand_block))\n",
    "print(list(test_object.items())[:10])    \n",
    "print('Total entries: {}'.format(len(list(test_object.items()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Baseline Capacity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute in seconds: 205.32206177711487\n"
     ]
    }
   ],
   "source": [
    "# Calculate Baseline degree\n",
    "snapshot_capacity=collection_measure(bucket,extract_keys,'capacity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save to S3 succesful\n",
      "Example of capacity saved for block 586042:\n",
      "[(6038, 8878679), (5314, 14026340), (934, 1171934), (3023, 1111934), (3452, 2063908), (576, 40000), (3436, 6948131), (3310, 100000), (2378, 400000), (4223, 24298841)]\n",
      "Total entries: 4920\n"
     ]
    }
   ],
   "source": [
    "# TEST all saved correctly\n",
    "responses=np.array([r for b,r in snapshot_capacity.items()])\n",
    "\n",
    "if np.amax(responses)==np.amin(responses) and len(responses) == len(extract_keys):\n",
    "    print('Save to S3 succesful')\n",
    "else:\n",
    "    print('Error saving to S3')\n",
    "    \n",
    "measurement='capacity'    \n",
    "rand_block=str(random.choice(blocks))    \n",
    "test_key='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/raw_score/'+rand_block+'.pkl'\n",
    "test_file = s3.Object(bucket_name=bucket, key=test_key).get()\n",
    "test_object = pickle.loads(test_file['Body'].read())\n",
    "print('Example of {} saved for block {}:'.format(measurement,rand_block))\n",
    "print(list(test_object.items())[:10])\n",
    "print('Total entries: {}'.format(len(list(test_object.items()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Baseline Age**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5b44ee0cd944909ff8de4320c225b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=36536.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compute in seconds: 511.2060635089874\n"
     ]
    }
   ],
   "source": [
    "# Calculate Baseline age\n",
    "snapshot_age=collection_measure(bucket,extract_keys,'age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save to S3 succesful\n",
      "Example of age saved for block 568893:\n",
      "[(6038, 63744), (5314, 63744), (934, 62491), (3023, 62491), (3452, 62046), (576, 62046), (3436, 60818), (3310, 60818), (2378, 60803), (4223, 60803)]\n",
      "Total entries: 3872\n"
     ]
    }
   ],
   "source": [
    "# TEST all saved correctly\n",
    "responses=np.array([r for b,r in snapshot_age.items()])\n",
    "\n",
    "if np.amax(responses)==np.amin(responses) and len(responses) == len(extract_keys):\n",
    "    print('Save to S3 succesful')\n",
    "else:\n",
    "    print('Error saving to S3')\n",
    "    \n",
    "measurement='age'    \n",
    "rand_block=str(random.choice(blocks))    \n",
    "test_key='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/raw_score/'+rand_block+'.pkl'\n",
    "test_file = s3.Object(bucket_name=bucket, key=test_key).get()\n",
    "test_object = pickle.loads(test_file['Body'].read())\n",
    "print('Example of {} saved for block {}:'.format(measurement,rand_block))\n",
    "print(list(test_object.items())[:10])\n",
    "print('Total entries: {}'.format(len(list(test_object.items()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Baseline Growth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b0e0f5ade74b48b47e43aecb8afed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=36536.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compute in seconds: 20984.033442735672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n",
      "_GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=CancelledError()>\n",
      "concurrent.futures._base.CancelledError\n"
     ]
    }
   ],
   "source": [
    "# Calculate Baseline capacity growth\n",
    "snapshot_capgrowth=collection_measure(bucket,extract_keys,'capacity_growth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save to S3 succesful\n",
      "Example of capacity_growth saved for block 603179:\n",
      "[(5314, 0), (934, 0), (3023, 0), (3452, 0), (576, 0), (3436, -12000), (3310, 0), (2378, 0), (4223, 500000), (422, 0)]\n",
      "Total entries: 5338\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba8263bf8cc47e490cfc7d960ea1f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5338.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Growth calculated correctly for all nodes\n"
     ]
    }
   ],
   "source": [
    "# TEST all saved correctly\n",
    "responses=np.array([r for b,r in snapshot_capgrowth.items()])\n",
    "\n",
    "if np.amax(responses)==np.amin(responses) and len(responses) == len(extract_keys):\n",
    "    print('Save to S3 succesful')\n",
    "else:\n",
    "    print('Error saving to S3')\n",
    "    \n",
    "measurement='capacity_growth'    \n",
    "rand_block=str(random.choice(blocks))    \n",
    "test_key='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/raw_score/'+rand_block+'.pkl'\n",
    "test_file = s3.Object(bucket_name=bucket, key=test_key).get()\n",
    "test_object = pickle.loads(test_file['Body'].read())\n",
    "print('Example of {} saved for block {}:'.format(measurement,rand_block))\n",
    "print(list(test_object.items())[:10])\n",
    "print('Total entries: {}'.format(len(list(test_object.items()))))\n",
    "\n",
    "# Get test graph\n",
    "g_test_key='graph_snapshots/'+str(extraction_id)+'_connected/'+str(rand_block)+'.gpickle'\n",
    "response = s3.Object(bucket_name=bucket, key=g_test_key).get()\n",
    "g_test=pickle.loads(response['Body'].read())\n",
    "\n",
    "# Run test function on test graph\n",
    "test_growth(g_test,test_object,int(rand_block),channel_opens,channel_closures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparissons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Compare property changes for nodes in a collection of graphs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###NEW \n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "collection_compare\n",
    "    Iterates over blocks to calculate marginal change in metric for nodes that made decisions (opens/closures)\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "blocks : list\n",
    "    List of blocks extracted when reading graphs\n",
    "    \n",
    "    \n",
    "dec_dic: dic\n",
    "    Dictionary with channel decisions (open or closure) per block\n",
    "    \n",
    "    \n",
    "graph_snapshots: list\n",
    "    List of delayed dask objects each pointing to a graph snapshot to be loaded from S3\n",
    "\n",
    "snapshots_base: dic\n",
    "    Dictionary of dictionaries containing the base measurments per block per node\n",
    "\n",
    "measurement: string\n",
    "    Name of the type of measurment that will applied to the graph\n",
    "    \n",
    "type_dec: string\n",
    "    The type of decisions that will be analyzed 'opens' or 'closures'\n",
    "    \n",
    "\n",
    "Returns\n",
    "-------\n",
    "futures: list\n",
    "    List of tuples of the form (dic_node0,dic_node1) where dic_node0/1 is a dictionary containing the marginal changes for node0/1 \n",
    "    for every decision in a given block. The dictionaries are future dask objects that still need to be explicitly computed. \n",
    "\n",
    "\"\"\"\n",
    "def collection_compare(blocks,dec_dic,graph_keys,snapshots_base,measurement,type_dec):\n",
    "\n",
    "    futures_list=[] # list to populate with futures per block\n",
    "    \n",
    "    with tqdm(total=len(range(1,len(graph_keys)))) as pbar:\n",
    "        for i in range(1,len(graph_keys)): # iterate through blocks\n",
    "\n",
    "            # extract information from parameters and construct input tuple to delayed function\n",
    "            block=blocks[i]\n",
    "            block_prev=blocks[i-1]\n",
    "            block_dec=dec_dic[block]\n",
    "            key=graph_keys[i-1] # Pass previous graph\n",
    "            block_base=snapshots_base[block_prev]\n",
    "            block_res=snapshots_base[block]\n",
    "            input_tuple=(block,g,block_dec,block_base,measurement,type_dec,block_res)\n",
    "            \n",
    "            # submit to delayed function and append to list\n",
    "            output_tuple=dask.delayed(graph_compare)(input_tuple)\n",
    "            futures_list.append(output_tuple)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # persist to disk and return\n",
    "    futures = dask.persist(*futures_list)\n",
    "    return futures\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "collection_compare\n",
    "    Iterates over blocks to calculate marginal change in metric for nodes that made decisions (opens/closures)\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "blocks : list\n",
    "    List of blocks extracted when reading graphs\n",
    "    \n",
    "    \n",
    "dec_dic: dic\n",
    "    Dictionary with channel decisions (open or closure) per block\n",
    "    \n",
    "    \n",
    "graph_snapshots: list\n",
    "    List of delayed dask objects each pointing to a graph snapshot to be loaded from S3\n",
    "\n",
    "snapshots_base: dic\n",
    "    Dictionary of dictionaries containing the base measurments per block per node\n",
    "\n",
    "measurement: string\n",
    "    Name of the type of measurment that will applied to the graph\n",
    "    \n",
    "type_dec: string\n",
    "    The type of decisions that will be analyzed 'opens' or 'closures'\n",
    "    \n",
    "\n",
    "Returns\n",
    "-------\n",
    "futures: list\n",
    "    List of tuples of the form (dic_node0,dic_node1) where dic_node0/1 is a dictionary containing the marginal changes for node0/1 \n",
    "    for every decision in a given block. The dictionaries are future dask objects that still need to be explicitly computed. \n",
    "\n",
    "\n",
    "def collection_compare(blocks,dec_dic,graph_snapshots,snapshots_base,measurement,type_dec):\n",
    "\n",
    "    futures_list=[] # list to populate with futures per block\n",
    "    \n",
    "    with tqdm(total=len(range(1,len(graph_snapshots)))) as pbar:\n",
    "        for i in range(1,len(graph_snapshots)): # iterate through blocks\n",
    "\n",
    "            # extract information from parameters and construct input tuple to delayed function\n",
    "            block=blocks[i]\n",
    "            block_prev=blocks[i-1]\n",
    "            block_dec=dec_dic[block]\n",
    "            g=graph_snapshots[i-1] # Pass previous graph\n",
    "            block_base=snapshots_base[block_prev]\n",
    "            block_res=snapshots_base[block]\n",
    "            input_tuple=(block,g,block_dec,block_base,measurement,type_dec,block_res)\n",
    "            \n",
    "            # submit to delayed function and append to list\n",
    "            output_tuple=dask.delayed(graph_compare)(input_tuple)\n",
    "            futures_list.append(output_tuple)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # persist to disk and return\n",
    "    futures = dask.persist(*futures_list)\n",
    "    return futures\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Compare property changes for nodes in a graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "graph_compare\n",
    "    Calculates marginal change in metric for node0, node1 make decisions (open/close channels) in a single block\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "input_tuple : tuple\n",
    "    \n",
    "    block: int\n",
    "        Block number\n",
    "    g: nx_graph \n",
    "        Graph snapshot (as dask delayed object)\n",
    "    block_dec: list\n",
    "        List of tuples in nx edge format (u,v,att_dic) for all the decisions (channel opens/closures) made in that block  \n",
    "    block_base: dic\n",
    "        Dictionary of base measurements for each node in the graph snapshot corresponding to that block\n",
    "    measurement: string\n",
    "        Name of measurement to be computed\n",
    "    type_dec: string\n",
    "    The type of decisions that will be analyzed 'opens' or 'closures'\n",
    "    \n",
    "    block_res: dic\n",
    "        Dictionary of base measurements for each node in the graph snapshot corresponding to the next block\n",
    "    \n",
    "\n",
    "Returns\n",
    "-------\n",
    "nodes_mar_dic: tuple\n",
    "    Tuples of the form (mar_node0_dic_i,mar_node0_dic_i) where each element in the tuple is a dictionary containing the marginal changes for node0/1 \n",
    "    for every node0 and node1 involved in a decision (channel open/closures) in the block.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def graph_compare(input_tuple):\n",
    "    \n",
    "    block=input_tuple[0]\n",
    "    key=input_tuple[1]\n",
    "    block_dec=input_tuple[2]\n",
    "    block_base=input_tuple[3]\n",
    "    measurement=input_tuple[4]\n",
    "    type_dec=input_tuple[5]\n",
    "    block_res=input_tuple[6]\n",
    "   \n",
    "    mar_node0_dic_i={} # dictionary to story function output\n",
    "    mar_node1_dic_i={} \n",
    "    \n",
    "    \n",
    "    # Load data\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.resource('s3')\n",
    "    response = s3.Object(bucket_name=bucket, key=key).get()\n",
    "    G=pickle.loads(response['Body'].read())\n",
    "    \n",
    "    ###########################---------------------\n",
    "    ##if decisiono \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # For each decision calculate marginal change in measure for node0 and node1\n",
    "    for edge in block_dec:\n",
    "        \n",
    "        # Extract info about channel\n",
    "        \n",
    "        node0=edge[0]\n",
    "        node1=edge[1]\n",
    "        channel_id=edge[2]['channel_id']\n",
    "        capacity=edge[2]['capacity']\n",
    "\n",
    "        \n",
    "        #Â Copy original graph\n",
    "        g_mar=G.copy()   \n",
    "        old_nodes=False\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Retrieve base measurement before channel if nodes existed, else define base measure as 0\n",
    "        if (g_mar.has_node(node0)):\n",
    "            node0_base=block_base[node0]\n",
    "            old_nodes=True\n",
    "        else:\n",
    "            node0_base=0\n",
    "            \n",
    "        if (g_mar.has_node(node1)):\n",
    "            node1_base=block_base[node1]\n",
    "            old_nodes=True\n",
    "        else:\n",
    "            node1_base=0\n",
    "        \n",
    "            \n",
    "        if old_nodes: # If at least one node is old (part of the connected graph)\n",
    "            \n",
    "            if type_dec=='mar_opens': # marginal calculation for opens\n",
    "                \n",
    "                \n",
    "                # Define and add edges and calculate betweeness if at least one of the nodes is in graph \n",
    "                edge_list=[edge]\n",
    "                \n",
    "                \n",
    "                # If channel exists increase capacity\n",
    "                \n",
    "                if g_mar.has_edge(node0,node1):\n",
    "                   \n",
    "                    g_mar.edges[node0,node1]['capacity']+=capacity\n",
    "                    g_mar.edges[node0,node1]['no_channels']+=1\n",
    "\n",
    "                else:\n",
    "                    g_mar.add_edges_from(edge_list)\n",
    "                \n",
    "                \n",
    "                g_mar_mes=node_measurement(g_mar,measurement,node0,node1)\n",
    "                \n",
    "                # Update measurement values after marginal change\n",
    "                node0_new_mes=g_mar_mes[0]\n",
    "                node1_new_mes=g_mar_mes[1]\n",
    "            \n",
    "            elif type_dec=='mar_closures': # marginal calculation for closes\n",
    "                \n",
    "                # Define and remove edges, define new connected graph and calculate betweeness \n",
    "                edge_list=[(node0,node1)]\n",
    "                \n",
    "                \n",
    "                # If channel exists decrease capacity\n",
    "                if g_mar.edges[node0,node1]['no_channels']>1:\n",
    "                    g_mar.edges[node0,node1]['capacity']-=capacity\n",
    "                    g_mar.edges[node0,node1]['no_channels']-=1\n",
    "                \n",
    "                else: \n",
    "                    g_mar.remove_edges_from(edge_list) \n",
    "                    connected_components=[c for c in nx.algorithms.components.connected_components(g_mar)]\n",
    "                    g_mar=g_mar.subgraph(connected_components[0]).copy()\n",
    "                    \n",
    "                g_mar_mes=node_measurment(g_mar,measurement,node0,node1)\n",
    "                node0_new_mes=g_mar_mes[0]\n",
    "                node1_new_mes=g_mar_mes[1]\n",
    "                \n",
    "            elif type_dec=='actual': # actual calculation for both opens and closures\n",
    "                \n",
    "                # Check individualy if in the graph for the resulting block the node is present (in the connected component, \n",
    "                # if not assign measurment to 0. \n",
    "                \n",
    "                try:\n",
    "                    node0_new_mes=block_res[node0]\n",
    "\n",
    "                except KeyError:\n",
    "                    node0_new_mes=0\n",
    "\n",
    "                try:\n",
    "                    node1_new_mes=block_res[node1]\n",
    "\n",
    "                except KeyError:\n",
    "                    node1_new_mes=0\n",
    "\n",
    "                \n",
    "                   \n",
    "            node0_mar=(node0_new_mes-node0_base)\n",
    "            node1_mar=(node1_new_mes-node1_base) \n",
    "        \n",
    "        \n",
    "        else: # If both nodes are new (outside of connected graph) their marginal decision outcome is 0\n",
    "            node0_mar=0\n",
    "            node1_mar=0\n",
    "\n",
    "        \n",
    "        # Update dictionary - new betweenness\n",
    "        mar_node0_dic_i[channel_id]=node0_mar\n",
    "        mar_node1_dic_i[channel_id]=node1_mar\n",
    "        \n",
    "    \n",
    "    return (mar_node0_dic_i,mar_node1_dic_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MARGINAL - CURRENT BETWEENESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Marginal current betweeness for opens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute marginal betweeness for channel openings\n",
    "\n",
    "futures_bet_maropen=collection_compare(blocks,channel_opens,graph_snapshots,snapshot_bet,measurement='current_betweeness',type_dec='mar_opens')\n",
    "start=time.time()\n",
    "bet_maropen_diclist = dask.compute(*futures_bet_maropen)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(bet_maropen_diclist)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to marginal current betweeness for opens into decisions Dataframe\n",
    "\n",
    "bet_maropen_channels=add_columns(bet_maropen_diclist,decisions_df,'bet_maropen_node0','bet_maropen_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with marginal current betweeness for opens: {}'.format(len(bet_maropen_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(bet_maropen_channels)].head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Marginal current betweeness for closures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute marginal betweeness for channel closures\n",
    "\n",
    "futures_bet_marclose=collection_compare(blocks,channel_closures,graph_snapshots,snapshot_bet,measurement='current_betweeness',type_dec='mar_closures')\n",
    "start=time.time()\n",
    "bet_marclose_diclist = dask.compute(*futures_bet_marclose)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(bet_marclose_diclist)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to marginal current betweeness for closures into decisions Dataframe\n",
    "\n",
    "bet_marclose_channels=add_columns(bet_marclose_diclist,decisions_df,'bet_marclose_node0','bet_marclose_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with marginal current betweeness for closures: {}'.format(len(bet_marclose_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(bet_marclose_channels)].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MARGINAL - CURRENT CLOSENESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Marginal current closeness for opens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute marginal betweeness for channel openings\n",
    "\n",
    "futures_clo_maropen=collection_compare(blocks,channel_opens,graph_snapshots,snapshot_clo,measurement='current_closeness',type_dec='mar_opens')\n",
    "start=time.time()\n",
    "clo_maropen_diclist = dask.compute(*futures_clo_maropen)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(clo_maropen_diclist)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to marginal current betweeness for opens into decisions Dataframe\n",
    "\n",
    "clo_maropen_channels=add_columns(clo_maropen_diclist,decisions_df,'clo_maropen_node0','clo_maropen_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with marginal current closeness for opens: {}'.format(len(clo_maropen_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(clo_maropen_channels)].head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Marginal current closeness for closures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute marginal current closeness for channel closures\n",
    "\n",
    "futures_clo_marclose=collection_compare(blocks,channel_closures,graph_snapshots,snapshot_clo,measurement='current_closeness',type_dec='mar_closures')\n",
    "start=time.time()\n",
    "clo_marclose_diclist = dask.compute(*futures_clo_marclose)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(clo_marclose_diclist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to marginal current betweeness for opens into decisions Dataframe\n",
    "\n",
    "clo_marclose_channels=add_columns(clo_marclose_diclist,decisions_df,'clo_marclose_node0','clo_marclose_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with marginal current closeness for closures: {}'.format(len(clo_marclose_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(clo_marclose_channels)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACTUAL - CURRENT BETWEENESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Actual current betweeness for opens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute actual current betweeness for channel opens\n",
    "\n",
    "futures_bet_actopen=collection_compare(blocks,channel_opens,graph_snapshots,snapshot_bet,measurement='current_betweeness',type_dec='actual')\n",
    "start=time.time()\n",
    "bet_actopen_diclist = dask.compute(*futures_bet_actopen)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(bet_actopen_diclist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to marginal current betweeness for opens into decisions Dataframe\n",
    "\n",
    "bet_actopen_channels=add_columns(bet_actopen_diclist,decisions_df,'bet_actopen_node0','bet_actopen_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with actual current betweeness for opens: {}'.format(len(bet_actopen_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(bet_actopen_channels)].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Actual current betweeness for closures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute actual current betweeness for channel closures\n",
    "\n",
    "futures_bet_actclose=collection_compare(blocks,channel_closures,graph_snapshots,snapshot_bet,measurement='current_betweeness',type_dec='actual')\n",
    "start=time.time()\n",
    "bet_actclose_diclist = dask.compute(*futures_bet_actclose)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(bet_actclose_diclist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to marginal current betweeness for opens into decisions Dataframe\n",
    "\n",
    "bet_actclose_channels=add_columns(bet_actclose_diclist,decisions_df,'bet_actclose_node0','bet_actclose_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with actual current betweeness for closures: {}'.format(len(bet_actclose_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(bet_actclose_channels)].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACTUAL - CURRENT CLOSENESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Actual current closeness for opens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute actual current closeness for channel opens\n",
    "\n",
    "futures_clo_actopen=collection_compare(blocks,channel_opens,graph_snapshots,snapshot_bet,measurement='current_closeness',type_dec='actual')\n",
    "start=time.time()\n",
    "clo_actopen_diclist = dask.compute(*futures_clo_actopen)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(bet_actopen_diclist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to marginal current closeness for opens into decisions Dataframe\n",
    "\n",
    "clo_actopen_channels=add_columns(clo_actopen_diclist,decisions_df,'clo_actopen_node0','clo_actopen_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with actual current closeness for opens: {}'.format(len(clo_actopen_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(clo_actopen_channels)].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Actual current closeness for closures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute actual current closeness for channel closures\n",
    "\n",
    "futures_clo_actclose=collection_compare(blocks,channel_closures,graph_snapshots,snapshot_bet,measurement='current_closeness',type_dec='actual')\n",
    "start=time.time()\n",
    "clo_actclose_diclist = dask.compute(*futures_clo_actclose)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(clo_actclose_diclist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to marginal current closeness for opens into decisions Dataframe\n",
    "\n",
    "clo_actclose_channels=add_columns(clo_actclose_diclist,decisions_df,'clo_actclose_node0','clo_actclose_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with actual current closeness for closures: {}'.format(len(clo_actclose_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(clo_actclose_channels)].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise stability \n",
    "\n",
    "- **Marginal betweenness (bet_mar_nodei)**: The % change between the betweenness centrality, for the node under analysis, given the graph from the previous block and the betweenness centrality of the resulting graph after enacting the decission (adding or removing a channel). Weighted current betweenness centrality is used for this measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Marginal betweenness for opens** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Why is length of Dataframe longer than the number of snapshots extracted? Could it be that some channels appear more than once in dataframe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Marginal betweenness for closures** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Actual change in betweenness (bet_act_nodei)**: The % change between the betweenness centrality, for the node under analysis, given the graph from the previous block and the betweenness centrality of the resulting graph after enacting **all** the decissions (adding or removing a channels) in the current block. Weighted current betweenness centrality is used for this measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Marginal betweeness pairwise stability (bet_mar_pairst/open/close)**: Evaluates if given the marginal graph that results from just enacting this decission is consistent with pairwise stability, from a betweenness perspective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MARGINAL - Add column with check for pairwise stability compatability using marginal outcomes\n",
    "\n",
    "# OPEN - Channel is opened if both nodes gain\n",
    "\n",
    "# Define function \n",
    "def bet_pairst_maropen(row):\n",
    "    if not math.isnan(row['bet_mar_node0']):\n",
    "        pairst=(row['bet_maropen_node0']>=0 and row['bet_maropen_node1']>=0)\n",
    "    else:\n",
    "        pairst=row['bet_maropen_node0']\n",
    "    return pairst\n",
    "\n",
    "# Apply function\n",
    "decisions_df['bet_mar_pairstopen']=decisions_df.apply(bet_pairst_maropen,axis=1)\n",
    "\n",
    "# CLOSE - Channel is closed if at least one node is better off\n",
    "\n",
    "# Define function \n",
    "def bet_pairst_marclose(row):\n",
    "    if not math.isnan(row['bet_marclose_node0']):\n",
    "        pairst=(row['bet_marclose_node0']>0 or row['bet_marclose_node1']>0)\n",
    "    else:\n",
    "        pairst=row['bet_marclose_node0']\n",
    "    return pairst\n",
    "\n",
    "# Apply function\n",
    "decisions_df['bet_mar_pairstclose']=decisions_df.apply(bet_pairst_marclose,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MARGINAL OPEN\n",
    "decisions_df[decisions_df['bet_mar_node0'].notnull()][['bet_mar_node0','bet_mar_node1','bet_mar_pairstopen']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MARGINAL CLOSE\n",
    "decisions_df[decisions_df['bet_marclose_node0'].notnull()][['bet_marclose_node0','bet_marclose_node1','bet_mar_pairstclose']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Actual betweeness pairwise stability (bet_act_pairstopen/close)**: Evaluates if given the marginal graph that results from all the decisions in the block is consitend with pairwise stability, from a betweenness perspective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTUAL - Add column with check for pairwise stability compatability using marginal outcomes\n",
    "\n",
    "# OPEN - Channel is opened if both nodes gain\n",
    "\n",
    "# Define function \n",
    "def bet_pairst_actopen(row):\n",
    "    if not math.isnan(row['bet_actopen_node0']):\n",
    "        pairst=(row['bet_actopen_node0']>=0 and row['bet_actopen_node1']>=0)\n",
    "    else:\n",
    "        pairst=row['bet_actopen_node0']\n",
    "    return pairst\n",
    "\n",
    "# Apply function\n",
    "decisions_df['bet_act_pairstopen']=decisions_df.apply(bet_pairst_actopen,axis=1)\n",
    "\n",
    "# CLOSE - Channel is closed if at least one node is better off\n",
    "\n",
    "# Define function \n",
    "def bet_pairst_actclose(row):\n",
    "    if not math.isnan(row['bet_actclose_node0']):\n",
    "        pairst=(row['bet_actclose_node0']>0 or row['bet_actclose_node1']>0)\n",
    "    else:\n",
    "        pairst=row['bet_actclose_node0']\n",
    "    return pairst\n",
    "\n",
    "# Apply function\n",
    "decisions_df['bet_act_pairstclose']=decisions_df.apply(bet_pairst_actclose,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ACTUAL OPEN\n",
    "decisions_df[decisions_df['bet_actopen_node0'].notnull()][['bet_actopen_node0','bet_actopen_node1','bet_act_pairstopen']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ACTUAL CLOSE\n",
    "decisions_df[decisions_df['bet_actclose_node0'].notnull()][['bet_actclose_node0','bet_actclose_node1','bet_act_pairstclose']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Updated DataFrame to S3\n",
    "\n",
    "# Create S3 resource and define values\n",
    "session = boto3.session.Session()\n",
    "s3 = session.resource('s3')\n",
    "csv_buffer = io.StringIO()\n",
    "\n",
    "# File path and name ([extraction_id]snapshot_bet-[no_blocks]-[start_block]-[end_block])\n",
    "key_decisions_df='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+str(extraction_id)+'decisions_df_bet-'+str(no_blocks)+'-'+str(start_block)+'-'+str(end_block)+'.csv'\n",
    "\n",
    "\n",
    "# Safe DataFrame\n",
    "decisions_df.to_csv(csv_buffer)\n",
    "s3.Object(bucket, key_decisions_df).put(Body=csv_buffer.getvalue())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Save\n",
    "decisions_df_test_load = s3.Object(bucket_name=bucket, key=key_decisions_df).get()\n",
    "decisions_df_test=pd.read_csv(io.BytesIO(decisions_df_test_load['Body'].read()),index_col=0)\n",
    "decisions_df_test==decisions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions_df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency\n",
    "- **Average betweeness per block (bet_effic)**: Average betweenness centrality for all the nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nash stability \n",
    "\n",
    "- **% Change with respect to not making decision (bet_binstat_deltai)**: The % change in betwewnness centrality, for the node under analysis, given the resulting graph after all of the decissions have been executed. \n",
    "- **Nash compatible - binary strategy (bet_binstat_nash)**: Returns true if given the other decissions enacted in the block not making decision would have NOT have resulted in higher betweenness centrality. This tells me if my strategy helped me be better off (took into account what others were doing)\n",
    "\n",
    "(Optional approaches - Check for tracktability)\n",
    "- **Nash compatible - close only strategy (bet_closestat_nash)**: Returns true if given the other decissions enacted in the block, closing any other channels would NOT have not resulted in higher betwneenness centrality. (NOTE: Check if there are combinatorial considerations, if so just look at closings up to x) \n",
    "- **Nash compatible - close/open (bet_allstat_nash)**: Returns true if given the other decissions enacted in the block, closing any other channels (with any node) or opening a channel with one of the round participants would NOT have not resulted in lower betwneenness centrality. (NOTE: To make it reasonable and constraint the strategy space only consider 'similar nodes' or with relationships in the past?).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Connectivity\n",
    "\n",
    "### Pairwise stability \n",
    "\n",
    "- **Marginal % change in connectivity (con_mar_deltai)**: The % change between the shortest path average, for the node under analysis, given the graph from the previous block and the shortest path average of the resulting graph after enacting the decission (adding or removing a channel). Weighted shortest path (_single_source_dijkstra_path_) is used for this measure.\n",
    "\n",
    "- **Actual % change in connectivity (con_act_deltai)**: The % change between the shortest path average, for the node under analysis, given the graph from the previous block and the shortest path average of the resulting graph after enacting **all** the decissions (adding or removing a channels) in the current block. Weighted shortest path (_single_source_dijkstra_path_) is used for this measure.\n",
    "\n",
    "- **Marginal connectivity pairwise stability (con_mar_pairstab)**: Evaluates if given the marginal graph that results from just enacting this decission is consistent with pairwise stability, from a connectivity perspective.\n",
    "\n",
    "- **Actual connectivity pairwise stability (con_act_pairstab)**: Evaluates if given the marginal graph that results from all the decisions in the block is consitend with pairwise stability, from a connectivity perspective.  \n",
    "\n",
    "\n",
    "\n",
    "### Nash stability \n",
    "\n",
    "- **% Change with respect to not making decision (con_binstat_deltai)**: The % change in shortest path average, for the node under analysis, given the resulting graph after all of the decissions have been executed. \n",
    "- **Nash compatible - binary strategy (con_binstat_nash)**: Returns true if given the other decissions enacted in the block not making decision would have NOT have resulted in higher shortest path average. NOTE: This indicates if the strategy selected made the node better off (took into account what others were doing)\n",
    "\n",
    "(Optional approaches - Check for tracktability)\n",
    "- **Nash compatible - close only strategy (con_closestat_nash)**: Returns true if given the other decissions enacted in the block, closing any other channels would NOT have not resulted in higher shortest path average. (NOTE: Check if there are combinatorial considerations, if so just look at closings up to x) \n",
    "- **Nash compatible - close/open (con_allstat_nash)**: Returns true if given the other decissions enacted in the block, closing any other channels (with any node) or opening a channel with one of the round participants would NOT have not resulted in lower shortest path average. (NOTE: To make it reasonable and constraint the strategy space only consider 'similar nodes' or with relationships in the past?).\n",
    "\n",
    "\n",
    "\n",
    "### Efficiency\n",
    "- **Average betweeness per block (bet_effic)**: Average shortest path average for all the nodes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
