{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LN - Data PP - Stability and efficiency calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import itertools\n",
    "#import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import io\n",
    "import random\n",
    "from itertools import islice\n",
    "import math\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from time import sleep\n",
    "\n",
    "from dask_cloudprovider import FargateCluster\n",
    "from dask.distributed import Client\n",
    "import dask.array as da\n",
    "import dask\n",
    "dask.config.set({'distributed.scheduler.allowed-failures': 50}) \n",
    "\n",
    "\n",
    "import boto3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "\n",
    "bucket='ln-strategy-data'\n",
    "extraction_id=1587447789\n",
    "#extraction_id=1585344554"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection to AWS Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate s3 resource\n",
    "\n",
    "session = boto3.session.Session()\n",
    "s3 = session.resource('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fargate Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster = FargateCluster(n_workers=100,scheduler_timeout='20 minutes',image='dsrincon/dask-graph:nx-scipy-v1',scheduler_cpu=4096,scheduler_mem=16384,worker_mem=32768)\n",
    "cluster = FargateCluster(n_workers=100,scheduler_timeout='20 minutes',image='dsrincon/dask-graph:nx-scipy-v1',scheduler_cpu=4096,scheduler_mem=16384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dade886fe95244cda8dadaf5ffe65b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>FargateCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n  â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client = Client(cluster)\n",
    "cluster=Client('tcp://3.214.224.107:8786')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Write output to DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function write output to DataFrame\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "add_columns\n",
    "    Function that takes an output from a decision comparisson computation and adds it's results for nodes 1 and 0 in the main DataFrame\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "output_diclist : list\n",
    "    Dictionary of the form (node0_dic_i,node1_dic_i) where i runs for all of the blocks being compared. \n",
    "\n",
    "original_df: Pandas DataFrame\n",
    "    Original DataFrame containing the opening and closure information for each channel, with a column named 'short_channel_id' to denote \n",
    "    id of channel. \n",
    "\n",
    "column_name_node0: string\n",
    "    Name for column in dataframe where the results will be stored for node 0\n",
    "    \n",
    "column_name_node1: string\n",
    "    Name for column in dataframe where the results will be stored for node 1\n",
    "\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "no_changes: list\n",
    "    List with the 'short_channel_id' of the channels edited. \n",
    " \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def add_columns(output_diclist,original_df,column_name_node0,column_name_node1):\n",
    "\n",
    "\n",
    "    # Merge individual dictionaries into one for each node\n",
    "    node0_dic={}\n",
    "    node1_dic={}\n",
    "    for dic_tuple in output_diclist:\n",
    "        node0_dic.update(dic_tuple[0])\n",
    "        node1_dic.update(dic_tuple[1])\n",
    "    \n",
    "    # Add to DataFrame\n",
    "\n",
    "    # Create empty columns\n",
    "    original_df[column_name_node0]=np.nan\n",
    "    original_df[column_name_node1]=np.nan\n",
    "\n",
    "    # Populate df with values\n",
    "    original_df[column_name_node0]=original_df['short_channel_id'].map(node0_dic)\n",
    "    original_df[column_name_node1]=original_df['short_channel_id'].map(node1_dic)\n",
    "    \n",
    "    # Calculate values changed\n",
    "    rows_edited=(original_df[original_df[column_name_node0].notnull()]['short_channel_id']).tolist()\n",
    "    \n",
    "    return rows_edited\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Save python object to S3 using pickle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to write pickle data to S3 bucket\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "pickle_save_s3\n",
    "    Saves Python object to S3 as pickle object\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "obj : <any>\n",
    "    Python Object\n",
    "\n",
    "blocks: list\n",
    "    List of extracted blocks\n",
    "\n",
    "extraction_id: int\n",
    "    Number of block extraction\n",
    "    \n",
    "name: string\n",
    "    Name of object to add to filename in S3\n",
    "\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "response: int\n",
    "    HTTP response code from S3 \n",
    " \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def pickle_save_s3(obj,blocks,extraction_id,name):\n",
    "\n",
    "\n",
    "    # Define number of blocks\n",
    "    start_block=np.min(np.array(blocks))\n",
    "    end_block=np.max(np.array(blocks))\n",
    "    no_blocks=len(blocks)\n",
    "\n",
    "    # Load S3 and bucket details\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.resource('s3')\n",
    "\n",
    "    # File path and name ([extraction_id][name]-[no_blocks]-[start_block]-[end_block])\n",
    "    key='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+str(extraction_id)+name+'-'+str(no_blocks)+'-'+str(start_block)+'-'+str(end_block)+'.pkl'\n",
    "\n",
    "    # Create pickle object and send to S3\n",
    "    pickle_byte_obj = pickle.dumps(obj) \n",
    "    response=s3.Object(bucket,key).put(Body=pickle_byte_obj)\n",
    "    \n",
    "    return response['ResponseMetadata']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Load single graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "load_graph\n",
    "    Loads networkX (pickle serialized) object from S3\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "key : str\n",
    "    Path in S3 bucket for individual pickled serialized networkX graph object \n",
    "\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "response: networkX graph\n",
    "    Graph object\n",
    " \n",
    "\"\"\"\n",
    "\n",
    "def load_graph(key):\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.resource('s3')\n",
    "    response = s3.Object(bucket_name=bucket, key=key).get()\n",
    "    G=pickle.loads(response['Body'].read())\n",
    "    \n",
    "    return G\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load objects form S3\n",
    "# Dataframe\n",
    "\n",
    "decisions_load = s3.Object(bucket_name=bucket, key='decisions_df.csv').get()\n",
    "decisions_df=pd.read_csv(io.BytesIO(decisions_load['Body'].read()))\n",
    "\n",
    "# Channel closures\n",
    "closure_file = s3.Object(bucket_name=bucket, key='channel_closures.p').get()\n",
    "channel_closures = pickle.loads(closure_file['Body'].read())\n",
    "    \n",
    "    \n",
    "# Channel openings \n",
    "opens_file = s3.Object(bucket_name=bucket, key='channel_opens.p').get()\n",
    "channel_opens = pickle.loads(opens_file['Body'].read())\n",
    "\n",
    "    \n",
    "\n",
    "# Create list with graph keys\n",
    "\n",
    "#TODO: Save graphs as numpy array in single H5 file to reduce. Test if creating graphs takes longer than reading from S3\n",
    "\n",
    "# graph_dir='./data/graph_snapshots' - For local tests\n",
    "\n",
    "\n",
    "graph_keys = [obj.key \n",
    "        for obj in s3.Bucket(name=bucket).objects.all()\n",
    "        if re.match(\".*\"+str(extraction_id)+\"_connected/.*\\.gpickle\",obj.key)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Blocks to be extracted and define graph\n",
    "\n",
    "\n",
    "# Base lists to be populated\n",
    "graph_snapshots=[]\n",
    "blocks=[]\n",
    "\n",
    "\n",
    "extract_keys=graph_keys[6:] # Blocks below 6th index are <3 and affect some graph metrics\n",
    "\n",
    "for key in extract_keys: # Change to [700:] for full range\n",
    "    \n",
    "    # Create block list from file_names\n",
    "    block_i=int(key.split(\".\")[0].split(\"/\")[-1]) \n",
    "    blocks.append(block_i)\n",
    "    \n",
    "    #Extract graphs - UNCOMMENT TO have them out of function\n",
    "    #G=dask.delayed(load_graph)(key)\n",
    "    #graph_snapshots.append(G)\n",
    "    \n",
    "   \n",
    "start_block=np.min(np.array(blocks))\n",
    "end_block=np.max(np.array(blocks))\n",
    "no_blocks=len(blocks)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total graph keys:36542\n",
      "Number of blocks to be processed:994\n",
      "---Sample graph keys---\n",
      "graph_snapshots/1587447789_connected/505149.gpickle\n",
      "---Sample channel opens---\n",
      "[(7521, 6156, {'capacity': 50000, 'open_fee': 306, 'dec_id': 1, 'channel_id': '513675x2245x0', 'no_channels': 0})]\n",
      "---Sample channel closures---\n",
      "[(2643, 6038, {'close_type': 'force', 'dec_id': 26620, 'channel_id': '570913x720x1', 'capacity': 300000}), (6038, 5314, {'close_type': 'mutual', 'dec_id': 0, 'channel_id': '505149x622x0', 'capacity': 300000})]\n"
     ]
    }
   ],
   "source": [
    "# Test: extracted formats\n",
    "print(\"Number of total graph keys:{}\".format(len(graph_keys)))\n",
    "print(\"Number of blocks to be processed:{}\".format(len(extract_keys)))\n",
    "print(\"---Sample graph keys---\")\n",
    "print(graph_keys[0])\n",
    "print(\"---Sample channel opens---\")\n",
    "print(channel_opens[513675])\n",
    "print(\"---Sample channel closures---\")\n",
    "print(channel_closures[592638])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of DataFrame in Memory:64821774\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>short_channel_id</th>\n",
       "      <th>open_block</th>\n",
       "      <th>open_transaction</th>\n",
       "      <th>address</th>\n",
       "      <th>close_block</th>\n",
       "      <th>close_transaction</th>\n",
       "      <th>node0</th>\n",
       "      <th>node1</th>\n",
       "      <th>satoshis</th>\n",
       "      <th>...</th>\n",
       "      <th>close_fee</th>\n",
       "      <th>last_update</th>\n",
       "      <th>close_type</th>\n",
       "      <th>close_htlc_count</th>\n",
       "      <th>close_balance_a</th>\n",
       "      <th>close_balance_b</th>\n",
       "      <th>dec_id</th>\n",
       "      <th>node0_id</th>\n",
       "      <th>node1_id</th>\n",
       "      <th>node_pair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72475</th>\n",
       "      <td>0</td>\n",
       "      <td>505149x622x0</td>\n",
       "      <td>505149</td>\n",
       "      <td>f6bc767df9148ebf76d2b9baf4eb46e3230712c2bf5a51...</td>\n",
       "      <td>bc1qjmg6ev344fenh3zhg0yjl6hyvxpxluw6x9nn2a5lv4...</td>\n",
       "      <td>592638.0</td>\n",
       "      <td>82cb2ea2a06c8c453d8b9ca08e17bbefe87225aa380b2d...</td>\n",
       "      <td>0250373555232cec757ea141273e75381c84cc3ab22f1e...</td>\n",
       "      <td>02ef61a252f9504a42fc264a28476f44cea0711a44b2da...</td>\n",
       "      <td>300000</td>\n",
       "      <td>...</td>\n",
       "      <td>184.0</td>\n",
       "      <td>1.563172e+09</td>\n",
       "      <td>mutual</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3570.0</td>\n",
       "      <td>296246.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6038</td>\n",
       "      <td>5314</td>\n",
       "      <td>32085932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72474</th>\n",
       "      <td>38787</td>\n",
       "      <td>506402x1391x1</td>\n",
       "      <td>506402</td>\n",
       "      <td>2cdfc4fec2049d66a04fa5bdf468efb19c0354c60b8cf2...</td>\n",
       "      <td>bc1qvjx5t8y7j83udzuj38ukmqecv5d9jn762mchxkgvaf...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0313f9449cdb528dc9707c02da507cc9306eedc415091c...</td>\n",
       "      <td>035f1498c929d4cefba4701ae36a554691f526ff60b176...</td>\n",
       "      <td>1111934</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38787</td>\n",
       "      <td>934</td>\n",
       "      <td>3023</td>\n",
       "      <td>2823482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72473</th>\n",
       "      <td>38788</td>\n",
       "      <td>506847x1633x0</td>\n",
       "      <td>506847</td>\n",
       "      <td>19ee11ce977facd380b92126834a3aca318f3cb905d99b...</td>\n",
       "      <td>bc1q29g43xrz9gujgt60gykzq3vh0ewfav7vmfqcnmf50u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>023d280ae29f84dcfd289eb66b57227fea3a7bde97ec28...</td>\n",
       "      <td>0273081ce642554d5a68a5236564fe88a3783457dc09e5...</td>\n",
       "      <td>40000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38788</td>\n",
       "      <td>3452</td>\n",
       "      <td>576</td>\n",
       "      <td>1988352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72472</th>\n",
       "      <td>38789</td>\n",
       "      <td>508075x1694x1</td>\n",
       "      <td>508075</td>\n",
       "      <td>e267e54872053a7618567f31a9d27e38cdbff0e4176144...</td>\n",
       "      <td>bc1qpxzqp2xyy0gzn8xwu6lqg3a66tuhsg5w849t0j5rdr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>03557fd11b58cb93d2ad4fab4dd4cff7462a97e21e8f6b...</td>\n",
       "      <td>03cbf298b068300be33f06c947b9d3f00a0f0e8089da32...</td>\n",
       "      <td>100000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38789</td>\n",
       "      <td>3436</td>\n",
       "      <td>3310</td>\n",
       "      <td>11373160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72471</th>\n",
       "      <td>58766</td>\n",
       "      <td>508090x1515x1</td>\n",
       "      <td>508090</td>\n",
       "      <td>33d645657de8a587137b8039e52452557d4279a3f47366...</td>\n",
       "      <td>bc1qneudwey0dpgy9nj2g8ech0lqqrhz52agcj984rs6zh...</td>\n",
       "      <td>616838.0</td>\n",
       "      <td>123777e4dfadc7c008a54c2d55b670067a58cdcbc8b2ec...</td>\n",
       "      <td>028314f021602092779aedd4ef39f3b5809f9b6046f8bc...</td>\n",
       "      <td>02d4531a2f2e6e5a9033d37d548cff4834a3898e74c3ab...</td>\n",
       "      <td>400000</td>\n",
       "      <td>...</td>\n",
       "      <td>4887.0</td>\n",
       "      <td>1.581152e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58766</td>\n",
       "      <td>2378</td>\n",
       "      <td>4223</td>\n",
       "      <td>10042294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0 short_channel_id  open_block  \\\n",
       "72475           0     505149x622x0      505149   \n",
       "72474       38787    506402x1391x1      506402   \n",
       "72473       38788    506847x1633x0      506847   \n",
       "72472       38789    508075x1694x1      508075   \n",
       "72471       58766    508090x1515x1      508090   \n",
       "\n",
       "                                        open_transaction  \\\n",
       "72475  f6bc767df9148ebf76d2b9baf4eb46e3230712c2bf5a51...   \n",
       "72474  2cdfc4fec2049d66a04fa5bdf468efb19c0354c60b8cf2...   \n",
       "72473  19ee11ce977facd380b92126834a3aca318f3cb905d99b...   \n",
       "72472  e267e54872053a7618567f31a9d27e38cdbff0e4176144...   \n",
       "72471  33d645657de8a587137b8039e52452557d4279a3f47366...   \n",
       "\n",
       "                                                 address  close_block  \\\n",
       "72475  bc1qjmg6ev344fenh3zhg0yjl6hyvxpxluw6x9nn2a5lv4...     592638.0   \n",
       "72474  bc1qvjx5t8y7j83udzuj38ukmqecv5d9jn762mchxkgvaf...          NaN   \n",
       "72473  bc1q29g43xrz9gujgt60gykzq3vh0ewfav7vmfqcnmf50u...          NaN   \n",
       "72472  bc1qpxzqp2xyy0gzn8xwu6lqg3a66tuhsg5w849t0j5rdr...          NaN   \n",
       "72471  bc1qneudwey0dpgy9nj2g8ech0lqqrhz52agcj984rs6zh...     616838.0   \n",
       "\n",
       "                                       close_transaction  \\\n",
       "72475  82cb2ea2a06c8c453d8b9ca08e17bbefe87225aa380b2d...   \n",
       "72474                                                NaN   \n",
       "72473                                                NaN   \n",
       "72472                                                NaN   \n",
       "72471  123777e4dfadc7c008a54c2d55b670067a58cdcbc8b2ec...   \n",
       "\n",
       "                                                   node0  \\\n",
       "72475  0250373555232cec757ea141273e75381c84cc3ab22f1e...   \n",
       "72474  0313f9449cdb528dc9707c02da507cc9306eedc415091c...   \n",
       "72473  023d280ae29f84dcfd289eb66b57227fea3a7bde97ec28...   \n",
       "72472  03557fd11b58cb93d2ad4fab4dd4cff7462a97e21e8f6b...   \n",
       "72471  028314f021602092779aedd4ef39f3b5809f9b6046f8bc...   \n",
       "\n",
       "                                                   node1  satoshis  ...  \\\n",
       "72475  02ef61a252f9504a42fc264a28476f44cea0711a44b2da...    300000  ...   \n",
       "72474  035f1498c929d4cefba4701ae36a554691f526ff60b176...   1111934  ...   \n",
       "72473  0273081ce642554d5a68a5236564fe88a3783457dc09e5...     40000  ...   \n",
       "72472  03cbf298b068300be33f06c947b9d3f00a0f0e8089da32...    100000  ...   \n",
       "72471  02d4531a2f2e6e5a9033d37d548cff4834a3898e74c3ab...    400000  ...   \n",
       "\n",
       "      close_fee   last_update  close_type  close_htlc_count  close_balance_a  \\\n",
       "72475     184.0  1.563172e+09      mutual               0.0           3570.0   \n",
       "72474       NaN           NaN         NaN               NaN              NaN   \n",
       "72473       NaN           NaN         NaN               NaN              NaN   \n",
       "72472       NaN           NaN         NaN               NaN              NaN   \n",
       "72471    4887.0  1.581152e+09         NaN               NaN              NaN   \n",
       "\n",
       "       close_balance_b dec_id  node0_id  node1_id  node_pair  \n",
       "72475         296246.0      0      6038      5314   32085932  \n",
       "72474              NaN  38787       934      3023    2823482  \n",
       "72473              NaN  38788      3452       576    1988352  \n",
       "72472              NaN  38789      3436      3310   11373160  \n",
       "72471              NaN  58766      2378      4223   10042294  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort and visualize DataFrame\n",
    "\n",
    "decisions_df.sort_values(by=['open_block'],inplace=True,ascending=True)\n",
    "print('Size of DataFrame in Memory:{}'.format(sys.getsizeof(decisions_df)))\n",
    "# Check specific channel id\n",
    "#decisions_df[decisions_df['short_channel_id']=='513675x2245x0'].head()\n",
    "\n",
    "decisions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TESTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Lazy Graph extract\n",
    "blocks_att=[]\n",
    "for i in range(len(graph_snapshots)):\n",
    "    graph_i=dask.compute(graph_snapshots[i])\n",
    "    block=graph_i.graph['block']\n",
    "    blocks_att.append(block)\n",
    "\n",
    "print(blocks_att)\n",
    "\n",
    "#graph_snapshots=dask.compute(*graph_snapshots)\n",
    "#block=graph_snapshots[0].graph['block']\n",
    "    \n",
    "#print(len(graph_snapshots[5]))\n",
    "#print(graph_snapshots[3].graph['block'])\n",
    "\n",
    "# Delayed testing\n",
    "#results = dask.compute(*futures)\n",
    "#graphs=dask.compute(*graph_snapshots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Comparative Analysis\n",
    "\n",
    "In order to understand the potential motivations behind each decision we analyze each decission (opening or closure of a channel) independently from the perspective of each of the participants in the decission, which we'll call the node under analysis. For each decission we extract or compute the following information: \n",
    "\n",
    "Betweenness centrality measures how central is a network to the flow of information in a network. In the case of the Lightning Network the higher the betweenness centrality of a node, the more transactions (messages) that are routed through it. In particular, we will use a measure of betweenness centrality defined in (Brandes and Fleischer 2005 - https://link.springer.com/chapter/10.1007/978-3-540-31856-9_44) that models infomation through a network, as electric current, efficiently and not only considering shortest path. This allows us to account for the fact that not all transactions travel through shortes path given that there are fee and capacity considerations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Measurments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Property measurement for a collection of graphs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEW\n",
    "# Function to calculate base measurement for every graph in snapshot\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "collection_measure\n",
    "    Iterates over graph snapshots and calculates measurement for every node in each of the graphs.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "g_snapshots : list\n",
    "    List of delayed nx graph elements contianing graph snapshots\n",
    "    \n",
    "\n",
    "measurement: string\n",
    "    Name of the type of measurment that will applied to the graph. (See graph_measurement function for options)\n",
    "\n",
    "\n",
    "\n",
    "Returns\n",
    "-------\n",
    "snapshot_mes_dic: dic\n",
    "    Dictionary with blocks as keys and dic of measurements as values\n",
    "\n",
    "\"\"\"\n",
    "def collection_measure(bucket,graph_keys,measurement):\n",
    "    \n",
    "    snapshot_mes_list=[]\n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    for key in graph_keys:\n",
    "      \n",
    "        \n",
    "        measurement_input=(key,measurement,'capacity',bucket)\n",
    "        \n",
    "        b_g_tuple=dask.delayed(graph_measurement)(measurement_input)\n",
    "        snapshot_mes_list.append(b_g_tuple)\n",
    "        \n",
    "\n",
    "    futures = dask.persist(*snapshot_mes_list)\n",
    "    \n",
    "    start=time.time()\n",
    "    snapshot_mes_list = dask.compute(*futures)\n",
    "    #snapshot_mes_list = dask.compute(*snapshot_mes_list)\n",
    "    snapshot_mes_dic={record[0]:record[1] for record in snapshot_mes_list}\n",
    "    end=time.time()\n",
    "    print('Compute in seconds: {}'.format(end-start))\n",
    "    \n",
    "    return snapshot_mes_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate base measurement for every graph in snapshot\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "collection_measure\n",
    "    Iterates over graph snapshots and calculates measurement for every node in each of the graphs.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "g_snapshots : list\n",
    "    List of delayed nx graph elements contianing graph snapshots\n",
    "    \n",
    "\n",
    "measurement: string\n",
    "    Name of the type of measurment that will applied to the graph. (See graph_measurement function for options)\n",
    "\n",
    "\n",
    "\n",
    "Returns\n",
    "-------\n",
    "snapshot_mes_dic: dic\n",
    "    Dictionary with blocks as keys and dic of measurements as values\n",
    "\n",
    "\n",
    "def collection_measure(g_snapshots,measurement):\n",
    "    \n",
    "    snapshot_mes_list=[]\n",
    "   \n",
    "    for g in graph_snapshots:\n",
    "      \n",
    "        \n",
    "        measurement_input=(g,measurement,'capacity')\n",
    "        \n",
    "        b_g_tuple=dask.delayed(graph_measurement)(measurement_input)\n",
    "        snapshot_mes_list.append(b_g_tuple)\n",
    "        \n",
    "\n",
    "    #futures = dask.persist(*snapshot_mes_list)\n",
    "    \n",
    "    start=time.time()\n",
    "    #snapshot_mes_list = dask.compute(*futures)\n",
    "    snapshot_mes_list = dask.compute(*snapshot_mes_list)\n",
    "    snapshot_mes_dic={record[0]:record[1] for record in snapshot_mes_list}\n",
    "    end=time.time()\n",
    "    print('Compute in seconds: {}'.format(end-start))\n",
    "    \n",
    "    return snapshot_mes_dic\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Property measurment for a single graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "###NEW\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "graph_measurement\n",
    "    Performs specific graph measurement \n",
    "\n",
    "Parameters\n",
    "----------\n",
    "measurment_input: tuple\n",
    "    g : nx graph\n",
    "        NetworkX graph object over which measurment will be calculated for each node\n",
    "\n",
    "    measurment: string\n",
    "        Type of measurement to be performed on graph\n",
    "        \n",
    "    weight: string\n",
    "        Edge attribute to be used as weight\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "g_dir: dir\n",
    "    Dictionary with measurment values for each node\n",
    "\"\"\"\n",
    "\n",
    "def graph_measurement(measurment_input):\n",
    "    \n",
    "    # Extract inputs\n",
    "    key=measurment_input[0]\n",
    "    measurement=measurment_input[1]\n",
    "    weight=measurment_input[2]\n",
    "    bucket=measurment_input[3]\n",
    "    \n",
    "    # Download graph\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.resource('s3')\n",
    "    response = s3.Object(bucket_name=bucket, key=key).get()\n",
    "    g=pickle.loads(response['Body'].read())\n",
    "    \n",
    "    \n",
    "    if measurement=='current_betweeness_full':\n",
    "        g_dir=nx.algorithms.centrality.current_flow_betweenness_centrality(g,weight=weight)\n",
    "    \n",
    "    elif measurement=='current_betweeness':\n",
    "        g_dir=nx.algorithms.centrality.approximate_current_flow_betweenness_centrality(g,weight=weight)\n",
    "    \n",
    "    elif measurement=='current_closeness':\n",
    "        g_dir=nx.algorithms.centrality.current_flow_closeness_centrality(g,weight=weight)\n",
    "    \n",
    "    elif measurement=='closeness':\n",
    "        g_dir=nx.closeness_centrality(g)\n",
    "        \n",
    "    elif measurement=='clustering':\n",
    "        g_dir=nx.clustering(g,weight=weight)\n",
    "        \n",
    "    elif measurement=='node_count':\n",
    "        g_dir=len(g.nodes())\n",
    "        \n",
    "    elif measurement=='channels':\n",
    "        g_dir=dict(list(g.degree(g.nodes())))\n",
    "    \n",
    "    elif measurement=='capacity':\n",
    "        g_dir=dict(list(g.degree(g.nodes(),weight=weight)))\n",
    "        \n",
    "    elif measurement=='age': \n",
    "        pass\n",
    "    elif measurement=='growth': \n",
    "        pass\n",
    "    \n",
    "    \n",
    "    # Safe graph processing to S3\n",
    "    \n",
    "    extraction_id=key.split('/')[1].split('_')[0]\n",
    "    block=key.split('/')[2].split('.')[0]\n",
    "    key_out='graph_snapshots/'+extraction_id+'_connected/.data_transformations/'+measurement+'/raw_score/'+block+'.pkl'\n",
    "    pickle_byte_obj = pickle.dumps(g_dir) \n",
    "    response=s3.Object(bucket,key_out).put(Body=pickle_byte_obj)['ResponseMetadata']['HTTPStatusCode']\n",
    "    \n",
    "    block=g.graph['block']\n",
    "    \n",
    "    return (block,response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph_snapshots/1587447789_connected/.data_transformations/current_betweeness_full/505149.pkl\n"
     ]
    }
   ],
   "source": [
    "key='graph_snapshots/1587447789_connected/505149.gpickle'\n",
    "extraction_id=key.split('/')[1].split('_')[0]\n",
    "block=key.split('/')[2].split('.')[0]\n",
    "measurement='current_betweeness_full'\n",
    "key_out='graph_snapshots/'+extraction_id+'_connected/.data_transformations/'+measurement+'/'+block+'.pkl'\n",
    "print (key_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "graph_measurement\n",
    "    Performs specific graph measurement \n",
    "\n",
    "Parameters\n",
    "----------\n",
    "measurment_input: tuple\n",
    "    g : nx graph\n",
    "        NetworkX graph object over which measurment will be calculated for each node\n",
    "\n",
    "    measurment: string\n",
    "        Type of measurement to be performed on graph\n",
    "        \n",
    "    weight: string\n",
    "        Edge attribute to be used as weight\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "g_dir: dir\n",
    "    Dictionary with measurment values for each node\n",
    "\n",
    "\n",
    "def graph_measurement(measurment_input):\n",
    "    \n",
    "    g=measurment_input[0]\n",
    "    measurement=measurment_input[1]\n",
    "    weight=measurment_input[2]\n",
    "    \n",
    "    if measurement=='current_betweeness':\n",
    "        g_dir=nx.algorithms.centrality.current_flow_betweenness_centrality(g,weight=weight)\n",
    "    \n",
    "    elif measurement=='current_closeness':\n",
    "        g_dir=nx.algorithms.centrality.current_flow_closeness_centrality(g,weight=weight)\n",
    "    \n",
    "    block=g.graph['block']\n",
    "    \n",
    "    return (block,g_dir)\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Property measurment for a couple of nodes in a graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "node_measurement\n",
    "    Performs selected graph measurment on specific nodes in graph\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "g : nx graph\n",
    "    NetworkX graph object over which the measurment will be performed\n",
    "\n",
    "measurement: string\n",
    "    Type of measurement to be performend in graph\n",
    "    \n",
    "node0: int\n",
    "    Node id for node 0\n",
    "\n",
    "node1: int\n",
    "    Node id for node 1\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "node_tuple: tuple\n",
    "    Tuple of the form (node0_mes,node1_mes)\n",
    "    \n",
    "    node0_mes: float\n",
    "        Graph measurement for node0\n",
    "    node1_mes: float\n",
    "        Graph measurement for node1\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "def node_measurement(g,measurement,node0,node1):\n",
    "    \n",
    "    measurement_input=(g,measurement,'capacity')\n",
    "    block,g_dir=graph_measurement(measurement_input)\n",
    "    \n",
    "    node0_mes=g_dir[node0]\n",
    "    node1_mes=g_dir[node1]\n",
    "        \n",
    "    # Update marginal values for node0 and node1\n",
    "        \n",
    "    if (g.has_node(node0)): #If connected component of marginal graph contains node0 find betweeness\n",
    "        node0_mes=g_dir[node0]\n",
    "    else: # else update with fixed value\n",
    "        node0_mes=0\n",
    "            \n",
    "    if (g.has_node(node1)): #If connected component of marginal graph contains node1 find betweeness\n",
    "        node1_mes=g_dir[node1]\n",
    "    else: # else update with fixed value\n",
    "        node1_mes=0\n",
    "    \n",
    "    return (node0_mes,node1_mes)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incremental_closeness(bucket,graph_keys,blocks):\n",
    "    \n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.resource('s3')\n",
    "    counter=0\n",
    "    \n",
    "    \n",
    "    # Channel closures\n",
    "    closure_file = s3.Object(bucket_name=bucket, key='channel_closures.p').get()\n",
    "    channel_closures = pickle.loads(closure_file['Body'].read())\n",
    "    \n",
    "    \n",
    "    # Channel openings \n",
    "    opens_file = s3.Object(bucket_name=bucket, key='channel_opens.p').get()\n",
    "    channel_opens = pickle.loads(opens_file['Body'].read())\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i  in range(1,len(graph_keys)):\n",
    "        \n",
    "        graph_keys\n",
    "        \n",
    "        response = s3.Object(bucket_name=bucket, key=key).get()\n",
    "        G=pickle.loads(response['Body'].read())\n",
    "        block=g.graph['block']\n",
    "        new_edges=list(itertools.chain.from_iterable([opens for block_key,opens in channel_opens.items() if block_key==block]))\n",
    "        closed_edges=list(itertools.chain.from_iterable([closures for block_key,closures in channel_closures.items() if block_key==block])) \n",
    "    \n",
    "        \n",
    "        if counter==0:\n",
    "            closeness=nx.closeness_centrality(g)\n",
    "\n",
    "        else:\n",
    "\n",
    "\n",
    "            # Add edges from CHANNEL OPENS\n",
    "\n",
    "            for edge in new_edges:\n",
    "\n",
    "                #If edge existed, update value and channel counter\n",
    "                if G.has_edge(edge[0],edge[1]):\n",
    "\n",
    "                    base_capacity=G.edges[edge[0],edge[1]]['capacity']\n",
    "                    added_capacity=edge[2]['capacity']\n",
    "                    G.edges[edge[0],edge[1]]['capacity']=base_capacity+added_capacity\n",
    "\n",
    "                else:\n",
    "                    G.add_edges_from([edge])\n",
    "\n",
    "                # Increase channel counter for edge\n",
    "                G.edges[edge[0],edge[1]]['no_channels']+=1\n",
    "\n",
    "\n",
    "\n",
    "            # Remove edges from CHANNEL CLOSURES\n",
    "\n",
    "            ## Create temp list of closures that removes metadata to remove from graph and apply modifications\n",
    "            #closure_pairs=[(closure[0],closure[1]) for closure in closed_edges]\n",
    "\n",
    "\n",
    "            for edge in closed_edges:\n",
    "\n",
    "                # Verify if existing edges result from multiple channels, if so, only reduce capacity otherwise remove edge\n",
    "\n",
    "                no_channels=G.edges[edge[0],edge[1]]['no_channels']\n",
    "                if no_channels>1:\n",
    "                    base_capacity=G.edges[edge[0],edge[1]]['capacity']\n",
    "                    reduced_capacity=edge[2]['capacity']\n",
    "                    G.edges[edge[0],edge[1]]['capacity']=base_capacity-reduced_capacity\n",
    "                    G.edges[edge[0],edge[1]]['no_channels']-=1 # Decrease counter\n",
    "                else:\n",
    "                    G.remove_edges_from([(edge[0],edge[1])])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With aproximate betweeness: 3.782655715942383s \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nmeasurement_input_2=(extract_keys[10000],'current_betweeness_full','capacity',bucket)\\n\\nstart=time.time()\\ntest_full = graph_measurement(measurement_input_2)\\nend=time.time()\\n\\nprint('With full betweeness: {}s '.format(end-start))\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "measurement_input=(extract_keys[35000],'clustering','capacity',bucket)\n",
    "\n",
    "start=time.time()\n",
    "test_aprox = graph_measurement(measurement_input)\n",
    "end=time.time()\n",
    "\n",
    "print('With aproximate betweeness: {}s '.format(end-start))\n",
    "\n",
    "'''\n",
    "measurement_input_2=(extract_keys[10000],'current_betweeness_full','capacity',bucket)\n",
    "\n",
    "start=time.time()\n",
    "test_full = graph_measurement(measurement_input_2)\n",
    "end=time.time()\n",
    "\n",
    "print('With full betweeness: {}s '.format(end-start))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute in seconds: 10.438627481460571\n"
     ]
    }
   ],
   "source": [
    "snapshot_nodes=collection_measure(bucket,extract_keys,'node_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(snapshot_nodes.items())[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Baseline Current Betweeness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute in seconds: 8009.403836488724\n"
     ]
    }
   ],
   "source": [
    "# Calculate Baseline betweeness\n",
    "snapshot_bet=collection_measure(bucket,extract_keys,'current_betweeness')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save to S3 succesful\n"
     ]
    }
   ],
   "source": [
    "# Save baseline current betweeness to S3\n",
    "response=pickle_save_s3(snapshot_bet,blocks,extraction_id,'snapshot_bet')\n",
    "if response['HTTPStatusCode']==200:\n",
    "    print('Save to S3 succesful')\n",
    "else:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Baseline Current Closeness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Baseline current closeness\n",
    "snapshot_clo=collection_measure(bucket,extract_keys,'current_closeness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n",
      "_GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=CancelledError()>\n",
      "concurrent.futures._base.CancelledError\n",
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n",
      "_GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=CancelledError()>\n",
      "concurrent.futures._base.CancelledError\n"
     ]
    }
   ],
   "source": [
    "snapshot_clo=collection_measure(bucket,extract_keys,'closeness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST all saved correctly\n",
    "responses=np.array([r for b,r in snapshot_clo.items()])\n",
    "\n",
    "if np.amax(responses)==np.amin(responses) and len(responses) == len(extract_keys):\n",
    "    print('Save to S3 succesful')\n",
    "else:\n",
    "    print('Error saving to S3')\n",
    "    \n",
    "measurement='current_closeness'    \n",
    "rand_block=str(random.choice(blocks))    \n",
    "test_key='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/raw_score/'+rand_block+'.pkl'\n",
    "test_file = s3.Object(bucket_name=bucket, key=test_key).get()\n",
    "test_object = pickle.loads(test_file['Body'].read())\n",
    "print('Example of {} saved for block {}:'.format(measurement,rand_block))\n",
    "print(test_object.items()[:10])    \n",
    "print('Total entries: {}'.format(len(list(test_object.items()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Baseline Channels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute in seconds: 186.33308386802673\n"
     ]
    }
   ],
   "source": [
    "# Calculate Baseline degree\n",
    "snapshot_channels=collection_measure(bucket,extract_keys,'channels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save to S3 succesful\n",
      "Example of channels saved for block 593261:\n",
      "[(5314, 7), (934, 3), (3023, 1), (3452, 4), (576, 1), (3436, 38), (3310, 1), (2378, 1), (4223, 9), (422, 39)]\n",
      "Total entries: 5154\n"
     ]
    }
   ],
   "source": [
    "# TEST all saved correctly\n",
    "responses=np.array([r for b,r in snapshot_channels.items()])\n",
    "\n",
    "if np.amax(responses)==np.amin(responses) and len(responses) == len(extract_keys):\n",
    "    print('Save to S3 succesful')\n",
    "else:\n",
    "    print('Error saving to S3')\n",
    "    \n",
    "measurement='channels'    \n",
    "rand_block=str(random.choice(blocks))    \n",
    "test_key='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/raw_score/'+rand_block+'.pkl'\n",
    "test_file = s3.Object(bucket_name=bucket, key=test_key).get()\n",
    "test_object = pickle.loads(test_file['Body'].read())\n",
    "print('Example of {} saved for block {}:'.format(measurement,rand_block))\n",
    "print(list(test_object.items())[:10])    \n",
    "print('Total entries: {}'.format(len(list(test_object.items()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Baseline Capacity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute in seconds: 205.32206177711487\n"
     ]
    }
   ],
   "source": [
    "# Calculate Baseline degree\n",
    "snapshot_capacity=collection_measure(bucket,extract_keys,'capacity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save to S3 succesful\n",
      "Example of capacity saved for block 586042:\n",
      "[(6038, 8878679), (5314, 14026340), (934, 1171934), (3023, 1111934), (3452, 2063908), (576, 40000), (3436, 6948131), (3310, 100000), (2378, 400000), (4223, 24298841)]\n",
      "Total entries: 4920\n"
     ]
    }
   ],
   "source": [
    "# TEST all saved correctly\n",
    "responses=np.array([r for b,r in snapshot_capacity.items()])\n",
    "\n",
    "if np.amax(responses)==np.amin(responses) and len(responses) == len(extract_keys):\n",
    "    print('Save to S3 succesful')\n",
    "else:\n",
    "    print('Error saving to S3')\n",
    "    \n",
    "measurement='capacity'    \n",
    "rand_block=str(random.choice(blocks))    \n",
    "test_key='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/raw_score/'+rand_block+'.pkl'\n",
    "test_file = s3.Object(bucket_name=bucket, key=test_key).get()\n",
    "test_object = pickle.loads(test_file['Body'].read())\n",
    "print('Example of {} saved for block {}:'.format(measurement,rand_block))\n",
    "print(list(test_object.items())[:10])\n",
    "print('Total entries: {}'.format(len(list(test_object.items()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparissons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Compare property changes for nodes in a collection of graphs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###NEW \n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "collection_compare\n",
    "    Iterates over blocks to calculate marginal change in metric for nodes that made decisions (opens/closures)\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "blocks : list\n",
    "    List of blocks extracted when reading graphs\n",
    "    \n",
    "    \n",
    "dec_dic: dic\n",
    "    Dictionary with channel decisions (open or closure) per block\n",
    "    \n",
    "    \n",
    "graph_snapshots: list\n",
    "    List of delayed dask objects each pointing to a graph snapshot to be loaded from S3\n",
    "\n",
    "snapshots_base: dic\n",
    "    Dictionary of dictionaries containing the base measurments per block per node\n",
    "\n",
    "measurement: string\n",
    "    Name of the type of measurment that will applied to the graph\n",
    "    \n",
    "type_dec: string\n",
    "    The type of decisions that will be analyzed 'opens' or 'closures'\n",
    "    \n",
    "\n",
    "Returns\n",
    "-------\n",
    "futures: list\n",
    "    List of tuples of the form (dic_node0,dic_node1) where dic_node0/1 is a dictionary containing the marginal changes for node0/1 \n",
    "    for every decision in a given block. The dictionaries are future dask objects that still need to be explicitly computed. \n",
    "\n",
    "\"\"\"\n",
    "def collection_compare(blocks,dec_dic,graph_keys,snapshots_base,measurement,type_dec):\n",
    "\n",
    "    futures_list=[] # list to populate with futures per block\n",
    "    \n",
    "    with tqdm(total=len(range(1,len(graph_keys)))) as pbar:\n",
    "        for i in range(1,len(graph_keys)): # iterate through blocks\n",
    "\n",
    "            # extract information from parameters and construct input tuple to delayed function\n",
    "            block=blocks[i]\n",
    "            block_prev=blocks[i-1]\n",
    "            block_dec=dec_dic[block]\n",
    "            key=graph_keys[i-1] # Pass previous graph\n",
    "            block_base=snapshots_base[block_prev]\n",
    "            block_res=snapshots_base[block]\n",
    "            input_tuple=(block,g,block_dec,block_base,measurement,type_dec,block_res)\n",
    "            \n",
    "            # submit to delayed function and append to list\n",
    "            output_tuple=dask.delayed(graph_compare)(input_tuple)\n",
    "            futures_list.append(output_tuple)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # persist to disk and return\n",
    "    futures = dask.persist(*futures_list)\n",
    "    return futures\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "collection_compare\n",
    "    Iterates over blocks to calculate marginal change in metric for nodes that made decisions (opens/closures)\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "blocks : list\n",
    "    List of blocks extracted when reading graphs\n",
    "    \n",
    "    \n",
    "dec_dic: dic\n",
    "    Dictionary with channel decisions (open or closure) per block\n",
    "    \n",
    "    \n",
    "graph_snapshots: list\n",
    "    List of delayed dask objects each pointing to a graph snapshot to be loaded from S3\n",
    "\n",
    "snapshots_base: dic\n",
    "    Dictionary of dictionaries containing the base measurments per block per node\n",
    "\n",
    "measurement: string\n",
    "    Name of the type of measurment that will applied to the graph\n",
    "    \n",
    "type_dec: string\n",
    "    The type of decisions that will be analyzed 'opens' or 'closures'\n",
    "    \n",
    "\n",
    "Returns\n",
    "-------\n",
    "futures: list\n",
    "    List of tuples of the form (dic_node0,dic_node1) where dic_node0/1 is a dictionary containing the marginal changes for node0/1 \n",
    "    for every decision in a given block. The dictionaries are future dask objects that still need to be explicitly computed. \n",
    "\n",
    "\n",
    "def collection_compare(blocks,dec_dic,graph_snapshots,snapshots_base,measurement,type_dec):\n",
    "\n",
    "    futures_list=[] # list to populate with futures per block\n",
    "    \n",
    "    with tqdm(total=len(range(1,len(graph_snapshots)))) as pbar:\n",
    "        for i in range(1,len(graph_snapshots)): # iterate through blocks\n",
    "\n",
    "            # extract information from parameters and construct input tuple to delayed function\n",
    "            block=blocks[i]\n",
    "            block_prev=blocks[i-1]\n",
    "            block_dec=dec_dic[block]\n",
    "            g=graph_snapshots[i-1] # Pass previous graph\n",
    "            block_base=snapshots_base[block_prev]\n",
    "            block_res=snapshots_base[block]\n",
    "            input_tuple=(block,g,block_dec,block_base,measurement,type_dec,block_res)\n",
    "            \n",
    "            # submit to delayed function and append to list\n",
    "            output_tuple=dask.delayed(graph_compare)(input_tuple)\n",
    "            futures_list.append(output_tuple)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # persist to disk and return\n",
    "    futures = dask.persist(*futures_list)\n",
    "    return futures\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Compare property changes for nodes in a graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "graph_compare\n",
    "    Calculates marginal change in metric for node0, node1 make decisions (open/close channels) in a single block\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "input_tuple : tuple\n",
    "    \n",
    "    block: int\n",
    "        Block number\n",
    "    g: nx_graph \n",
    "        Graph snapshot (as dask delayed object)\n",
    "    block_dec: list\n",
    "        List of tuples in nx edge format (u,v,att_dic) for all the decisions (channel opens/closures) made in that block  \n",
    "    block_base: dic\n",
    "        Dictionary of base measurements for each node in the graph snapshot corresponding to that block\n",
    "    measurement: string\n",
    "        Name of measurement to be computed\n",
    "    type_dec: string\n",
    "    The type of decisions that will be analyzed 'opens' or 'closures'\n",
    "    \n",
    "    block_res: dic\n",
    "        Dictionary of base measurements for each node in the graph snapshot corresponding to the next block\n",
    "    \n",
    "\n",
    "Returns\n",
    "-------\n",
    "nodes_mar_dic: tuple\n",
    "    Tuples of the form (mar_node0_dic_i,mar_node0_dic_i) where each element in the tuple is a dictionary containing the marginal changes for node0/1 \n",
    "    for every node0 and node1 involved in a decision (channel open/closures) in the block.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def graph_compare(input_tuple):\n",
    "    \n",
    "    block=input_tuple[0]\n",
    "    key=input_tuple[1]\n",
    "    block_dec=input_tuple[2]\n",
    "    block_base=input_tuple[3]\n",
    "    measurement=input_tuple[4]\n",
    "    type_dec=input_tuple[5]\n",
    "    block_res=input_tuple[6]\n",
    "   \n",
    "    mar_node0_dic_i={} # dictionary to story function output\n",
    "    mar_node1_dic_i={} \n",
    "    \n",
    "    \n",
    "    # Load data\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.resource('s3')\n",
    "    response = s3.Object(bucket_name=bucket, key=key).get()\n",
    "    G=pickle.loads(response['Body'].read())\n",
    "    \n",
    "    ###########################---------------------\n",
    "    ##if decisiono \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # For each decision calculate marginal change in measure for node0 and node1\n",
    "    for edge in block_dec:\n",
    "        \n",
    "        # Extract info about channel\n",
    "        \n",
    "        node0=edge[0]\n",
    "        node1=edge[1]\n",
    "        channel_id=edge[2]['channel_id']\n",
    "        capacity=edge[2]['capacity']\n",
    "\n",
    "        \n",
    "        #Â Copy original graph\n",
    "        g_mar=G.copy()   \n",
    "        old_nodes=False\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Retrieve base measurement before channel if nodes existed, else define base measure as 0\n",
    "        if (g_mar.has_node(node0)):\n",
    "            node0_base=block_base[node0]\n",
    "            old_nodes=True\n",
    "        else:\n",
    "            node0_base=0\n",
    "            \n",
    "        if (g_mar.has_node(node1)):\n",
    "            node1_base=block_base[node1]\n",
    "            old_nodes=True\n",
    "        else:\n",
    "            node1_base=0\n",
    "        \n",
    "            \n",
    "        if old_nodes: # If at least one node is old (part of the connected graph)\n",
    "            \n",
    "            if type_dec=='mar_opens': # marginal calculation for opens\n",
    "                \n",
    "                \n",
    "                # Define and add edges and calculate betweeness if at least one of the nodes is in graph \n",
    "                edge_list=[edge]\n",
    "                \n",
    "                \n",
    "                # If channel exists increase capacity\n",
    "                \n",
    "                if g_mar.has_edge(node0,node1):\n",
    "                   \n",
    "                    g_mar.edges[node0,node1]['capacity']+=capacity\n",
    "                    g_mar.edges[node0,node1]['no_channels']+=1\n",
    "\n",
    "                else:\n",
    "                    g_mar.add_edges_from(edge_list)\n",
    "                \n",
    "                \n",
    "                g_mar_mes=node_measurement(g_mar,measurement,node0,node1)\n",
    "                \n",
    "                # Update measurement values after marginal change\n",
    "                node0_new_mes=g_mar_mes[0]\n",
    "                node1_new_mes=g_mar_mes[1]\n",
    "            \n",
    "            elif type_dec=='mar_closures': # marginal calculation for closes\n",
    "                \n",
    "                # Define and remove edges, define new connected graph and calculate betweeness \n",
    "                edge_list=[(node0,node1)]\n",
    "                \n",
    "                \n",
    "                # If channel exists decrease capacity\n",
    "                if g_mar.edges[node0,node1]['no_channels']>1:\n",
    "                    g_mar.edges[node0,node1]['capacity']-=capacity\n",
    "                    g_mar.edges[node0,node1]['no_channels']-=1\n",
    "                \n",
    "                else: \n",
    "                    g_mar.remove_edges_from(edge_list) \n",
    "                    connected_components=[c for c in nx.algorithms.components.connected_components(g_mar)]\n",
    "                    g_mar=g_mar.subgraph(connected_components[0]).copy()\n",
    "                    \n",
    "                g_mar_mes=node_measurment(g_mar,measurement,node0,node1)\n",
    "                node0_new_mes=g_mar_mes[0]\n",
    "                node1_new_mes=g_mar_mes[1]\n",
    "                \n",
    "            elif type_dec=='actual': # actual calculation for both opens and closures\n",
    "                \n",
    "                # Check individualy if in the graph for the resulting block the node is present (in the connected component, \n",
    "                # if not assign measurment to 0. \n",
    "                \n",
    "                try:\n",
    "                    node0_new_mes=block_res[node0]\n",
    "\n",
    "                except KeyError:\n",
    "                    node0_new_mes=0\n",
    "\n",
    "                try:\n",
    "                    node1_new_mes=block_res[node1]\n",
    "\n",
    "                except KeyError:\n",
    "                    node1_new_mes=0\n",
    "\n",
    "                \n",
    "                   \n",
    "            node0_mar=(node0_new_mes-node0_base)\n",
    "            node1_mar=(node1_new_mes-node1_base) \n",
    "        \n",
    "        \n",
    "        else: # If both nodes are new (outside of connected graph) their marginal decision outcome is 0\n",
    "            node0_mar=0\n",
    "            node1_mar=0\n",
    "\n",
    "        \n",
    "        # Update dictionary - new betweenness\n",
    "        mar_node0_dic_i[channel_id]=node0_mar\n",
    "        mar_node1_dic_i[channel_id]=node1_mar\n",
    "        \n",
    "    \n",
    "    return (mar_node0_dic_i,mar_node1_dic_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MARGINAL - CURRENT BETWEENESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Marginal current betweeness for opens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute marginal betweeness for channel openings\n",
    "\n",
    "futures_bet_maropen=collection_compare(blocks,channel_opens,graph_snapshots,snapshot_bet,measurement='current_betweeness',type_dec='mar_opens')\n",
    "start=time.time()\n",
    "bet_maropen_diclist = dask.compute(*futures_bet_maropen)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(bet_maropen_diclist)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to marginal current betweeness for opens into decisions Dataframe\n",
    "\n",
    "bet_maropen_channels=add_columns(bet_maropen_diclist,decisions_df,'bet_maropen_node0','bet_maropen_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with marginal current betweeness for opens: {}'.format(len(bet_maropen_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(bet_maropen_channels)].head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Marginal current betweeness for closures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute marginal betweeness for channel closures\n",
    "\n",
    "futures_bet_marclose=collection_compare(blocks,channel_closures,graph_snapshots,snapshot_bet,measurement='current_betweeness',type_dec='mar_closures')\n",
    "start=time.time()\n",
    "bet_marclose_diclist = dask.compute(*futures_bet_marclose)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(bet_marclose_diclist)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to marginal current betweeness for closures into decisions Dataframe\n",
    "\n",
    "bet_marclose_channels=add_columns(bet_marclose_diclist,decisions_df,'bet_marclose_node0','bet_marclose_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with marginal current betweeness for closures: {}'.format(len(bet_marclose_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(bet_marclose_channels)].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MARGINAL - CURRENT CLOSENESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Marginal current closeness for opens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute marginal betweeness for channel openings\n",
    "\n",
    "futures_clo_maropen=collection_compare(blocks,channel_opens,graph_snapshots,snapshot_clo,measurement='current_closeness',type_dec='mar_opens')\n",
    "start=time.time()\n",
    "clo_maropen_diclist = dask.compute(*futures_clo_maropen)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(clo_maropen_diclist)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to marginal current betweeness for opens into decisions Dataframe\n",
    "\n",
    "clo_maropen_channels=add_columns(clo_maropen_diclist,decisions_df,'clo_maropen_node0','clo_maropen_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with marginal current closeness for opens: {}'.format(len(clo_maropen_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(clo_maropen_channels)].head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Marginal current closeness for closures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute marginal current closeness for channel closures\n",
    "\n",
    "futures_clo_marclose=collection_compare(blocks,channel_closures,graph_snapshots,snapshot_clo,measurement='current_closeness',type_dec='mar_closures')\n",
    "start=time.time()\n",
    "clo_marclose_diclist = dask.compute(*futures_clo_marclose)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(clo_marclose_diclist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to marginal current betweeness for opens into decisions Dataframe\n",
    "\n",
    "clo_marclose_channels=add_columns(clo_marclose_diclist,decisions_df,'clo_marclose_node0','clo_marclose_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with marginal current closeness for closures: {}'.format(len(clo_marclose_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(clo_marclose_channels)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACTUAL - CURRENT BETWEENESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Actual current betweeness for opens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute actual current betweeness for channel opens\n",
    "\n",
    "futures_bet_actopen=collection_compare(blocks,channel_opens,graph_snapshots,snapshot_bet,measurement='current_betweeness',type_dec='actual')\n",
    "start=time.time()\n",
    "bet_actopen_diclist = dask.compute(*futures_bet_actopen)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(bet_actopen_diclist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to marginal current betweeness for opens into decisions Dataframe\n",
    "\n",
    "bet_actopen_channels=add_columns(bet_actopen_diclist,decisions_df,'bet_actopen_node0','bet_actopen_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with actual current betweeness for opens: {}'.format(len(bet_actopen_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(bet_actopen_channels)].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Actual current betweeness for closures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute actual current betweeness for channel closures\n",
    "\n",
    "futures_bet_actclose=collection_compare(blocks,channel_closures,graph_snapshots,snapshot_bet,measurement='current_betweeness',type_dec='actual')\n",
    "start=time.time()\n",
    "bet_actclose_diclist = dask.compute(*futures_bet_actclose)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(bet_actclose_diclist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to marginal current betweeness for opens into decisions Dataframe\n",
    "\n",
    "bet_actclose_channels=add_columns(bet_actclose_diclist,decisions_df,'bet_actclose_node0','bet_actclose_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with actual current betweeness for closures: {}'.format(len(bet_actclose_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(bet_actclose_channels)].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACTUAL - CURRENT CLOSENESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Actual current closeness for opens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute actual current closeness for channel opens\n",
    "\n",
    "futures_clo_actopen=collection_compare(blocks,channel_opens,graph_snapshots,snapshot_bet,measurement='current_closeness',type_dec='actual')\n",
    "start=time.time()\n",
    "clo_actopen_diclist = dask.compute(*futures_clo_actopen)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(bet_actopen_diclist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to marginal current closeness for opens into decisions Dataframe\n",
    "\n",
    "clo_actopen_channels=add_columns(clo_actopen_diclist,decisions_df,'clo_actopen_node0','clo_actopen_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with actual current closeness for opens: {}'.format(len(clo_actopen_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(clo_actopen_channels)].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Actual current closeness for closures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute actual current closeness for channel closures\n",
    "\n",
    "futures_clo_actclose=collection_compare(blocks,channel_closures,graph_snapshots,snapshot_bet,measurement='current_closeness',type_dec='actual')\n",
    "start=time.time()\n",
    "clo_actclose_diclist = dask.compute(*futures_clo_actclose)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(clo_actclose_diclist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to marginal current closeness for opens into decisions Dataframe\n",
    "\n",
    "clo_actclose_channels=add_columns(clo_actclose_diclist,decisions_df,'clo_actclose_node0','clo_actclose_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with actual current closeness for closures: {}'.format(len(clo_actclose_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(clo_actclose_channels)].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise stability \n",
    "\n",
    "- **Marginal betweenness (bet_mar_nodei)**: The % change between the betweenness centrality, for the node under analysis, given the graph from the previous block and the betweenness centrality of the resulting graph after enacting the decission (adding or removing a channel). Weighted current betweenness centrality is used for this measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Marginal betweenness for opens** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Why is length of Dataframe longer than the number of snapshots extracted? Could it be that some channels appear more than once in dataframe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Marginal betweenness for closures** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Actual change in betweenness (bet_act_nodei)**: The % change between the betweenness centrality, for the node under analysis, given the graph from the previous block and the betweenness centrality of the resulting graph after enacting **all** the decissions (adding or removing a channels) in the current block. Weighted current betweenness centrality is used for this measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Marginal betweeness pairwise stability (bet_mar_pairst/open/close)**: Evaluates if given the marginal graph that results from just enacting this decission is consistent with pairwise stability, from a betweenness perspective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MARGINAL - Add column with check for pairwise stability compatability using marginal outcomes\n",
    "\n",
    "# OPEN - Channel is opened if both nodes gain\n",
    "\n",
    "# Define function \n",
    "def bet_pairst_maropen(row):\n",
    "    if not math.isnan(row['bet_mar_node0']):\n",
    "        pairst=(row['bet_maropen_node0']>=0 and row['bet_maropen_node1']>=0)\n",
    "    else:\n",
    "        pairst=row['bet_maropen_node0']\n",
    "    return pairst\n",
    "\n",
    "# Apply function\n",
    "decisions_df['bet_mar_pairstopen']=decisions_df.apply(bet_pairst_maropen,axis=1)\n",
    "\n",
    "# CLOSE - Channel is closed if at least one node is better off\n",
    "\n",
    "# Define function \n",
    "def bet_pairst_marclose(row):\n",
    "    if not math.isnan(row['bet_marclose_node0']):\n",
    "        pairst=(row['bet_marclose_node0']>0 or row['bet_marclose_node1']>0)\n",
    "    else:\n",
    "        pairst=row['bet_marclose_node0']\n",
    "    return pairst\n",
    "\n",
    "# Apply function\n",
    "decisions_df['bet_mar_pairstclose']=decisions_df.apply(bet_pairst_marclose,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MARGINAL OPEN\n",
    "decisions_df[decisions_df['bet_mar_node0'].notnull()][['bet_mar_node0','bet_mar_node1','bet_mar_pairstopen']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MARGINAL CLOSE\n",
    "decisions_df[decisions_df['bet_marclose_node0'].notnull()][['bet_marclose_node0','bet_marclose_node1','bet_mar_pairstclose']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Actual betweeness pairwise stability (bet_act_pairstopen/close)**: Evaluates if given the marginal graph that results from all the decisions in the block is consitend with pairwise stability, from a betweenness perspective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTUAL - Add column with check for pairwise stability compatability using marginal outcomes\n",
    "\n",
    "# OPEN - Channel is opened if both nodes gain\n",
    "\n",
    "# Define function \n",
    "def bet_pairst_actopen(row):\n",
    "    if not math.isnan(row['bet_actopen_node0']):\n",
    "        pairst=(row['bet_actopen_node0']>=0 and row['bet_actopen_node1']>=0)\n",
    "    else:\n",
    "        pairst=row['bet_actopen_node0']\n",
    "    return pairst\n",
    "\n",
    "# Apply function\n",
    "decisions_df['bet_act_pairstopen']=decisions_df.apply(bet_pairst_actopen,axis=1)\n",
    "\n",
    "# CLOSE - Channel is closed if at least one node is better off\n",
    "\n",
    "# Define function \n",
    "def bet_pairst_actclose(row):\n",
    "    if not math.isnan(row['bet_actclose_node0']):\n",
    "        pairst=(row['bet_actclose_node0']>0 or row['bet_actclose_node1']>0)\n",
    "    else:\n",
    "        pairst=row['bet_actclose_node0']\n",
    "    return pairst\n",
    "\n",
    "# Apply function\n",
    "decisions_df['bet_act_pairstclose']=decisions_df.apply(bet_pairst_actclose,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ACTUAL OPEN\n",
    "decisions_df[decisions_df['bet_actopen_node0'].notnull()][['bet_actopen_node0','bet_actopen_node1','bet_act_pairstopen']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ACTUAL CLOSE\n",
    "decisions_df[decisions_df['bet_actclose_node0'].notnull()][['bet_actclose_node0','bet_actclose_node1','bet_act_pairstclose']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Updated DataFrame to S3\n",
    "\n",
    "# Create S3 resource and define values\n",
    "session = boto3.session.Session()\n",
    "s3 = session.resource('s3')\n",
    "csv_buffer = io.StringIO()\n",
    "\n",
    "# File path and name ([extraction_id]snapshot_bet-[no_blocks]-[start_block]-[end_block])\n",
    "key_decisions_df='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+str(extraction_id)+'decisions_df_bet-'+str(no_blocks)+'-'+str(start_block)+'-'+str(end_block)+'.csv'\n",
    "\n",
    "\n",
    "# Safe DataFrame\n",
    "decisions_df.to_csv(csv_buffer)\n",
    "s3.Object(bucket, key_decisions_df).put(Body=csv_buffer.getvalue())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Save\n",
    "decisions_df_test_load = s3.Object(bucket_name=bucket, key=key_decisions_df).get()\n",
    "decisions_df_test=pd.read_csv(io.BytesIO(decisions_df_test_load['Body'].read()),index_col=0)\n",
    "decisions_df_test==decisions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions_df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency\n",
    "- **Average betweeness per block (bet_effic)**: Average betweenness centrality for all the nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nash stability \n",
    "\n",
    "- **% Change with respect to not making decision (bet_binstat_deltai)**: The % change in betwewnness centrality, for the node under analysis, given the resulting graph after all of the decissions have been executed. \n",
    "- **Nash compatible - binary strategy (bet_binstat_nash)**: Returns true if given the other decissions enacted in the block not making decision would have NOT have resulted in higher betweenness centrality. This tells me if my strategy helped me be better off (took into account what others were doing)\n",
    "\n",
    "(Optional approaches - Check for tracktability)\n",
    "- **Nash compatible - close only strategy (bet_closestat_nash)**: Returns true if given the other decissions enacted in the block, closing any other channels would NOT have not resulted in higher betwneenness centrality. (NOTE: Check if there are combinatorial considerations, if so just look at closings up to x) \n",
    "- **Nash compatible - close/open (bet_allstat_nash)**: Returns true if given the other decissions enacted in the block, closing any other channels (with any node) or opening a channel with one of the round participants would NOT have not resulted in lower betwneenness centrality. (NOTE: To make it reasonable and constraint the strategy space only consider 'similar nodes' or with relationships in the past?).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Connectivity\n",
    "\n",
    "### Pairwise stability \n",
    "\n",
    "- **Marginal % change in connectivity (con_mar_deltai)**: The % change between the shortest path average, for the node under analysis, given the graph from the previous block and the shortest path average of the resulting graph after enacting the decission (adding or removing a channel). Weighted shortest path (_single_source_dijkstra_path_) is used for this measure.\n",
    "\n",
    "- **Actual % change in connectivity (con_act_deltai)**: The % change between the shortest path average, for the node under analysis, given the graph from the previous block and the shortest path average of the resulting graph after enacting **all** the decissions (adding or removing a channels) in the current block. Weighted shortest path (_single_source_dijkstra_path_) is used for this measure.\n",
    "\n",
    "- **Marginal connectivity pairwise stability (con_mar_pairstab)**: Evaluates if given the marginal graph that results from just enacting this decission is consistent with pairwise stability, from a connectivity perspective.\n",
    "\n",
    "- **Actual connectivity pairwise stability (con_act_pairstab)**: Evaluates if given the marginal graph that results from all the decisions in the block is consitend with pairwise stability, from a connectivity perspective.  \n",
    "\n",
    "\n",
    "\n",
    "### Nash stability \n",
    "\n",
    "- **% Change with respect to not making decision (con_binstat_deltai)**: The % change in shortest path average, for the node under analysis, given the resulting graph after all of the decissions have been executed. \n",
    "- **Nash compatible - binary strategy (con_binstat_nash)**: Returns true if given the other decissions enacted in the block not making decision would have NOT have resulted in higher shortest path average. NOTE: This indicates if the strategy selected made the node better off (took into account what others were doing)\n",
    "\n",
    "(Optional approaches - Check for tracktability)\n",
    "- **Nash compatible - close only strategy (con_closestat_nash)**: Returns true if given the other decissions enacted in the block, closing any other channels would NOT have not resulted in higher shortest path average. (NOTE: Check if there are combinatorial considerations, if so just look at closings up to x) \n",
    "- **Nash compatible - close/open (con_allstat_nash)**: Returns true if given the other decissions enacted in the block, closing any other channels (with any node) or opening a channel with one of the round participants would NOT have not resulted in lower shortest path average. (NOTE: To make it reasonable and constraint the strategy space only consider 'similar nodes' or with relationships in the past?).\n",
    "\n",
    "\n",
    "\n",
    "### Efficiency\n",
    "- **Average betweeness per block (bet_effic)**: Average shortest path average for all the nodes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
