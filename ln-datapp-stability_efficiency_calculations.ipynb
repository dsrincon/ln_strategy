{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LN - Data PP - Stability and efficiency calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import itertools\n",
    "#import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import io\n",
    "import random\n",
    "from itertools import islice\n",
    "import math\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "#from tqdm.notebook import trange\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "from time import sleep\n",
    "\n",
    "from dask_cloudprovider import FargateCluster\n",
    "from dask.distributed import Client\n",
    "import dask.array as da\n",
    "import dask\n",
    "dask.config.set({'distributed.scheduler.allowed-failures': 50}) \n",
    "\n",
    "\n",
    "import boto3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "\n",
    "bucket='ln-strategy-data'\n",
    "extraction_id=1587447789\n",
    "#extraction_id=1585344554"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection to AWS Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate s3 resource\n",
    "\n",
    "session = boto3.session.Session()\n",
    "s3 = session.resource('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fargate Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster = FargateCluster(n_workers=100,scheduler_timeout='20 minutes',image='dsrincon/dask-graph:nx-scipy-v1',scheduler_cpu=4096,scheduler_mem=16384,worker_mem=32768)\n",
    "cluster = FargateCluster(n_workers=20,scheduler_timeout='10 minutes',image='dsrincon/dask-graph:nx-scipy-v1',scheduler_cpu=4096,scheduler_mem=16384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "958b369f3b9445edb3d408cdeecb7104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>FargateCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n  â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/distributed/client.py:1079: VersionMismatchWarning: Mismatched versions found\n",
      "\n",
      "python\n",
      "+---------------------------+---------------+\n",
      "|                           | version       |\n",
      "+---------------------------+---------------+\n",
      "| client                    | 3.7.3.final.0 |\n",
      "| scheduler                 | 3.7.4.final.0 |\n",
      "| tcp://172.31.10.138:46825 | 3.7.4.final.0 |\n",
      "| tcp://172.31.11.169:42625 | 3.7.4.final.0 |\n",
      "| tcp://172.31.19.210:41527 | 3.7.4.final.0 |\n",
      "| tcp://172.31.2.56:33483   | 3.7.4.final.0 |\n",
      "| tcp://172.31.20.202:34557 | 3.7.4.final.0 |\n",
      "| tcp://172.31.21.138:37815 | 3.7.4.final.0 |\n",
      "| tcp://172.31.22.25:38343  | 3.7.4.final.0 |\n",
      "| tcp://172.31.26.50:41149  | 3.7.4.final.0 |\n",
      "| tcp://172.31.27.64:37437  | 3.7.4.final.0 |\n",
      "| tcp://172.31.29.168:42035 | 3.7.4.final.0 |\n",
      "| tcp://172.31.29.26:37969  | 3.7.4.final.0 |\n",
      "| tcp://172.31.29.41:44035  | 3.7.4.final.0 |\n",
      "| tcp://172.31.30.139:36575 | 3.7.4.final.0 |\n",
      "| tcp://172.31.32.196:40005 | 3.7.4.final.0 |\n",
      "| tcp://172.31.32.50:46365  | 3.7.4.final.0 |\n",
      "| tcp://172.31.33.2:45037   | 3.7.4.final.0 |\n",
      "| tcp://172.31.37.153:46491 | 3.7.4.final.0 |\n",
      "| tcp://172.31.39.211:38567 | 3.7.4.final.0 |\n",
      "| tcp://172.31.39.216:33667 | 3.7.4.final.0 |\n",
      "| tcp://172.31.42.156:41737 | 3.7.4.final.0 |\n",
      "| tcp://172.31.43.246:45123 | 3.7.4.final.0 |\n",
      "| tcp://172.31.44.110:40453 | 3.7.4.final.0 |\n",
      "| tcp://172.31.46.152:42649 | 3.7.4.final.0 |\n",
      "| tcp://172.31.48.168:36045 | 3.7.4.final.0 |\n",
      "| tcp://172.31.51.59:43903  | 3.7.4.final.0 |\n",
      "| tcp://172.31.54.245:43527 | 3.7.4.final.0 |\n",
      "| tcp://172.31.54.39:35449  | 3.7.4.final.0 |\n",
      "| tcp://172.31.55.130:39665 | 3.7.4.final.0 |\n",
      "| tcp://172.31.55.190:46667 | 3.7.4.final.0 |\n",
      "| tcp://172.31.57.14:45753  | 3.7.4.final.0 |\n",
      "| tcp://172.31.6.15:41805   | 3.7.4.final.0 |\n",
      "| tcp://172.31.6.2:43193    | 3.7.4.final.0 |\n",
      "| tcp://172.31.6.74:38641   | 3.7.4.final.0 |\n",
      "| tcp://172.31.62.243:39301 | 3.7.4.final.0 |\n",
      "| tcp://172.31.63.147:35601 | 3.7.4.final.0 |\n",
      "| tcp://172.31.63.253:34911 | 3.7.4.final.0 |\n",
      "| tcp://172.31.64.190:37619 | 3.7.4.final.0 |\n",
      "| tcp://172.31.65.154:37065 | 3.7.4.final.0 |\n",
      "| tcp://172.31.68.102:40449 | 3.7.4.final.0 |\n",
      "| tcp://172.31.69.125:42311 | 3.7.4.final.0 |\n",
      "| tcp://172.31.69.177:40741 | 3.7.4.final.0 |\n",
      "| tcp://172.31.7.73:37919   | 3.7.4.final.0 |\n",
      "| tcp://172.31.71.63:45235  | 3.7.4.final.0 |\n",
      "| tcp://172.31.71.80:37313  | 3.7.4.final.0 |\n",
      "| tcp://172.31.74.115:44043 | 3.7.4.final.0 |\n",
      "| tcp://172.31.75.78:42673  | 3.7.4.final.0 |\n",
      "| tcp://172.31.76.56:37845  | 3.7.4.final.0 |\n",
      "| tcp://172.31.77.253:40197 | 3.7.4.final.0 |\n",
      "| tcp://172.31.80.65:41545  | 3.7.4.final.0 |\n",
      "| tcp://172.31.82.10:40047  | 3.7.4.final.0 |\n",
      "| tcp://172.31.82.132:41415 | 3.7.4.final.0 |\n",
      "| tcp://172.31.83.182:37161 | 3.7.4.final.0 |\n",
      "| tcp://172.31.84.177:39739 | 3.7.4.final.0 |\n",
      "| tcp://172.31.84.96:42457  | 3.7.4.final.0 |\n",
      "| tcp://172.31.87.186:34607 | 3.7.4.final.0 |\n",
      "| tcp://172.31.9.236:33829  | 3.7.4.final.0 |\n",
      "| tcp://172.31.9.76:35817   | 3.7.4.final.0 |\n",
      "| tcp://172.31.90.166:46371 | 3.7.4.final.0 |\n",
      "| tcp://172.31.92.179:35827 | 3.7.4.final.0 |\n",
      "| tcp://172.31.92.6:37193   | 3.7.4.final.0 |\n",
      "+---------------------------+---------------+\n",
      "\n",
      "tornado\n",
      "+---------------------------+---------+\n",
      "|                           | version |\n",
      "+---------------------------+---------+\n",
      "| client                    | 6.0.3   |\n",
      "| scheduler                 | 6.0.4   |\n",
      "| tcp://172.31.10.138:46825 | 6.0.4   |\n",
      "| tcp://172.31.11.169:42625 | 6.0.4   |\n",
      "| tcp://172.31.19.210:41527 | 6.0.4   |\n",
      "| tcp://172.31.2.56:33483   | 6.0.4   |\n",
      "| tcp://172.31.20.202:34557 | 6.0.4   |\n",
      "| tcp://172.31.21.138:37815 | 6.0.4   |\n",
      "| tcp://172.31.22.25:38343  | 6.0.4   |\n",
      "| tcp://172.31.26.50:41149  | 6.0.4   |\n",
      "| tcp://172.31.27.64:37437  | 6.0.4   |\n",
      "| tcp://172.31.29.168:42035 | 6.0.4   |\n",
      "| tcp://172.31.29.26:37969  | 6.0.4   |\n",
      "| tcp://172.31.29.41:44035  | 6.0.4   |\n",
      "| tcp://172.31.30.139:36575 | 6.0.4   |\n",
      "| tcp://172.31.32.196:40005 | 6.0.4   |\n",
      "| tcp://172.31.32.50:46365  | 6.0.4   |\n",
      "| tcp://172.31.33.2:45037   | 6.0.4   |\n",
      "| tcp://172.31.37.153:46491 | 6.0.4   |\n",
      "| tcp://172.31.39.211:38567 | 6.0.4   |\n",
      "| tcp://172.31.39.216:33667 | 6.0.4   |\n",
      "| tcp://172.31.42.156:41737 | 6.0.4   |\n",
      "| tcp://172.31.43.246:45123 | 6.0.4   |\n",
      "| tcp://172.31.44.110:40453 | 6.0.4   |\n",
      "| tcp://172.31.46.152:42649 | 6.0.4   |\n",
      "| tcp://172.31.48.168:36045 | 6.0.4   |\n",
      "| tcp://172.31.51.59:43903  | 6.0.4   |\n",
      "| tcp://172.31.54.245:43527 | 6.0.4   |\n",
      "| tcp://172.31.54.39:35449  | 6.0.4   |\n",
      "| tcp://172.31.55.130:39665 | 6.0.4   |\n",
      "| tcp://172.31.55.190:46667 | 6.0.4   |\n",
      "| tcp://172.31.57.14:45753  | 6.0.4   |\n",
      "| tcp://172.31.6.15:41805   | 6.0.4   |\n",
      "| tcp://172.31.6.2:43193    | 6.0.4   |\n",
      "| tcp://172.31.6.74:38641   | 6.0.4   |\n",
      "| tcp://172.31.62.243:39301 | 6.0.4   |\n",
      "| tcp://172.31.63.147:35601 | 6.0.4   |\n",
      "| tcp://172.31.63.253:34911 | 6.0.4   |\n",
      "| tcp://172.31.64.190:37619 | 6.0.4   |\n",
      "| tcp://172.31.65.154:37065 | 6.0.4   |\n",
      "| tcp://172.31.68.102:40449 | 6.0.4   |\n",
      "| tcp://172.31.69.125:42311 | 6.0.4   |\n",
      "| tcp://172.31.69.177:40741 | 6.0.4   |\n",
      "| tcp://172.31.7.73:37919   | 6.0.4   |\n",
      "| tcp://172.31.71.63:45235  | 6.0.4   |\n",
      "| tcp://172.31.71.80:37313  | 6.0.4   |\n",
      "| tcp://172.31.74.115:44043 | 6.0.4   |\n",
      "| tcp://172.31.75.78:42673  | 6.0.4   |\n",
      "| tcp://172.31.76.56:37845  | 6.0.4   |\n",
      "| tcp://172.31.77.253:40197 | 6.0.4   |\n",
      "| tcp://172.31.80.65:41545  | 6.0.4   |\n",
      "| tcp://172.31.82.10:40047  | 6.0.4   |\n",
      "| tcp://172.31.82.132:41415 | 6.0.4   |\n",
      "| tcp://172.31.83.182:37161 | 6.0.4   |\n",
      "| tcp://172.31.84.177:39739 | 6.0.4   |\n",
      "| tcp://172.31.84.96:42457  | 6.0.4   |\n",
      "| tcp://172.31.87.186:34607 | 6.0.4   |\n",
      "| tcp://172.31.9.236:33829  | 6.0.4   |\n",
      "| tcp://172.31.9.76:35817   | 6.0.4   |\n",
      "| tcp://172.31.90.166:46371 | 6.0.4   |\n",
      "| tcp://172.31.92.179:35827 | 6.0.4   |\n",
      "| tcp://172.31.92.6:37193   | 6.0.4   |\n",
      "+---------------------------+---------+\n",
      "  warnings.warn(version_module.VersionMismatchWarning(msg[0][\"warning\"]))\n"
     ]
    }
   ],
   "source": [
    "client = Client(cluster)\n",
    "#cluster=Client('tcp://18.234.80.68:8786')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Write output to DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function write output to DataFrame\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "add_columns\n",
    "    Function that takes an output from a decision comparisson computation and adds it's results for nodes 1 and 0 in the main DataFrame\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "output_diclist : list\n",
    "    Dictionary of the form (node0_dic_i,node1_dic_i) where i runs for all of the blocks being compared. \n",
    "\n",
    "original_df: Pandas DataFrame\n",
    "    Original DataFrame containing the opening and closure information for each channel, with a column named 'short_channel_id' to denote \n",
    "    id of channel. \n",
    "\n",
    "column_name_node0: string\n",
    "    Name for column in dataframe where the results will be stored for node 0\n",
    "    \n",
    "column_name_node1: string\n",
    "    Name for column in dataframe where the results will be stored for node 1\n",
    "\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "no_changes: list\n",
    "    List with the 'short_channel_id' of the channels edited. \n",
    " \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def add_columns(output_diclist,original_df,column_name_node0,column_name_node1):\n",
    "\n",
    "\n",
    "    # Merge individual dictionaries into one for each node\n",
    "    node0_dic={}\n",
    "    node1_dic={}\n",
    "    for dic_tuple in output_diclist:\n",
    "        node0_dic.update(dic_tuple[0])\n",
    "        node1_dic.update(dic_tuple[1])\n",
    "    \n",
    "    # Add to DataFrame\n",
    "\n",
    "    # Create empty columns\n",
    "    original_df[column_name_node0]=np.nan\n",
    "    original_df[column_name_node1]=np.nan\n",
    "\n",
    "    # Populate df with values\n",
    "    original_df[column_name_node0]=original_df['short_channel_id'].map(node0_dic)\n",
    "    original_df[column_name_node1]=original_df['short_channel_id'].map(node1_dic)\n",
    "    \n",
    "    # Calculate values changed\n",
    "    rows_edited=(original_df[original_df[column_name_node0].notnull()]['short_channel_id']).tolist()\n",
    "    \n",
    "    return rows_edited\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Save python object to S3 using pickle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to write pickle data to S3 bucket\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "pickle_save_s3\n",
    "    Saves Python object to S3 as pickle object\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "obj : <any>\n",
    "    Python Object\n",
    "\n",
    "blocks: list\n",
    "    List of extracted blocks\n",
    "\n",
    "extraction_id: int\n",
    "    Number of block extraction\n",
    "    \n",
    "name: string\n",
    "    Name of object to add to filename in S3\n",
    "\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "response: int\n",
    "    HTTP response code from S3 \n",
    " \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def pickle_save_s3(obj,blocks,extraction_id,name):\n",
    "\n",
    "\n",
    "    # Define number of blocks\n",
    "    start_block=np.min(np.array(blocks))\n",
    "    end_block=np.max(np.array(blocks))\n",
    "    no_blocks=len(blocks)\n",
    "\n",
    "    # Load S3 and bucket details\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.resource('s3')\n",
    "\n",
    "    # File path and name ([extraction_id][name]-[no_blocks]-[start_block]-[end_block])\n",
    "    key='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+str(extraction_id)+name+'-'+str(no_blocks)+'-'+str(start_block)+'-'+str(end_block)+'.pkl'\n",
    "\n",
    "    # Create pickle object and send to S3\n",
    "    pickle_byte_obj = pickle.dumps(obj) \n",
    "    response=s3.Object(bucket,key).put(Body=pickle_byte_obj)\n",
    "    \n",
    "    return response['ResponseMetadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to write pickle data to S3 bucket\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "simple_psave_s3\n",
    "    Saves Python object to S3 as pickle object\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "input_tuple: tuple\n",
    "    \n",
    "    bucket: str\n",
    "        S3 bucket to save\n",
    "    key: str\n",
    "        S3 key to save\n",
    "    obj: obj\n",
    "        python object to save\n",
    "\n",
    "Returns\n",
    "-------\n",
    "response: int\n",
    "    HTTP response code from S3 \n",
    " \n",
    "\"\"\"\n",
    "def simple_psave_s3(input_tuple):\n",
    "    \n",
    "    bucket=input_tuple[0]\n",
    "    key=input_tuple[1]\n",
    "    obj=input_tuple[2]\n",
    "    \n",
    "    # Start S3 session\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.resource('s3')\n",
    "    \n",
    "    # Save to S3\n",
    "    pickle_byte_obj = pickle.dumps(obj) \n",
    "    response=s3.Object(bucket,key).put(Body=pickle_byte_obj)['ResponseMetadata']['HTTPStatusCode']\n",
    "    \n",
    "    return response\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Load single graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "load_graph\n",
    "    Loads networkX (pickle serialized) object from S3\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "key : str\n",
    "    Path in S3 bucket for individual pickled serialized networkX graph object \n",
    "\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "response: networkX graph\n",
    "    Graph object\n",
    " \n",
    "\"\"\"\n",
    "\n",
    "def load_graph(key):\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.resource('s3')\n",
    "    response = s3.Object(bucket_name=bucket, key=key).get()\n",
    "    G=pickle.loads(response['Body'].read())\n",
    "    \n",
    "    return G\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Run graph measurement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "load_graph_measurement:\n",
    "    Runs graph measurement for every node in a specific block and loads the created dictionary from S3\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "extraction_id: int\n",
    "    Timestamp of block extraction\n",
    "\n",
    "test_ix: int\n",
    "    Index of block to test. Can be negative to move backwards in array\n",
    "\n",
    "measurement: str\n",
    "    Type of graph measurement to perform\n",
    "\n",
    "weight: str\n",
    "    Node attribute to use for weighted calculations\n",
    "\n",
    "blocks: list\n",
    "    List of (ints) blocks extracted \n",
    "\n",
    "graph_keys: list\n",
    "    List of (str) graph paths in S3\n",
    "\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "g_test: NetworkX graph\n",
    "    NetworkX graph extracted for the given ix\n",
    "\n",
    "nodes_test: list\n",
    "    List of nodes in g_test\n",
    "\n",
    "g_dic_test: dict\n",
    "    Dictionary of graph measurment for each node in g_test\n",
    "\n",
    "block: int\n",
    "    Block selected for test\n",
    " \n",
    "\"\"\"\n",
    "\n",
    "def load_graph_measurement(extraction_id,measurement,weight,blocks,graph_keys,test_ix=None):\n",
    "\n",
    "    if test_ix==None: # If no index provided choose one at random\n",
    "        test_ix=random.choice(range(len(blocks)))\n",
    "        \n",
    "        \n",
    "    \n",
    "    # Define block and graph key\n",
    "    test_block=blocks[test_ix]\n",
    "    g_key=graph_keys[test_ix]\n",
    "    print('Block selected:{}'.format(test_block))\n",
    "    \n",
    "    # Load graph and nodes\n",
    "    response = s3.Object(bucket_name=bucket, key=g_key).get()\n",
    "    g_test=pickle.loads(response['Body'].read())\n",
    "    nodes_test=list(g_test.nodes())\n",
    "\n",
    "    # Run function\n",
    "    block,response_test=graph_measurement((g_key,measurement,weight,bucket))\n",
    "\n",
    "\n",
    "    if response_test==200:\n",
    "        # Load created dictionary with calculation from S3\n",
    "        \n",
    "        key_test='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/raw_score/'+str(test_block)+'.pkl'\n",
    "        g__test_load = s3.Object(bucket_name=bucket, key=key_test).get()\n",
    "        g_dic_test = pickle.loads(g__test_load['Body'].read())\n",
    "        print('Dic was saved correctly. Sample below:')\n",
    "        print(list(g_dic_test.items())[:10])\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        print('Measurement was not saved correctly')\n",
    "        return\n",
    "    \n",
    "    return g_test,nodes_test,g_dic_test,test_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load objects form S3\n",
    "# Dataframe\n",
    "\n",
    "decisions_load = s3.Object(bucket_name=bucket, key='decisions_df.csv').get()\n",
    "decisions_df=pd.read_csv(io.BytesIO(decisions_load['Body'].read()))\n",
    "\n",
    "# Channel closures\n",
    "closure_file = s3.Object(bucket_name=bucket, key='channel_closures.p').get()\n",
    "channel_closures = pickle.loads(closure_file['Body'].read())\n",
    "    \n",
    "    \n",
    "# Channel openings \n",
    "opens_file = s3.Object(bucket_name=bucket, key='channel_opens.p').get()\n",
    "channel_opens = pickle.loads(opens_file['Body'].read())\n",
    "\n",
    "    \n",
    "\n",
    "# Create list with graph keys\n",
    "\n",
    "#TODO: Save graphs as numpy array in single H5 file to reduce. Test if creating graphs takes longer than reading from S3\n",
    "\n",
    "# graph_dir='./data/graph_snapshots' - For local tests\n",
    "\n",
    "\n",
    "graph_keys = [obj.key \n",
    "        for obj in s3.Bucket(name=bucket).objects.all()\n",
    "        if re.match(\".*\"+str(extraction_id)+\"_connected/.*\\.gpickle\",obj.key)]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36536"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Blocks to be extracted and define graph\n",
    "\n",
    "\n",
    "# Base lists to be populated\n",
    "graph_snapshots=[]\n",
    "blocks=[]\n",
    "base_ix=6\n",
    "\n",
    "\n",
    "extract_keys=graph_keys[base_ix:] # Blocks below 6th index are <3 and affect some graph metrics\n",
    "\n",
    "for key in extract_keys: # Change to [700:] for full range\n",
    "    \n",
    "    # Create block list from file_names\n",
    "    block_i=int(key.split(\".\")[0].split(\"/\")[-1]) \n",
    "    blocks.append(block_i)\n",
    "    \n",
    "    #Extract graphs - UNCOMMENT TO have them out of function\n",
    "    #G=dask.delayed(load_graph)(key)\n",
    "    #graph_snapshots.append(G)\n",
    "    \n",
    "   \n",
    "start_block=np.min(np.array(blocks))\n",
    "end_block=np.max(np.array(blocks))\n",
    "no_blocks=len(blocks)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Extract nodes and calculate age of nodes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29bb147ec1c34b8292dda6e8a059fd81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7735.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Details extracted and save to S3 succesful\n"
     ]
    }
   ],
   "source": [
    "# Define nodes and calculate details\n",
    "\n",
    "nodes=list(set(decisions_df['node0_id'].tolist()).union(set(decisions_df['node1_id'].tolist())))\n",
    "\n",
    "# DETAIL_1:Calculate birth block\n",
    "\n",
    "# Create list of sets (node pairs )\n",
    "opens_list=sorted(list(channel_opens.items()))\n",
    "open_list_sets=[(opens[0],[{t[0],t[1]} for t in opens[1]]) for opens in opens_list]\n",
    "\n",
    "# Dic to store details per node \n",
    "node_details={}\n",
    "\n",
    "\n",
    "with tqdm(total=len(nodes)) as pbar:\n",
    "    for node in nodes:\n",
    "\n",
    "        for opens in open_list_sets:\n",
    "            if opens[1] and node in set.union(*opens[1]):\n",
    "                birth_block=opens[0]\n",
    "                node_details[node]={'birth_block':birth_block}\n",
    "                break\n",
    "    \n",
    "        pbar.update(1)\n",
    "        \n",
    "\n",
    "# SAVE to S3\n",
    "\n",
    "key='node_details.p'\n",
    "pickle_byte_obj = pickle.dumps(node_details) \n",
    "response=s3.Object(bucket,key).put(Body=pickle_byte_obj)['ResponseMetadata']\n",
    "\n",
    "if response['HTTPStatusCode']==200:\n",
    "    print('Details extracted and save to S3 succesful')\n",
    "else: \n",
    "    print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details extracted and save to S3 succesful\n"
     ]
    }
   ],
   "source": [
    "# TESTS\n",
    "len_nodes=len(nodes)\n",
    "len_details=len(list(node_details.items()))\n",
    "\n",
    "\n",
    "if len_nodes==len_details and response['HTTPStatusCode']==200:\n",
    "    print('Details extracted and save to S3 succesful')\n",
    "else:\n",
    "    print('Details for {} nodes were not extracted'.format(len_nodes-len_details))   \n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total graph keys:36542\n",
      "Number of blocks to be processed:94\n",
      "---Sample graph keys---\n",
      "graph_snapshots/1587447789_connected/505149.gpickle\n",
      "---Sample channel opens---\n",
      "[(2378, 4223, {'capacity': 400000, 'open_fee': 4557, 'dec_id': 58766, 'channel_id': '508090x1515x1', 'no_channels': 0})]\n",
      "---Sample channel closures---\n",
      "[(2643, 6038, {'close_type': 'force', 'dec_id': 26620, 'channel_id': '570913x720x1', 'capacity': 300000}), (6038, 5314, {'close_type': 'mutual', 'dec_id': 0, 'channel_id': '505149x622x0', 'capacity': 300000})]\n"
     ]
    }
   ],
   "source": [
    "# Test: extracted formats\n",
    "print(\"Number of total graph keys:{}\".format(len(graph_keys)))\n",
    "print(\"Number of blocks to be processed:{}\".format(len(extract_keys)))\n",
    "print(\"---Sample graph keys---\")\n",
    "print(graph_keys[0])\n",
    "print(\"---Sample channel opens---\")\n",
    "print(channel_opens[508090])\n",
    "print(\"---Sample channel closures---\")\n",
    "print(channel_closures[592638])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of DataFrame in Memory:64821774\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>short_channel_id</th>\n",
       "      <th>open_block</th>\n",
       "      <th>open_transaction</th>\n",
       "      <th>address</th>\n",
       "      <th>close_block</th>\n",
       "      <th>close_transaction</th>\n",
       "      <th>node0</th>\n",
       "      <th>node1</th>\n",
       "      <th>satoshis</th>\n",
       "      <th>...</th>\n",
       "      <th>close_fee</th>\n",
       "      <th>last_update</th>\n",
       "      <th>close_type</th>\n",
       "      <th>close_htlc_count</th>\n",
       "      <th>close_balance_a</th>\n",
       "      <th>close_balance_b</th>\n",
       "      <th>dec_id</th>\n",
       "      <th>node0_id</th>\n",
       "      <th>node1_id</th>\n",
       "      <th>node_pair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72475</th>\n",
       "      <td>0</td>\n",
       "      <td>505149x622x0</td>\n",
       "      <td>505149</td>\n",
       "      <td>f6bc767df9148ebf76d2b9baf4eb46e3230712c2bf5a51...</td>\n",
       "      <td>bc1qjmg6ev344fenh3zhg0yjl6hyvxpxluw6x9nn2a5lv4...</td>\n",
       "      <td>592638.0</td>\n",
       "      <td>82cb2ea2a06c8c453d8b9ca08e17bbefe87225aa380b2d...</td>\n",
       "      <td>0250373555232cec757ea141273e75381c84cc3ab22f1e...</td>\n",
       "      <td>02ef61a252f9504a42fc264a28476f44cea0711a44b2da...</td>\n",
       "      <td>300000</td>\n",
       "      <td>...</td>\n",
       "      <td>184.0</td>\n",
       "      <td>1.563172e+09</td>\n",
       "      <td>mutual</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3570.0</td>\n",
       "      <td>296246.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6038</td>\n",
       "      <td>5314</td>\n",
       "      <td>32085932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72474</th>\n",
       "      <td>38787</td>\n",
       "      <td>506402x1391x1</td>\n",
       "      <td>506402</td>\n",
       "      <td>2cdfc4fec2049d66a04fa5bdf468efb19c0354c60b8cf2...</td>\n",
       "      <td>bc1qvjx5t8y7j83udzuj38ukmqecv5d9jn762mchxkgvaf...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0313f9449cdb528dc9707c02da507cc9306eedc415091c...</td>\n",
       "      <td>035f1498c929d4cefba4701ae36a554691f526ff60b176...</td>\n",
       "      <td>1111934</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38787</td>\n",
       "      <td>934</td>\n",
       "      <td>3023</td>\n",
       "      <td>2823482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72473</th>\n",
       "      <td>38788</td>\n",
       "      <td>506847x1633x0</td>\n",
       "      <td>506847</td>\n",
       "      <td>19ee11ce977facd380b92126834a3aca318f3cb905d99b...</td>\n",
       "      <td>bc1q29g43xrz9gujgt60gykzq3vh0ewfav7vmfqcnmf50u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>023d280ae29f84dcfd289eb66b57227fea3a7bde97ec28...</td>\n",
       "      <td>0273081ce642554d5a68a5236564fe88a3783457dc09e5...</td>\n",
       "      <td>40000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38788</td>\n",
       "      <td>3452</td>\n",
       "      <td>576</td>\n",
       "      <td>1988352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72472</th>\n",
       "      <td>38789</td>\n",
       "      <td>508075x1694x1</td>\n",
       "      <td>508075</td>\n",
       "      <td>e267e54872053a7618567f31a9d27e38cdbff0e4176144...</td>\n",
       "      <td>bc1qpxzqp2xyy0gzn8xwu6lqg3a66tuhsg5w849t0j5rdr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>03557fd11b58cb93d2ad4fab4dd4cff7462a97e21e8f6b...</td>\n",
       "      <td>03cbf298b068300be33f06c947b9d3f00a0f0e8089da32...</td>\n",
       "      <td>100000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38789</td>\n",
       "      <td>3436</td>\n",
       "      <td>3310</td>\n",
       "      <td>11373160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72471</th>\n",
       "      <td>58766</td>\n",
       "      <td>508090x1515x1</td>\n",
       "      <td>508090</td>\n",
       "      <td>33d645657de8a587137b8039e52452557d4279a3f47366...</td>\n",
       "      <td>bc1qneudwey0dpgy9nj2g8ech0lqqrhz52agcj984rs6zh...</td>\n",
       "      <td>616838.0</td>\n",
       "      <td>123777e4dfadc7c008a54c2d55b670067a58cdcbc8b2ec...</td>\n",
       "      <td>028314f021602092779aedd4ef39f3b5809f9b6046f8bc...</td>\n",
       "      <td>02d4531a2f2e6e5a9033d37d548cff4834a3898e74c3ab...</td>\n",
       "      <td>400000</td>\n",
       "      <td>...</td>\n",
       "      <td>4887.0</td>\n",
       "      <td>1.581152e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58766</td>\n",
       "      <td>2378</td>\n",
       "      <td>4223</td>\n",
       "      <td>10042294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72470</th>\n",
       "      <td>38790</td>\n",
       "      <td>508320x2072x0</td>\n",
       "      <td>508320</td>\n",
       "      <td>40944505d8e1a55e6923283cee629757500aa344142650...</td>\n",
       "      <td>bc1qcrsrc0xzt3uumh3uyftn600ku9vmamth50zdzfjpv2...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...</td>\n",
       "      <td>034a5fdb2df3ce1bfd2c2aca205ce9cfeef1a5f4af21b0...</td>\n",
       "      <td>1050</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38790</td>\n",
       "      <td>422</td>\n",
       "      <td>1912</td>\n",
       "      <td>806864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72469</th>\n",
       "      <td>38791</td>\n",
       "      <td>508400x1699x0</td>\n",
       "      <td>508400</td>\n",
       "      <td>71c790031c4b99593848dde6054366d0df2ad85c2806b7...</td>\n",
       "      <td>bc1qc4r2cdzucu2xncqv8sde97c0kumvqlrv9ehmfxykzu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0268cbff355499bf97387dec397614ab3bd0e3e4361475...</td>\n",
       "      <td>034a5fdb2df3ce1bfd2c2aca205ce9cfeef1a5f4af21b0...</td>\n",
       "      <td>1050</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38791</td>\n",
       "      <td>5154</td>\n",
       "      <td>1912</td>\n",
       "      <td>9854448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72465</th>\n",
       "      <td>38795</td>\n",
       "      <td>508447x661x0</td>\n",
       "      <td>508447</td>\n",
       "      <td>b592861b271aedadf6ad529b158df0e962b830e2b6056b...</td>\n",
       "      <td>bc1qxq49306phnepteaxx98gxx9tfjxw35umasvvep3w0k...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>023c5f2b69161742cc49da5be6cb7c940465fc608ff216...</td>\n",
       "      <td>0333b58f86a0c31da7ee61a4bc2232faccf8c4fe4167fe...</td>\n",
       "      <td>50000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38795</td>\n",
       "      <td>4688</td>\n",
       "      <td>2120</td>\n",
       "      <td>9938560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72468</th>\n",
       "      <td>38792</td>\n",
       "      <td>508447x652x0</td>\n",
       "      <td>508447</td>\n",
       "      <td>5b88d5737be23e70e62413461764e9ee1753d7c0fe90ba...</td>\n",
       "      <td>bc1qzp4ufw5jy4drk9kw56hvtmdeuvtqxnmvrdqjwaj6ms...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>02829de2d8762ac70f6a243c7e210e1a41d7bc9fe909d0...</td>\n",
       "      <td>0333b58f86a0c31da7ee61a4bc2232faccf8c4fe4167fe...</td>\n",
       "      <td>50000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38792</td>\n",
       "      <td>6656</td>\n",
       "      <td>2120</td>\n",
       "      <td>14110720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72467</th>\n",
       "      <td>38793</td>\n",
       "      <td>508447x656x0</td>\n",
       "      <td>508447</td>\n",
       "      <td>5f741522f787c5fde67f2fd73b220e9af6aff9e4ee700f...</td>\n",
       "      <td>bc1qe7uvtpg6klu0awfq479xz56pgm9mh9d67k7m3f6hc2...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>02952cc6b7f92226e490d6354636604a26484a396627c9...</td>\n",
       "      <td>0333b58f86a0c31da7ee61a4bc2232faccf8c4fe4167fe...</td>\n",
       "      <td>50000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38793</td>\n",
       "      <td>6595</td>\n",
       "      <td>2120</td>\n",
       "      <td>13981400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72466</th>\n",
       "      <td>38794</td>\n",
       "      <td>508447x659x0</td>\n",
       "      <td>508447</td>\n",
       "      <td>abc55727b4d5cfa39d84e48df410230d9c1b100b3cc07a...</td>\n",
       "      <td>bc1qhacv3gscv40muy3u57sr9msrqspa2lcdxrhehm04ak...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>023e2b9bb357b1bf135d59551418178ef020cdd77efee7...</td>\n",
       "      <td>0333b58f86a0c31da7ee61a4bc2232faccf8c4fe4167fe...</td>\n",
       "      <td>50000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38794</td>\n",
       "      <td>4119</td>\n",
       "      <td>2120</td>\n",
       "      <td>8732280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72462</th>\n",
       "      <td>38799</td>\n",
       "      <td>508503x2007x0</td>\n",
       "      <td>508503</td>\n",
       "      <td>e7ea8df19fbae1925cc33d1245aaa09eaeb6dd2cb20baa...</td>\n",
       "      <td>bc1q96v2p3ghqrg5xe3qr2zqa5y49g860ufs7sgm3qyf4x...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...</td>\n",
       "      <td>03617110be362e43a7445941970637d710eb46486d8a59...</td>\n",
       "      <td>20000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38799</td>\n",
       "      <td>422</td>\n",
       "      <td>7478</td>\n",
       "      <td>3155716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72461</th>\n",
       "      <td>38801</td>\n",
       "      <td>508503x2009x0</td>\n",
       "      <td>508503</td>\n",
       "      <td>46c8db8b6fed487c0f8e3ee6f69eb114afaa0028fd5b89...</td>\n",
       "      <td>bc1qu0jcnkf546c7rc3hc4wv0pnglwhp7kqylqf2edl33p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>02f2fccc11d34d7fce415a97ff9fd46867e5af780c9c71...</td>\n",
       "      <td>0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...</td>\n",
       "      <td>20000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38801</td>\n",
       "      <td>2953</td>\n",
       "      <td>422</td>\n",
       "      <td>1246166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72459</th>\n",
       "      <td>38797</td>\n",
       "      <td>508503x2005x0</td>\n",
       "      <td>508503</td>\n",
       "      <td>b0a2717efa8662542d74f029c92f231b9fad8784de6252...</td>\n",
       "      <td>bc1qt5jlls5r6j80mtmhmlrznaujwk9xnulaz7cxx9mf0m...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>03397f996a69f58ba4fd3d20e21daa36ff3be3c3624c03...</td>\n",
       "      <td>0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...</td>\n",
       "      <td>20000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38797</td>\n",
       "      <td>3957</td>\n",
       "      <td>422</td>\n",
       "      <td>1669854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72464</th>\n",
       "      <td>38802</td>\n",
       "      <td>508503x2010x0</td>\n",
       "      <td>508503</td>\n",
       "      <td>51a67ba1b325419bed2184683eb61678da4dd3d8cc9ab4...</td>\n",
       "      <td>bc1qdlstpk53ku7kazw3ptfqfqsaacdeudxlhuj4rqngx9...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>02d6284ccb2ce82f79e938a82da23b9f2d2fb659c5147e...</td>\n",
       "      <td>0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...</td>\n",
       "      <td>20000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38802</td>\n",
       "      <td>5725</td>\n",
       "      <td>422</td>\n",
       "      <td>2415950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72458</th>\n",
       "      <td>38798</td>\n",
       "      <td>508503x2006x0</td>\n",
       "      <td>508503</td>\n",
       "      <td>36596579fb974d8b189d831b12d30f291daefe20b89e0c...</td>\n",
       "      <td>bc1qd4uaz47jjzyuq5qc92lc72llzqkz0s4ljew0qhjd85...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...</td>\n",
       "      <td>038fe44b4159e224ecbd92a8c63c24948a807d39c08e2d...</td>\n",
       "      <td>20000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38798</td>\n",
       "      <td>422</td>\n",
       "      <td>5426</td>\n",
       "      <td>2289772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72460</th>\n",
       "      <td>38796</td>\n",
       "      <td>508503x1987x0</td>\n",
       "      <td>508503</td>\n",
       "      <td>4ae70b9e265b7e63ba6e41cc345aad72cce24b43f4d141...</td>\n",
       "      <td>bc1qqsvk880wywwmcr4wqk9tenc75lezxph9ekrsmpj89e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0230d1a9462fae76bc20e9f5eff885a70f4152d9eca912...</td>\n",
       "      <td>0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...</td>\n",
       "      <td>20000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38796</td>\n",
       "      <td>7059</td>\n",
       "      <td>422</td>\n",
       "      <td>2978898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72463</th>\n",
       "      <td>38800</td>\n",
       "      <td>508503x2008x0</td>\n",
       "      <td>508503</td>\n",
       "      <td>4a94ba64aea2537acc5ec8555911c2e2f58b454f7668e3...</td>\n",
       "      <td>bc1qpru56flmhvc9lpx5kjdsgrsw5z9vpkvsgp7z5zmlt4...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>03456c38eb93de16adf2555d5b3cf7030ce2537bac453d...</td>\n",
       "      <td>0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...</td>\n",
       "      <td>20000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38800</td>\n",
       "      <td>2518</td>\n",
       "      <td>422</td>\n",
       "      <td>1062596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72457</th>\n",
       "      <td>38803</td>\n",
       "      <td>508666x124x0</td>\n",
       "      <td>508666</td>\n",
       "      <td>2eee222e9d6bd4d8b2b44925549ace8f8d2d385a18b6e3...</td>\n",
       "      <td>bc1q9mn6zddje0y6l0fvhw37e39sktaxjdsk7wmd8en85m...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>026977ceabe8a4bd0f38153d781210b0bb685d8fea10a5...</td>\n",
       "      <td>0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...</td>\n",
       "      <td>1100</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38803</td>\n",
       "      <td>5294</td>\n",
       "      <td>422</td>\n",
       "      <td>2234068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72455</th>\n",
       "      <td>38805</td>\n",
       "      <td>508668x388x0</td>\n",
       "      <td>508668</td>\n",
       "      <td>c2b40e30fd34d134c52b79c5cb42fa0ea3538c4e1ae3c6...</td>\n",
       "      <td>bc1q73tg2twa7jdsvz2eq70n09n5c2gxpnmzu5zf5xdn30...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>022c715b377856f5e176c52dfbe1f19b0283c4a85d563b...</td>\n",
       "      <td>0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...</td>\n",
       "      <td>1100</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38805</td>\n",
       "      <td>3813</td>\n",
       "      <td>422</td>\n",
       "      <td>1609086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72454</th>\n",
       "      <td>38806</td>\n",
       "      <td>508668x390x0</td>\n",
       "      <td>508668</td>\n",
       "      <td>bbb79db464c0f445f5c7d3a760eac96c7ca75e709de537...</td>\n",
       "      <td>bc1qzk56xtm6kuay3p4a7z9cmwsqpfwlryzpw6cal67ns2...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0211ad124eaecaf2b223bad2f5381e81b3639f2a652b5a...</td>\n",
       "      <td>0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...</td>\n",
       "      <td>1100</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38806</td>\n",
       "      <td>3336</td>\n",
       "      <td>422</td>\n",
       "      <td>1407792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72456</th>\n",
       "      <td>38804</td>\n",
       "      <td>508668x387x0</td>\n",
       "      <td>508668</td>\n",
       "      <td>98830c56ea8130f6a5e7d5fca585f437711667c3947afb...</td>\n",
       "      <td>bc1qefmz0fr5kgzayl3vkwscqtaarcejdtrzxjze2qr6pl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...</td>\n",
       "      <td>03a98b60b81e126f5f588d8c77cf621ac3cd57bc445701...</td>\n",
       "      <td>1100</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38804</td>\n",
       "      <td>422</td>\n",
       "      <td>6317</td>\n",
       "      <td>2665774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72453</th>\n",
       "      <td>38808</td>\n",
       "      <td>508672x551x0</td>\n",
       "      <td>508672</td>\n",
       "      <td>8ee974b19881ad9f4b767402215a50e3ea130d0d6bc85e...</td>\n",
       "      <td>bc1q5a99ptwtzte60qxrln9k3g97t3gzgl95zqznftpqpy...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>020d3d5995a973c878e3f6e5f59da54078304c537f981d...</td>\n",
       "      <td>0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...</td>\n",
       "      <td>1100</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38808</td>\n",
       "      <td>7332</td>\n",
       "      <td>422</td>\n",
       "      <td>3094104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72452</th>\n",
       "      <td>38809</td>\n",
       "      <td>508672x553x0</td>\n",
       "      <td>508672</td>\n",
       "      <td>5deaa8910fec8e245417b9583583712703c7e938a9e42b...</td>\n",
       "      <td>bc1qm5up2f6n555akgls4fxj80emln956qsy5ztvyc4aqh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>025e6fdb6af937a959e70f83648542c63ba7dc478cecf2...</td>\n",
       "      <td>0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...</td>\n",
       "      <td>1100</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38809</td>\n",
       "      <td>2834</td>\n",
       "      <td>422</td>\n",
       "      <td>1195948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72451</th>\n",
       "      <td>38807</td>\n",
       "      <td>508672x550x0</td>\n",
       "      <td>508672</td>\n",
       "      <td>815659075445df3f1425b7f7199a8959fad78014d211d0...</td>\n",
       "      <td>bc1qr0zs76s2dsxdmtkg3tu4j5y9cxwnk4xphw672djn09...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>02587704695048661f32522b3ae2517260154443579161...</td>\n",
       "      <td>0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...</td>\n",
       "      <td>1100</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38807</td>\n",
       "      <td>1527</td>\n",
       "      <td>422</td>\n",
       "      <td>644394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72450</th>\n",
       "      <td>38810</td>\n",
       "      <td>508865x158x0</td>\n",
       "      <td>508865</td>\n",
       "      <td>fe684cd2d2bd274e6514593a5d6686d429300322b64ba8...</td>\n",
       "      <td>bc1qs2xv3qerg7t4fru4xrrwgz4w3dsnnf65krzdzk38td...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...</td>\n",
       "      <td>03e3d670d86f33181ee7451f14998b376a0d5deba8ab06...</td>\n",
       "      <td>1100</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38810</td>\n",
       "      <td>422</td>\n",
       "      <td>4271</td>\n",
       "      <td>1802362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72449</th>\n",
       "      <td>38811</td>\n",
       "      <td>508866x1427x0</td>\n",
       "      <td>508866</td>\n",
       "      <td>3602ffe5c7e9158b5ed7755a02a6cdbfa4ab62d98628f9...</td>\n",
       "      <td>bc1qpckwx22lr83p6h02us0p32y6j23hmpj74ynjs5yw29...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0312c53f40de91c5a1a64126a0f205116d4ce397ebefdb...</td>\n",
       "      <td>0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...</td>\n",
       "      <td>1100</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38811</td>\n",
       "      <td>5302</td>\n",
       "      <td>422</td>\n",
       "      <td>2237444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72448</th>\n",
       "      <td>38812</td>\n",
       "      <td>509031x1459x0</td>\n",
       "      <td>509031</td>\n",
       "      <td>bcbb33bffc6ae9493a5b8aee4fef58521d612c659f85c2...</td>\n",
       "      <td>bc1qg7uap38zak74s4cw63dk6uqda5ng2xlg8jptldy9ee...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>024b82d69859d63f9fce375041a34e61f4bfd445494b41...</td>\n",
       "      <td>0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...</td>\n",
       "      <td>1100</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38812</td>\n",
       "      <td>1331</td>\n",
       "      <td>422</td>\n",
       "      <td>561682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72447</th>\n",
       "      <td>38813</td>\n",
       "      <td>509031x1461x0</td>\n",
       "      <td>509031</td>\n",
       "      <td>9f632b465487dfee8e85dee82ce4f4d842336b9c6416a9...</td>\n",
       "      <td>bc1qswh7rqjq9et0j9ens04eu6qm37v8k6ute55fr3qzq2...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0253271f3a0cf1876dfaa381728ce1514d2254b0012725...</td>\n",
       "      <td>0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...</td>\n",
       "      <td>1100</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38813</td>\n",
       "      <td>6306</td>\n",
       "      <td>422</td>\n",
       "      <td>2661132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72446</th>\n",
       "      <td>39583</td>\n",
       "      <td>509340x749x1</td>\n",
       "      <td>509340</td>\n",
       "      <td>abb9c04681dec8fb1ad0b5fef6e520ea232e4f405e86f7...</td>\n",
       "      <td>bc1q6qda36cth9py079hs9899cn76w2aq0r56766df0tfh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>034593537dc19047de45ef90ec22b83243b184df595a61...</td>\n",
       "      <td>0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...</td>\n",
       "      <td>450000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39583</td>\n",
       "      <td>2446</td>\n",
       "      <td>422</td>\n",
       "      <td>1032212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72445</th>\n",
       "      <td>38815</td>\n",
       "      <td>509496x1530x0</td>\n",
       "      <td>509496</td>\n",
       "      <td>d99eb3dabf350ebf3465ec47d32420231a49f42991c1c0...</td>\n",
       "      <td>bc1q0u9grd4xj0wsgl2cx2wzrq70z4swjz6k3e3m5ladsq...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0347469dea682bd59255a5e60eaed8079186f85b5b5ea6...</td>\n",
       "      <td>0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...</td>\n",
       "      <td>1050</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38815</td>\n",
       "      <td>5784</td>\n",
       "      <td>422</td>\n",
       "      <td>2440848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72444</th>\n",
       "      <td>38814</td>\n",
       "      <td>509496x1440x1</td>\n",
       "      <td>509496</td>\n",
       "      <td>e576b581ad717a69394fb7ded3f60aff261f14d7ddc4d5...</td>\n",
       "      <td>bc1qkzav59cldqqd9jqa6lze9d8rcs3ll30k0mjlrkdsep...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>03220b9d15ee4fdd5ab3cf16bee7b640ae648962aef50e...</td>\n",
       "      <td>03557fd11b58cb93d2ad4fab4dd4cff7462a97e21e8f6b...</td>\n",
       "      <td>250000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38814</td>\n",
       "      <td>7586</td>\n",
       "      <td>3436</td>\n",
       "      <td>26065496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72443</th>\n",
       "      <td>38816</td>\n",
       "      <td>509891x774x0</td>\n",
       "      <td>509891</td>\n",
       "      <td>2bfde8bf73c09b7a36de9b6797e686ffcd85b84fed4601...</td>\n",
       "      <td>bc1qan83k0qqvp9qg7mj97vqza6yq42m0d4kep5r9n4hgz...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>02e8ebf3b925c166ecef09b48873246356323483f4b644...</td>\n",
       "      <td>02f6725f9c1c40333b67faea92fd211c183050f28df32c...</td>\n",
       "      <td>1100</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38816</td>\n",
       "      <td>3353</td>\n",
       "      <td>362</td>\n",
       "      <td>1213786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72442</th>\n",
       "      <td>38817</td>\n",
       "      <td>509944x550x0</td>\n",
       "      <td>509944</td>\n",
       "      <td>cf70e5e92a60a27e97bbef6172f9fdde04c33ddb7e1b9b...</td>\n",
       "      <td>bc1q94qfqc8w9huyqew6ruk3k08wenawkcs3wz4v6h9jrc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...</td>\n",
       "      <td>037f709e708f83bf2217369154316c769f5754e5eaf6a2...</td>\n",
       "      <td>20000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38817</td>\n",
       "      <td>422</td>\n",
       "      <td>349</td>\n",
       "      <td>147278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72441</th>\n",
       "      <td>38818</td>\n",
       "      <td>510476x471x0</td>\n",
       "      <td>510476</td>\n",
       "      <td>561f126ef4404f7d59c6996f1f429509cd0cb8513f58e5...</td>\n",
       "      <td>bc1qrfmufyugffuy0rq8pmm0dtdr2czafxm8s0ayl48gmu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>027ccec61f4bf1fafb5156931da6527dc104ec3613dd4f...</td>\n",
       "      <td>03557fd11b58cb93d2ad4fab4dd4cff7462a97e21e8f6b...</td>\n",
       "      <td>18235</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38818</td>\n",
       "      <td>7053</td>\n",
       "      <td>3436</td>\n",
       "      <td>24234108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72440</th>\n",
       "      <td>38819</td>\n",
       "      <td>510637x755x1</td>\n",
       "      <td>510637</td>\n",
       "      <td>5c6715958a1743b7c50d9fdde776cdc12b20863f48160b...</td>\n",
       "      <td>bc1qknfvwegkkzufmu7w0v9ae5fk5nr3j940vssxgee5uz...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>03557fd11b58cb93d2ad4fab4dd4cff7462a97e21e8f6b...</td>\n",
       "      <td>039a64895e50e2fb4381c908308fe155355ea3332faff5...</td>\n",
       "      <td>700000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38819</td>\n",
       "      <td>3436</td>\n",
       "      <td>1741</td>\n",
       "      <td>5982076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72439</th>\n",
       "      <td>38820</td>\n",
       "      <td>510990x376x0</td>\n",
       "      <td>510990</td>\n",
       "      <td>7bf8e252b920423b6d215b5c3033cc28748f47c07ddc49...</td>\n",
       "      <td>bc1qzjz9depsgyx47kmqguelxqkcn77gzlvu3msu2ytc92...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...</td>\n",
       "      <td>034c5c5250c08acb62df2ebd0ab728bf2d6c757c7d433e...</td>\n",
       "      <td>1000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38820</td>\n",
       "      <td>422</td>\n",
       "      <td>4141</td>\n",
       "      <td>1747502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72438</th>\n",
       "      <td>38821</td>\n",
       "      <td>511717x1483x0</td>\n",
       "      <td>511717</td>\n",
       "      <td>0a13849d8afa2ead480e8ea5952db2e57e904ac675f395...</td>\n",
       "      <td>bc1quwtkrr0twxahm62fyggj3k6pfslc7ptrx24c74pcd3...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...</td>\n",
       "      <td>03fa94bc4aead4375faaade28d33ef336d4124347843f2...</td>\n",
       "      <td>3000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38821</td>\n",
       "      <td>422</td>\n",
       "      <td>5197</td>\n",
       "      <td>2193134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72437</th>\n",
       "      <td>38822</td>\n",
       "      <td>511798x513x0</td>\n",
       "      <td>511798</td>\n",
       "      <td>ba1278647bc921ba8e9a11f4cd260ba58da265902ee86d...</td>\n",
       "      <td>bc1qvayk7r5eul20ajc3n8fthsq9xthaxpyk7uslvhhk2m...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>02b58092be437eadbaee5d48d77e58a32e9e28873f5544...</td>\n",
       "      <td>0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...</td>\n",
       "      <td>10000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38822</td>\n",
       "      <td>7064</td>\n",
       "      <td>422</td>\n",
       "      <td>2981008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72436</th>\n",
       "      <td>38823</td>\n",
       "      <td>511813x1989x0</td>\n",
       "      <td>511813</td>\n",
       "      <td>1e99cdbc5134c223fafc48d72c483471936bb4b1703c9a...</td>\n",
       "      <td>bc1qedqhkap8m0vhurvq8fme2uek2ajt4zsa8ysgtygszd...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0333b58f86a0c31da7ee61a4bc2232faccf8c4fe4167fe...</td>\n",
       "      <td>038d108e6cd9b8e84fa0b89d018c2e254324d3674b2bb3...</td>\n",
       "      <td>1050</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38823</td>\n",
       "      <td>2120</td>\n",
       "      <td>3124</td>\n",
       "      <td>6622880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72435</th>\n",
       "      <td>38824</td>\n",
       "      <td>511828x931x0</td>\n",
       "      <td>511828</td>\n",
       "      <td>9298ba619e05900883e7c78b4160fe9f2e304639acd3c8...</td>\n",
       "      <td>bc1q8uu8tjgudn4lekldqglvrzrgau7q3c90px42d3ep7c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...</td>\n",
       "      <td>038d108e6cd9b8e84fa0b89d018c2e254324d3674b2bb3...</td>\n",
       "      <td>1050</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38824</td>\n",
       "      <td>422</td>\n",
       "      <td>3124</td>\n",
       "      <td>1318328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72434</th>\n",
       "      <td>38825</td>\n",
       "      <td>511852x780x1</td>\n",
       "      <td>511852</td>\n",
       "      <td>13e9d2f579fdade3d919b26dace4a90d1afe5967c0db11...</td>\n",
       "      <td>bc1qsphtwx2jgpcp869x755whum8veply8q7y0xmh9m8yj...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>022b662aa03cf4db4820bcf4f47f1006d2bfae9bee416a...</td>\n",
       "      <td>03557fd11b58cb93d2ad4fab4dd4cff7462a97e21e8f6b...</td>\n",
       "      <td>10000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38825</td>\n",
       "      <td>3137</td>\n",
       "      <td>3436</td>\n",
       "      <td>10778732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72433</th>\n",
       "      <td>38827</td>\n",
       "      <td>511949x429x0</td>\n",
       "      <td>511949</td>\n",
       "      <td>5096dc65943191e9f754a5034a797124e93ddf1c8dd7cd...</td>\n",
       "      <td>bc1q8a58rj3g57xgfgr5n96eruja0d7phlcmmx8mt54p00...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0201d3fb63901ffaaa6ce535c9227fb3dc4646b6276096...</td>\n",
       "      <td>024bd94f0425590434538fd21d4e58982f7e9cfd8f3392...</td>\n",
       "      <td>1234</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38827</td>\n",
       "      <td>760</td>\n",
       "      <td>3130</td>\n",
       "      <td>2378800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72432</th>\n",
       "      <td>38826</td>\n",
       "      <td>511949x295x0</td>\n",
       "      <td>511949</td>\n",
       "      <td>1d26cf88cc05ac61051cdaaa061ccb0655d240f91f5b18...</td>\n",
       "      <td>bc1qkvdxks4q4qsl6g27d37u4ht9k5dq7lpg3gqqgp8cjn...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>024bd94f0425590434538fd21d4e58982f7e9cfd8f3392...</td>\n",
       "      <td>02b65dbae39dc11c2577c318ebb82696651377be09187b...</td>\n",
       "      <td>1234</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38826</td>\n",
       "      <td>3130</td>\n",
       "      <td>3621</td>\n",
       "      <td>11333730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72431</th>\n",
       "      <td>38828</td>\n",
       "      <td>511978x2333x0</td>\n",
       "      <td>511978</td>\n",
       "      <td>c85de4d8ecced5b65a854ddcd1a1f6d5006aa84a930210...</td>\n",
       "      <td>bc1q3fq6wv4ye2quqgvnycayft5d0utj6t3pcpfznvw2zj...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>024bd94f0425590434538fd21d4e58982f7e9cfd8f3392...</td>\n",
       "      <td>03bdeef26954cc08ec6b3bec9685d8610b491121e227b6...</td>\n",
       "      <td>1234</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38828</td>\n",
       "      <td>3130</td>\n",
       "      <td>5411</td>\n",
       "      <td>16936430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72430</th>\n",
       "      <td>39584</td>\n",
       "      <td>512160x1306x0</td>\n",
       "      <td>512160</td>\n",
       "      <td>fde9c2c25cc57548861e79b8592cca9b6a03abdcd02c9f...</td>\n",
       "      <td>bc1qvjqnwyactyqyu7d59gqtl07a3etdaecpja6ec83wc9...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>024bd94f0425590434538fd21d4e58982f7e9cfd8f3392...</td>\n",
       "      <td>02ef8eee471a04b6f5bc9eb69a2ea9625e71f3b93a5ee7...</td>\n",
       "      <td>1234</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39584</td>\n",
       "      <td>3130</td>\n",
       "      <td>7486</td>\n",
       "      <td>23431180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72429</th>\n",
       "      <td>38829</td>\n",
       "      <td>512177x1268x0</td>\n",
       "      <td>512177</td>\n",
       "      <td>e285c18ad1ae49b2b2dcc24e615a19684dfff341eb1f88...</td>\n",
       "      <td>bc1qz2977ql8dcummlaqpde7dfuy5ykpy7wwt9cqv3lell...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0246da2b1a74e63cf28fb50d6eaf59db0248a36629abd8...</td>\n",
       "      <td>024bd94f0425590434538fd21d4e58982f7e9cfd8f3392...</td>\n",
       "      <td>1234</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38829</td>\n",
       "      <td>7380</td>\n",
       "      <td>3130</td>\n",
       "      <td>23099400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72428</th>\n",
       "      <td>38830</td>\n",
       "      <td>512412x176x0</td>\n",
       "      <td>512412</td>\n",
       "      <td>04b784ebd9d7dfdaa64c9e20b4d72a4274f21aea81eb76...</td>\n",
       "      <td>bc1qwq352ts4lnnnwtfvsgfadejgsaelumwfeg9e73chsr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>024f8a8350dbc4a572f51618c9aec5458559d5fdf255b0...</td>\n",
       "      <td>030d3bd4450403644caa1950b83f0a04f31b88c42f436e...</td>\n",
       "      <td>10000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38830</td>\n",
       "      <td>3992</td>\n",
       "      <td>3563</td>\n",
       "      <td>14223496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72427</th>\n",
       "      <td>38831</td>\n",
       "      <td>512418x1585x0</td>\n",
       "      <td>512418</td>\n",
       "      <td>e2c1df01bb23442b5c52a4d127c481b30afa738e2e6f5a...</td>\n",
       "      <td>bc1qpazz7ganzk5p0zem5zcd7xy5ndanvx080ev2altkua...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>023d280ae29f84dcfd289eb66b57227fea3a7bde97ec28...</td>\n",
       "      <td>03feafbb48c239c6a8f70d35b2796d1741614e7a62b12b...</td>\n",
       "      <td>10000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38831</td>\n",
       "      <td>3452</td>\n",
       "      <td>5350</td>\n",
       "      <td>18468200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72426</th>\n",
       "      <td>36275</td>\n",
       "      <td>512733x303x0</td>\n",
       "      <td>512733</td>\n",
       "      <td>5943db82ff0bf9b7089ec397001e6ca0af52347213b777...</td>\n",
       "      <td>bc1qrepn0qy7ga0rjydhkxeds3kecjh7exfj6tuh0cwn9l...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>024bd94f0425590434538fd21d4e58982f7e9cfd8f3392...</td>\n",
       "      <td>02eae56f155bae8a8eaab82ddc6fef04d5a79a6b0b0d7b...</td>\n",
       "      <td>1234</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36275</td>\n",
       "      <td>3130</td>\n",
       "      <td>2418</td>\n",
       "      <td>7568340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0 short_channel_id  open_block  \\\n",
       "72475           0     505149x622x0      505149   \n",
       "72474       38787    506402x1391x1      506402   \n",
       "72473       38788    506847x1633x0      506847   \n",
       "72472       38789    508075x1694x1      508075   \n",
       "72471       58766    508090x1515x1      508090   \n",
       "72470       38790    508320x2072x0      508320   \n",
       "72469       38791    508400x1699x0      508400   \n",
       "72465       38795     508447x661x0      508447   \n",
       "72468       38792     508447x652x0      508447   \n",
       "72467       38793     508447x656x0      508447   \n",
       "72466       38794     508447x659x0      508447   \n",
       "72462       38799    508503x2007x0      508503   \n",
       "72461       38801    508503x2009x0      508503   \n",
       "72459       38797    508503x2005x0      508503   \n",
       "72464       38802    508503x2010x0      508503   \n",
       "72458       38798    508503x2006x0      508503   \n",
       "72460       38796    508503x1987x0      508503   \n",
       "72463       38800    508503x2008x0      508503   \n",
       "72457       38803     508666x124x0      508666   \n",
       "72455       38805     508668x388x0      508668   \n",
       "72454       38806     508668x390x0      508668   \n",
       "72456       38804     508668x387x0      508668   \n",
       "72453       38808     508672x551x0      508672   \n",
       "72452       38809     508672x553x0      508672   \n",
       "72451       38807     508672x550x0      508672   \n",
       "72450       38810     508865x158x0      508865   \n",
       "72449       38811    508866x1427x0      508866   \n",
       "72448       38812    509031x1459x0      509031   \n",
       "72447       38813    509031x1461x0      509031   \n",
       "72446       39583     509340x749x1      509340   \n",
       "72445       38815    509496x1530x0      509496   \n",
       "72444       38814    509496x1440x1      509496   \n",
       "72443       38816     509891x774x0      509891   \n",
       "72442       38817     509944x550x0      509944   \n",
       "72441       38818     510476x471x0      510476   \n",
       "72440       38819     510637x755x1      510637   \n",
       "72439       38820     510990x376x0      510990   \n",
       "72438       38821    511717x1483x0      511717   \n",
       "72437       38822     511798x513x0      511798   \n",
       "72436       38823    511813x1989x0      511813   \n",
       "72435       38824     511828x931x0      511828   \n",
       "72434       38825     511852x780x1      511852   \n",
       "72433       38827     511949x429x0      511949   \n",
       "72432       38826     511949x295x0      511949   \n",
       "72431       38828    511978x2333x0      511978   \n",
       "72430       39584    512160x1306x0      512160   \n",
       "72429       38829    512177x1268x0      512177   \n",
       "72428       38830     512412x176x0      512412   \n",
       "72427       38831    512418x1585x0      512418   \n",
       "72426       36275     512733x303x0      512733   \n",
       "\n",
       "                                        open_transaction  \\\n",
       "72475  f6bc767df9148ebf76d2b9baf4eb46e3230712c2bf5a51...   \n",
       "72474  2cdfc4fec2049d66a04fa5bdf468efb19c0354c60b8cf2...   \n",
       "72473  19ee11ce977facd380b92126834a3aca318f3cb905d99b...   \n",
       "72472  e267e54872053a7618567f31a9d27e38cdbff0e4176144...   \n",
       "72471  33d645657de8a587137b8039e52452557d4279a3f47366...   \n",
       "72470  40944505d8e1a55e6923283cee629757500aa344142650...   \n",
       "72469  71c790031c4b99593848dde6054366d0df2ad85c2806b7...   \n",
       "72465  b592861b271aedadf6ad529b158df0e962b830e2b6056b...   \n",
       "72468  5b88d5737be23e70e62413461764e9ee1753d7c0fe90ba...   \n",
       "72467  5f741522f787c5fde67f2fd73b220e9af6aff9e4ee700f...   \n",
       "72466  abc55727b4d5cfa39d84e48df410230d9c1b100b3cc07a...   \n",
       "72462  e7ea8df19fbae1925cc33d1245aaa09eaeb6dd2cb20baa...   \n",
       "72461  46c8db8b6fed487c0f8e3ee6f69eb114afaa0028fd5b89...   \n",
       "72459  b0a2717efa8662542d74f029c92f231b9fad8784de6252...   \n",
       "72464  51a67ba1b325419bed2184683eb61678da4dd3d8cc9ab4...   \n",
       "72458  36596579fb974d8b189d831b12d30f291daefe20b89e0c...   \n",
       "72460  4ae70b9e265b7e63ba6e41cc345aad72cce24b43f4d141...   \n",
       "72463  4a94ba64aea2537acc5ec8555911c2e2f58b454f7668e3...   \n",
       "72457  2eee222e9d6bd4d8b2b44925549ace8f8d2d385a18b6e3...   \n",
       "72455  c2b40e30fd34d134c52b79c5cb42fa0ea3538c4e1ae3c6...   \n",
       "72454  bbb79db464c0f445f5c7d3a760eac96c7ca75e709de537...   \n",
       "72456  98830c56ea8130f6a5e7d5fca585f437711667c3947afb...   \n",
       "72453  8ee974b19881ad9f4b767402215a50e3ea130d0d6bc85e...   \n",
       "72452  5deaa8910fec8e245417b9583583712703c7e938a9e42b...   \n",
       "72451  815659075445df3f1425b7f7199a8959fad78014d211d0...   \n",
       "72450  fe684cd2d2bd274e6514593a5d6686d429300322b64ba8...   \n",
       "72449  3602ffe5c7e9158b5ed7755a02a6cdbfa4ab62d98628f9...   \n",
       "72448  bcbb33bffc6ae9493a5b8aee4fef58521d612c659f85c2...   \n",
       "72447  9f632b465487dfee8e85dee82ce4f4d842336b9c6416a9...   \n",
       "72446  abb9c04681dec8fb1ad0b5fef6e520ea232e4f405e86f7...   \n",
       "72445  d99eb3dabf350ebf3465ec47d32420231a49f42991c1c0...   \n",
       "72444  e576b581ad717a69394fb7ded3f60aff261f14d7ddc4d5...   \n",
       "72443  2bfde8bf73c09b7a36de9b6797e686ffcd85b84fed4601...   \n",
       "72442  cf70e5e92a60a27e97bbef6172f9fdde04c33ddb7e1b9b...   \n",
       "72441  561f126ef4404f7d59c6996f1f429509cd0cb8513f58e5...   \n",
       "72440  5c6715958a1743b7c50d9fdde776cdc12b20863f48160b...   \n",
       "72439  7bf8e252b920423b6d215b5c3033cc28748f47c07ddc49...   \n",
       "72438  0a13849d8afa2ead480e8ea5952db2e57e904ac675f395...   \n",
       "72437  ba1278647bc921ba8e9a11f4cd260ba58da265902ee86d...   \n",
       "72436  1e99cdbc5134c223fafc48d72c483471936bb4b1703c9a...   \n",
       "72435  9298ba619e05900883e7c78b4160fe9f2e304639acd3c8...   \n",
       "72434  13e9d2f579fdade3d919b26dace4a90d1afe5967c0db11...   \n",
       "72433  5096dc65943191e9f754a5034a797124e93ddf1c8dd7cd...   \n",
       "72432  1d26cf88cc05ac61051cdaaa061ccb0655d240f91f5b18...   \n",
       "72431  c85de4d8ecced5b65a854ddcd1a1f6d5006aa84a930210...   \n",
       "72430  fde9c2c25cc57548861e79b8592cca9b6a03abdcd02c9f...   \n",
       "72429  e285c18ad1ae49b2b2dcc24e615a19684dfff341eb1f88...   \n",
       "72428  04b784ebd9d7dfdaa64c9e20b4d72a4274f21aea81eb76...   \n",
       "72427  e2c1df01bb23442b5c52a4d127c481b30afa738e2e6f5a...   \n",
       "72426  5943db82ff0bf9b7089ec397001e6ca0af52347213b777...   \n",
       "\n",
       "                                                 address  close_block  \\\n",
       "72475  bc1qjmg6ev344fenh3zhg0yjl6hyvxpxluw6x9nn2a5lv4...     592638.0   \n",
       "72474  bc1qvjx5t8y7j83udzuj38ukmqecv5d9jn762mchxkgvaf...          NaN   \n",
       "72473  bc1q29g43xrz9gujgt60gykzq3vh0ewfav7vmfqcnmf50u...          NaN   \n",
       "72472  bc1qpxzqp2xyy0gzn8xwu6lqg3a66tuhsg5w849t0j5rdr...          NaN   \n",
       "72471  bc1qneudwey0dpgy9nj2g8ech0lqqrhz52agcj984rs6zh...     616838.0   \n",
       "72470  bc1qcrsrc0xzt3uumh3uyftn600ku9vmamth50zdzfjpv2...          NaN   \n",
       "72469  bc1qc4r2cdzucu2xncqv8sde97c0kumvqlrv9ehmfxykzu...          NaN   \n",
       "72465  bc1qxq49306phnepteaxx98gxx9tfjxw35umasvvep3w0k...          NaN   \n",
       "72468  bc1qzp4ufw5jy4drk9kw56hvtmdeuvtqxnmvrdqjwaj6ms...          NaN   \n",
       "72467  bc1qe7uvtpg6klu0awfq479xz56pgm9mh9d67k7m3f6hc2...          NaN   \n",
       "72466  bc1qhacv3gscv40muy3u57sr9msrqspa2lcdxrhehm04ak...          NaN   \n",
       "72462  bc1q96v2p3ghqrg5xe3qr2zqa5y49g860ufs7sgm3qyf4x...          NaN   \n",
       "72461  bc1qu0jcnkf546c7rc3hc4wv0pnglwhp7kqylqf2edl33p...          NaN   \n",
       "72459  bc1qt5jlls5r6j80mtmhmlrznaujwk9xnulaz7cxx9mf0m...          NaN   \n",
       "72464  bc1qdlstpk53ku7kazw3ptfqfqsaacdeudxlhuj4rqngx9...          NaN   \n",
       "72458  bc1qd4uaz47jjzyuq5qc92lc72llzqkz0s4ljew0qhjd85...          NaN   \n",
       "72460  bc1qqsvk880wywwmcr4wqk9tenc75lezxph9ekrsmpj89e...          NaN   \n",
       "72463  bc1qpru56flmhvc9lpx5kjdsgrsw5z9vpkvsgp7z5zmlt4...          NaN   \n",
       "72457  bc1q9mn6zddje0y6l0fvhw37e39sktaxjdsk7wmd8en85m...          NaN   \n",
       "72455  bc1q73tg2twa7jdsvz2eq70n09n5c2gxpnmzu5zf5xdn30...          NaN   \n",
       "72454  bc1qzk56xtm6kuay3p4a7z9cmwsqpfwlryzpw6cal67ns2...          NaN   \n",
       "72456  bc1qefmz0fr5kgzayl3vkwscqtaarcejdtrzxjze2qr6pl...          NaN   \n",
       "72453  bc1q5a99ptwtzte60qxrln9k3g97t3gzgl95zqznftpqpy...          NaN   \n",
       "72452  bc1qm5up2f6n555akgls4fxj80emln956qsy5ztvyc4aqh...          NaN   \n",
       "72451  bc1qr0zs76s2dsxdmtkg3tu4j5y9cxwnk4xphw672djn09...          NaN   \n",
       "72450  bc1qs2xv3qerg7t4fru4xrrwgz4w3dsnnf65krzdzk38td...          NaN   \n",
       "72449  bc1qpckwx22lr83p6h02us0p32y6j23hmpj74ynjs5yw29...          NaN   \n",
       "72448  bc1qg7uap38zak74s4cw63dk6uqda5ng2xlg8jptldy9ee...          NaN   \n",
       "72447  bc1qswh7rqjq9et0j9ens04eu6qm37v8k6ute55fr3qzq2...          NaN   \n",
       "72446  bc1q6qda36cth9py079hs9899cn76w2aq0r56766df0tfh...          NaN   \n",
       "72445  bc1q0u9grd4xj0wsgl2cx2wzrq70z4swjz6k3e3m5ladsq...          NaN   \n",
       "72444  bc1qkzav59cldqqd9jqa6lze9d8rcs3ll30k0mjlrkdsep...          NaN   \n",
       "72443  bc1qan83k0qqvp9qg7mj97vqza6yq42m0d4kep5r9n4hgz...          NaN   \n",
       "72442  bc1q94qfqc8w9huyqew6ruk3k08wenawkcs3wz4v6h9jrc...          NaN   \n",
       "72441  bc1qrfmufyugffuy0rq8pmm0dtdr2czafxm8s0ayl48gmu...          NaN   \n",
       "72440  bc1qknfvwegkkzufmu7w0v9ae5fk5nr3j940vssxgee5uz...          NaN   \n",
       "72439  bc1qzjz9depsgyx47kmqguelxqkcn77gzlvu3msu2ytc92...          NaN   \n",
       "72438  bc1quwtkrr0twxahm62fyggj3k6pfslc7ptrx24c74pcd3...          NaN   \n",
       "72437  bc1qvayk7r5eul20ajc3n8fthsq9xthaxpyk7uslvhhk2m...          NaN   \n",
       "72436  bc1qedqhkap8m0vhurvq8fme2uek2ajt4zsa8ysgtygszd...          NaN   \n",
       "72435  bc1q8uu8tjgudn4lekldqglvrzrgau7q3c90px42d3ep7c...          NaN   \n",
       "72434  bc1qsphtwx2jgpcp869x755whum8veply8q7y0xmh9m8yj...          NaN   \n",
       "72433  bc1q8a58rj3g57xgfgr5n96eruja0d7phlcmmx8mt54p00...          NaN   \n",
       "72432  bc1qkvdxks4q4qsl6g27d37u4ht9k5dq7lpg3gqqgp8cjn...          NaN   \n",
       "72431  bc1q3fq6wv4ye2quqgvnycayft5d0utj6t3pcpfznvw2zj...          NaN   \n",
       "72430  bc1qvjqnwyactyqyu7d59gqtl07a3etdaecpja6ec83wc9...          NaN   \n",
       "72429  bc1qz2977ql8dcummlaqpde7dfuy5ykpy7wwt9cqv3lell...          NaN   \n",
       "72428  bc1qwq352ts4lnnnwtfvsgfadejgsaelumwfeg9e73chsr...          NaN   \n",
       "72427  bc1qpazz7ganzk5p0zem5zcd7xy5ndanvx080ev2altkua...          NaN   \n",
       "72426  bc1qrepn0qy7ga0rjydhkxeds3kecjh7exfj6tuh0cwn9l...          NaN   \n",
       "\n",
       "                                       close_transaction  \\\n",
       "72475  82cb2ea2a06c8c453d8b9ca08e17bbefe87225aa380b2d...   \n",
       "72474                                                NaN   \n",
       "72473                                                NaN   \n",
       "72472                                                NaN   \n",
       "72471  123777e4dfadc7c008a54c2d55b670067a58cdcbc8b2ec...   \n",
       "72470                                                NaN   \n",
       "72469                                                NaN   \n",
       "72465                                                NaN   \n",
       "72468                                                NaN   \n",
       "72467                                                NaN   \n",
       "72466                                                NaN   \n",
       "72462                                                NaN   \n",
       "72461                                                NaN   \n",
       "72459                                                NaN   \n",
       "72464                                                NaN   \n",
       "72458                                                NaN   \n",
       "72460                                                NaN   \n",
       "72463                                                NaN   \n",
       "72457                                                NaN   \n",
       "72455                                                NaN   \n",
       "72454                                                NaN   \n",
       "72456                                                NaN   \n",
       "72453                                                NaN   \n",
       "72452                                                NaN   \n",
       "72451                                                NaN   \n",
       "72450                                                NaN   \n",
       "72449                                                NaN   \n",
       "72448                                                NaN   \n",
       "72447                                                NaN   \n",
       "72446                                                NaN   \n",
       "72445                                                NaN   \n",
       "72444                                                NaN   \n",
       "72443                                                NaN   \n",
       "72442                                                NaN   \n",
       "72441                                                NaN   \n",
       "72440                                                NaN   \n",
       "72439                                                NaN   \n",
       "72438                                                NaN   \n",
       "72437                                                NaN   \n",
       "72436                                                NaN   \n",
       "72435                                                NaN   \n",
       "72434                                                NaN   \n",
       "72433                                                NaN   \n",
       "72432                                                NaN   \n",
       "72431                                                NaN   \n",
       "72430                                                NaN   \n",
       "72429                                                NaN   \n",
       "72428                                                NaN   \n",
       "72427                                                NaN   \n",
       "72426                                                NaN   \n",
       "\n",
       "                                                   node0  \\\n",
       "72475  0250373555232cec757ea141273e75381c84cc3ab22f1e...   \n",
       "72474  0313f9449cdb528dc9707c02da507cc9306eedc415091c...   \n",
       "72473  023d280ae29f84dcfd289eb66b57227fea3a7bde97ec28...   \n",
       "72472  03557fd11b58cb93d2ad4fab4dd4cff7462a97e21e8f6b...   \n",
       "72471  028314f021602092779aedd4ef39f3b5809f9b6046f8bc...   \n",
       "72470  0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...   \n",
       "72469  0268cbff355499bf97387dec397614ab3bd0e3e4361475...   \n",
       "72465  023c5f2b69161742cc49da5be6cb7c940465fc608ff216...   \n",
       "72468  02829de2d8762ac70f6a243c7e210e1a41d7bc9fe909d0...   \n",
       "72467  02952cc6b7f92226e490d6354636604a26484a396627c9...   \n",
       "72466  023e2b9bb357b1bf135d59551418178ef020cdd77efee7...   \n",
       "72462  0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...   \n",
       "72461  02f2fccc11d34d7fce415a97ff9fd46867e5af780c9c71...   \n",
       "72459  03397f996a69f58ba4fd3d20e21daa36ff3be3c3624c03...   \n",
       "72464  02d6284ccb2ce82f79e938a82da23b9f2d2fb659c5147e...   \n",
       "72458  0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...   \n",
       "72460  0230d1a9462fae76bc20e9f5eff885a70f4152d9eca912...   \n",
       "72463  03456c38eb93de16adf2555d5b3cf7030ce2537bac453d...   \n",
       "72457  026977ceabe8a4bd0f38153d781210b0bb685d8fea10a5...   \n",
       "72455  022c715b377856f5e176c52dfbe1f19b0283c4a85d563b...   \n",
       "72454  0211ad124eaecaf2b223bad2f5381e81b3639f2a652b5a...   \n",
       "72456  0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...   \n",
       "72453  020d3d5995a973c878e3f6e5f59da54078304c537f981d...   \n",
       "72452  025e6fdb6af937a959e70f83648542c63ba7dc478cecf2...   \n",
       "72451  02587704695048661f32522b3ae2517260154443579161...   \n",
       "72450  0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...   \n",
       "72449  0312c53f40de91c5a1a64126a0f205116d4ce397ebefdb...   \n",
       "72448  024b82d69859d63f9fce375041a34e61f4bfd445494b41...   \n",
       "72447  0253271f3a0cf1876dfaa381728ce1514d2254b0012725...   \n",
       "72446  034593537dc19047de45ef90ec22b83243b184df595a61...   \n",
       "72445  0347469dea682bd59255a5e60eaed8079186f85b5b5ea6...   \n",
       "72444  03220b9d15ee4fdd5ab3cf16bee7b640ae648962aef50e...   \n",
       "72443  02e8ebf3b925c166ecef09b48873246356323483f4b644...   \n",
       "72442  0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...   \n",
       "72441  027ccec61f4bf1fafb5156931da6527dc104ec3613dd4f...   \n",
       "72440  03557fd11b58cb93d2ad4fab4dd4cff7462a97e21e8f6b...   \n",
       "72439  0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...   \n",
       "72438  0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...   \n",
       "72437  02b58092be437eadbaee5d48d77e58a32e9e28873f5544...   \n",
       "72436  0333b58f86a0c31da7ee61a4bc2232faccf8c4fe4167fe...   \n",
       "72435  0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...   \n",
       "72434  022b662aa03cf4db4820bcf4f47f1006d2bfae9bee416a...   \n",
       "72433  0201d3fb63901ffaaa6ce535c9227fb3dc4646b6276096...   \n",
       "72432  024bd94f0425590434538fd21d4e58982f7e9cfd8f3392...   \n",
       "72431  024bd94f0425590434538fd21d4e58982f7e9cfd8f3392...   \n",
       "72430  024bd94f0425590434538fd21d4e58982f7e9cfd8f3392...   \n",
       "72429  0246da2b1a74e63cf28fb50d6eaf59db0248a36629abd8...   \n",
       "72428  024f8a8350dbc4a572f51618c9aec5458559d5fdf255b0...   \n",
       "72427  023d280ae29f84dcfd289eb66b57227fea3a7bde97ec28...   \n",
       "72426  024bd94f0425590434538fd21d4e58982f7e9cfd8f3392...   \n",
       "\n",
       "                                                   node1  satoshis  ...  \\\n",
       "72475  02ef61a252f9504a42fc264a28476f44cea0711a44b2da...    300000  ...   \n",
       "72474  035f1498c929d4cefba4701ae36a554691f526ff60b176...   1111934  ...   \n",
       "72473  0273081ce642554d5a68a5236564fe88a3783457dc09e5...     40000  ...   \n",
       "72472  03cbf298b068300be33f06c947b9d3f00a0f0e8089da32...    100000  ...   \n",
       "72471  02d4531a2f2e6e5a9033d37d548cff4834a3898e74c3ab...    400000  ...   \n",
       "72470  034a5fdb2df3ce1bfd2c2aca205ce9cfeef1a5f4af21b0...      1050  ...   \n",
       "72469  034a5fdb2df3ce1bfd2c2aca205ce9cfeef1a5f4af21b0...      1050  ...   \n",
       "72465  0333b58f86a0c31da7ee61a4bc2232faccf8c4fe4167fe...     50000  ...   \n",
       "72468  0333b58f86a0c31da7ee61a4bc2232faccf8c4fe4167fe...     50000  ...   \n",
       "72467  0333b58f86a0c31da7ee61a4bc2232faccf8c4fe4167fe...     50000  ...   \n",
       "72466  0333b58f86a0c31da7ee61a4bc2232faccf8c4fe4167fe...     50000  ...   \n",
       "72462  03617110be362e43a7445941970637d710eb46486d8a59...     20000  ...   \n",
       "72461  0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...     20000  ...   \n",
       "72459  0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...     20000  ...   \n",
       "72464  0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...     20000  ...   \n",
       "72458  038fe44b4159e224ecbd92a8c63c24948a807d39c08e2d...     20000  ...   \n",
       "72460  0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...     20000  ...   \n",
       "72463  0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...     20000  ...   \n",
       "72457  0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...      1100  ...   \n",
       "72455  0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...      1100  ...   \n",
       "72454  0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...      1100  ...   \n",
       "72456  03a98b60b81e126f5f588d8c77cf621ac3cd57bc445701...      1100  ...   \n",
       "72453  0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...      1100  ...   \n",
       "72452  0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...      1100  ...   \n",
       "72451  0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...      1100  ...   \n",
       "72450  03e3d670d86f33181ee7451f14998b376a0d5deba8ab06...      1100  ...   \n",
       "72449  0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...      1100  ...   \n",
       "72448  0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...      1100  ...   \n",
       "72447  0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...      1100  ...   \n",
       "72446  0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...    450000  ...   \n",
       "72445  0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...      1050  ...   \n",
       "72444  03557fd11b58cb93d2ad4fab4dd4cff7462a97e21e8f6b...    250000  ...   \n",
       "72443  02f6725f9c1c40333b67faea92fd211c183050f28df32c...      1100  ...   \n",
       "72442  037f709e708f83bf2217369154316c769f5754e5eaf6a2...     20000  ...   \n",
       "72441  03557fd11b58cb93d2ad4fab4dd4cff7462a97e21e8f6b...     18235  ...   \n",
       "72440  039a64895e50e2fb4381c908308fe155355ea3332faff5...    700000  ...   \n",
       "72439  034c5c5250c08acb62df2ebd0ab728bf2d6c757c7d433e...   1000000  ...   \n",
       "72438  03fa94bc4aead4375faaade28d33ef336d4124347843f2...      3000  ...   \n",
       "72437  0347ffcb271ef54fa6103fc6392370ce398d1bc9846082...     10000  ...   \n",
       "72436  038d108e6cd9b8e84fa0b89d018c2e254324d3674b2bb3...      1050  ...   \n",
       "72435  038d108e6cd9b8e84fa0b89d018c2e254324d3674b2bb3...      1050  ...   \n",
       "72434  03557fd11b58cb93d2ad4fab4dd4cff7462a97e21e8f6b...     10000  ...   \n",
       "72433  024bd94f0425590434538fd21d4e58982f7e9cfd8f3392...      1234  ...   \n",
       "72432  02b65dbae39dc11c2577c318ebb82696651377be09187b...      1234  ...   \n",
       "72431  03bdeef26954cc08ec6b3bec9685d8610b491121e227b6...      1234  ...   \n",
       "72430  02ef8eee471a04b6f5bc9eb69a2ea9625e71f3b93a5ee7...      1234  ...   \n",
       "72429  024bd94f0425590434538fd21d4e58982f7e9cfd8f3392...      1234  ...   \n",
       "72428  030d3bd4450403644caa1950b83f0a04f31b88c42f436e...     10000  ...   \n",
       "72427  03feafbb48c239c6a8f70d35b2796d1741614e7a62b12b...     10000  ...   \n",
       "72426  02eae56f155bae8a8eaab82ddc6fef04d5a79a6b0b0d7b...      1234  ...   \n",
       "\n",
       "      close_fee   last_update  close_type  close_htlc_count  close_balance_a  \\\n",
       "72475     184.0  1.563172e+09      mutual               0.0           3570.0   \n",
       "72474       NaN           NaN         NaN               NaN              NaN   \n",
       "72473       NaN           NaN         NaN               NaN              NaN   \n",
       "72472       NaN           NaN         NaN               NaN              NaN   \n",
       "72471    4887.0  1.581152e+09         NaN               NaN              NaN   \n",
       "72470       NaN           NaN         NaN               NaN              NaN   \n",
       "72469       NaN           NaN         NaN               NaN              NaN   \n",
       "72465       NaN           NaN         NaN               NaN              NaN   \n",
       "72468       NaN           NaN         NaN               NaN              NaN   \n",
       "72467       NaN           NaN         NaN               NaN              NaN   \n",
       "72466       NaN           NaN         NaN               NaN              NaN   \n",
       "72462       NaN           NaN         NaN               NaN              NaN   \n",
       "72461       NaN           NaN         NaN               NaN              NaN   \n",
       "72459       NaN           NaN         NaN               NaN              NaN   \n",
       "72464       NaN           NaN         NaN               NaN              NaN   \n",
       "72458       NaN           NaN         NaN               NaN              NaN   \n",
       "72460       NaN           NaN         NaN               NaN              NaN   \n",
       "72463       NaN           NaN         NaN               NaN              NaN   \n",
       "72457       NaN           NaN         NaN               NaN              NaN   \n",
       "72455       NaN           NaN         NaN               NaN              NaN   \n",
       "72454       NaN           NaN         NaN               NaN              NaN   \n",
       "72456       NaN           NaN         NaN               NaN              NaN   \n",
       "72453       NaN           NaN         NaN               NaN              NaN   \n",
       "72452       NaN           NaN         NaN               NaN              NaN   \n",
       "72451       NaN           NaN         NaN               NaN              NaN   \n",
       "72450       NaN           NaN         NaN               NaN              NaN   \n",
       "72449       NaN           NaN         NaN               NaN              NaN   \n",
       "72448       NaN           NaN         NaN               NaN              NaN   \n",
       "72447       NaN           NaN         NaN               NaN              NaN   \n",
       "72446       NaN           NaN         NaN               NaN              NaN   \n",
       "72445       NaN           NaN         NaN               NaN              NaN   \n",
       "72444       NaN           NaN         NaN               NaN              NaN   \n",
       "72443       NaN           NaN         NaN               NaN              NaN   \n",
       "72442       NaN           NaN         NaN               NaN              NaN   \n",
       "72441       NaN           NaN         NaN               NaN              NaN   \n",
       "72440       NaN           NaN         NaN               NaN              NaN   \n",
       "72439       NaN           NaN         NaN               NaN              NaN   \n",
       "72438       NaN           NaN         NaN               NaN              NaN   \n",
       "72437       NaN           NaN         NaN               NaN              NaN   \n",
       "72436       NaN           NaN         NaN               NaN              NaN   \n",
       "72435       NaN           NaN         NaN               NaN              NaN   \n",
       "72434       NaN           NaN         NaN               NaN              NaN   \n",
       "72433       NaN           NaN         NaN               NaN              NaN   \n",
       "72432       NaN           NaN         NaN               NaN              NaN   \n",
       "72431       NaN           NaN         NaN               NaN              NaN   \n",
       "72430       NaN           NaN         NaN               NaN              NaN   \n",
       "72429       NaN           NaN         NaN               NaN              NaN   \n",
       "72428       NaN           NaN         NaN               NaN              NaN   \n",
       "72427       NaN           NaN         NaN               NaN              NaN   \n",
       "72426       NaN           NaN         NaN               NaN              NaN   \n",
       "\n",
       "       close_balance_b dec_id  node0_id  node1_id  node_pair  \n",
       "72475         296246.0      0      6038      5314   32085932  \n",
       "72474              NaN  38787       934      3023    2823482  \n",
       "72473              NaN  38788      3452       576    1988352  \n",
       "72472              NaN  38789      3436      3310   11373160  \n",
       "72471              NaN  58766      2378      4223   10042294  \n",
       "72470              NaN  38790       422      1912     806864  \n",
       "72469              NaN  38791      5154      1912    9854448  \n",
       "72465              NaN  38795      4688      2120    9938560  \n",
       "72468              NaN  38792      6656      2120   14110720  \n",
       "72467              NaN  38793      6595      2120   13981400  \n",
       "72466              NaN  38794      4119      2120    8732280  \n",
       "72462              NaN  38799       422      7478    3155716  \n",
       "72461              NaN  38801      2953       422    1246166  \n",
       "72459              NaN  38797      3957       422    1669854  \n",
       "72464              NaN  38802      5725       422    2415950  \n",
       "72458              NaN  38798       422      5426    2289772  \n",
       "72460              NaN  38796      7059       422    2978898  \n",
       "72463              NaN  38800      2518       422    1062596  \n",
       "72457              NaN  38803      5294       422    2234068  \n",
       "72455              NaN  38805      3813       422    1609086  \n",
       "72454              NaN  38806      3336       422    1407792  \n",
       "72456              NaN  38804       422      6317    2665774  \n",
       "72453              NaN  38808      7332       422    3094104  \n",
       "72452              NaN  38809      2834       422    1195948  \n",
       "72451              NaN  38807      1527       422     644394  \n",
       "72450              NaN  38810       422      4271    1802362  \n",
       "72449              NaN  38811      5302       422    2237444  \n",
       "72448              NaN  38812      1331       422     561682  \n",
       "72447              NaN  38813      6306       422    2661132  \n",
       "72446              NaN  39583      2446       422    1032212  \n",
       "72445              NaN  38815      5784       422    2440848  \n",
       "72444              NaN  38814      7586      3436   26065496  \n",
       "72443              NaN  38816      3353       362    1213786  \n",
       "72442              NaN  38817       422       349     147278  \n",
       "72441              NaN  38818      7053      3436   24234108  \n",
       "72440              NaN  38819      3436      1741    5982076  \n",
       "72439              NaN  38820       422      4141    1747502  \n",
       "72438              NaN  38821       422      5197    2193134  \n",
       "72437              NaN  38822      7064       422    2981008  \n",
       "72436              NaN  38823      2120      3124    6622880  \n",
       "72435              NaN  38824       422      3124    1318328  \n",
       "72434              NaN  38825      3137      3436   10778732  \n",
       "72433              NaN  38827       760      3130    2378800  \n",
       "72432              NaN  38826      3130      3621   11333730  \n",
       "72431              NaN  38828      3130      5411   16936430  \n",
       "72430              NaN  39584      3130      7486   23431180  \n",
       "72429              NaN  38829      7380      3130   23099400  \n",
       "72428              NaN  38830      3992      3563   14223496  \n",
       "72427              NaN  38831      3452      5350   18468200  \n",
       "72426              NaN  36275      3130      2418    7568340  \n",
       "\n",
       "[50 rows x 24 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort and visualize DataFrame\n",
    "\n",
    "decisions_df.sort_values(by=['open_block'],inplace=True,ascending=True)\n",
    "print('Size of DataFrame in Memory:{}'.format(sys.getsizeof(decisions_df)))\n",
    "# Check specific channel id\n",
    "#decisions_df[decisions_df['short_channel_id']=='513675x2245x0'].head()\n",
    "\n",
    "decisions_df.sort_values(by=['open_block'],ascending=True).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72476"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(decisions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38355"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(decisions_df[decisions_df['close_block'].notnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'short_channel_id', 'open_block', 'open_transaction',\n",
       "       'address', 'close_block', 'close_transaction', 'node0', 'node1',\n",
       "       'satoshis', 'last_seen', 'open_time', 'open_fee', 'close_time',\n",
       "       'close_fee', 'last_update', 'close_type', 'close_htlc_count',\n",
       "       'close_balance_a', 'close_balance_b', 'dec_id', 'node0_id', 'node1_id',\n",
       "       'node_pair'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decisions_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TESTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Lazy Graph extract\n",
    "blocks_att=[]\n",
    "for i in range(len(graph_snapshots)):\n",
    "    graph_i=dask.compute(graph_snapshots[i])\n",
    "    block=graph_i.graph['block']\n",
    "    blocks_att.append(block)\n",
    "\n",
    "print(blocks_att)\n",
    "\n",
    "#graph_snapshots=dask.compute(*graph_snapshots)\n",
    "#block=graph_snapshots[0].graph['block']\n",
    "    \n",
    "#print(len(graph_snapshots[5]))\n",
    "#print(graph_snapshots[3].graph['block'])\n",
    "\n",
    "# Delayed testing\n",
    "#results = dask.compute(*futures)\n",
    "#graphs=dask.compute(*graph_snapshots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Comparative Analysis\n",
    "\n",
    "In order to understand the potential motivations behind each decision we analyze each decission (opening or closure of a channel) independently from the perspective of each of the participants in the decission, which we'll call the node under analysis. For each decission we extract or compute the following information: \n",
    "\n",
    "Betweenness centrality measures how central is a network to the flow of information in a network. In the case of the Lightning Network the higher the betweenness centrality of a node, the more transactions (messages) that are routed through it. In particular, we will use a measure of betweenness centrality defined in (Brandes and Fleischer 2005 - https://link.springer.com/chapter/10.1007/978-3-540-31856-9_44) that models infomation through a network, as electric current, efficiently and not only considering shortest path. This allows us to account for the fact that not all transactions travel through shortes path given that there are fee and capacity considerations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Measurments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Property measurement for a collection of graphs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEW\n",
    "# Function to calculate base measurement for every graph in snapshot\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "collection_measure\n",
    "    Iterates over graph snapshots and calculates measurement for every node in each of the graphs.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "g_snapshots : list\n",
    "    List of delayed nx graph elements contianing graph snapshots\n",
    "    \n",
    "\n",
    "measurement: string\n",
    "    Name of the type of measurment that will applied to the graph. (See graph_measurement function for options)\n",
    "\n",
    "\n",
    "\n",
    "Returns\n",
    "-------\n",
    "snapshot_mes_dic: dic\n",
    "    Dictionary with blocks as keys and dic of measurements as values\n",
    "\n",
    "\"\"\"\n",
    "def collection_measure(bucket,graph_keys,measurement):\n",
    "    \n",
    "    snapshot_mes_list=[]\n",
    "    \n",
    "    \n",
    "    \n",
    "    with tqdm(total=len(range(1,len(graph_keys)))) as pbar:\n",
    "        for i in (range(1,len(graph_keys))):\n",
    "            \n",
    "            key=graph_keys[i]\n",
    "            prev_key=graph_keys[i-1]\n",
    "\n",
    "            measurement_input=(key,measurement,'capacity',bucket,prev_key)\n",
    "\n",
    "            b_g_tuple=dask.delayed(graph_measurement)(measurement_input)\n",
    "            snapshot_mes_list.append(b_g_tuple)\n",
    "            pbar.update(1)\n",
    "\n",
    "    futures = dask.persist(*snapshot_mes_list)\n",
    "    \n",
    "    start=time.time()\n",
    "    snapshot_mes_list = dask.compute(*futures)\n",
    "    #snapshot_mes_list = dask.compute(*snapshot_mes_list)\n",
    "    snapshot_mes_dic={record[0]:record[1] for record in snapshot_mes_list}\n",
    "    end=time.time()\n",
    "    print('Compute in seconds: {}'.format(end-start))\n",
    "    \n",
    "    return snapshot_mes_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Property measurment for a single graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "graph_measurement\n",
    "    Performs specific graph measurement \n",
    "\n",
    "Parameters\n",
    "----------\n",
    "measurment_input: tuple\n",
    "    g : nx graph\n",
    "        NetworkX graph object over which measurment will be calculated for each node\n",
    "\n",
    "    measurment: string\n",
    "        Type of measurement to be performed on graph\n",
    "        \n",
    "    weight: string\n",
    "        Edge attribute to be used as weight\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "g_dir: dir\n",
    "    Dictionary with measurment values for each node\n",
    "\"\"\"\n",
    "\n",
    "def graph_measurement(measurement_input):\n",
    "    \n",
    "    # Extract inputs\n",
    "    key=measurement_input[0]\n",
    "    measurement=measurement_input[1]\n",
    "    weight=measurement_input[2]\n",
    "    bucket=measurement_input[3]\n",
    "    \n",
    "    \n",
    "    if len(measurement_input)>4:\n",
    "        prev_key=measurement_input[4]\n",
    "    \n",
    "    # Download graph\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.resource('s3')\n",
    "    response = s3.Object(bucket_name=bucket, key=key).get()\n",
    "    g=pickle.loads(response['Body'].read())\n",
    "    \n",
    "    # Extract Block\n",
    "    \n",
    "    block=g.graph['block']\n",
    "    score_type='/raw_score/'\n",
    "   \n",
    "    \n",
    "    if measurement=='current_betweeness_full':\n",
    "        g_dir=nx.algorithms.centrality.current_flow_betweenness_centrality(g,weight=weight)\n",
    "    \n",
    "    elif measurement=='current_betweeness':\n",
    "        g_dir=nx.algorithms.centrality.approximate_current_flow_betweenness_centrality(g,weight=weight)\n",
    "        \n",
    "    elif measurement=='current_betweeness_unweighted': # for unweighted current betweeness\n",
    "        g_dir=nx.algorithms.centrality.approximate_current_flow_betweenness_centrality(g)\n",
    "    \n",
    "    elif measurement=='current_closeness':\n",
    "        g_dir=nx.algorithms.centrality.current_flow_closeness_centrality(g,weight=weight)\n",
    "    \n",
    "    elif measurement=='closeness':\n",
    "        g_dir=nx.closeness_centrality(g)\n",
    "        \n",
    "    elif measurement=='clustering':\n",
    "        g_dir=nx.clustering(g,weight=weight)\n",
    "        \n",
    "    elif measurement=='node_count':\n",
    "        g_dir=len(g.nodes())\n",
    "        \n",
    "    elif measurement=='channels':\n",
    "        g_dir=dict(list(g.degree(g.nodes())))\n",
    "    \n",
    "    elif measurement=='capacity':\n",
    "        g_dir=dict(list(g.degree(g.nodes(),weight=weight)))\n",
    "        \n",
    "    elif measurement=='age': \n",
    "        \n",
    "        # Get node_details from S3 \n",
    "        opens_file = s3.Object(bucket_name=bucket, key='node_details.p').get()\n",
    "        node_details = pickle.loads(opens_file['Body'].read())\n",
    "        \n",
    "        # Create dic with node's age in blocks\n",
    "        g_dir={node:block-node_details[node]['birth_block'] for node in list(g.nodes())} \n",
    "        \n",
    "        \n",
    "    elif measurement=='capacity_growth':  \n",
    "        g_dir=capacity_growth (weight,bucket,g,block,s3,block_frame=3600)\n",
    "        \n",
    "    elif measurement=='closeness_approx_rank':\n",
    "        \n",
    "        # Re-select previous block\n",
    "        response = s3.Object(bucket_name=bucket, key=prev_key).get()\n",
    "        g=pickle.loads(response['Body'].read())\n",
    "        \n",
    "        g_dir=closeness_approx_rank (s3,bucket,g,block,p=13.38,estimate_sample=50)\n",
    "        score_type='/norm_rank/'\n",
    "        \n",
    "    elif measurement=='closeness_approx_rank_post': # Same measurement as above, just looking at the rank after block decisions happen\n",
    "        \n",
    "        g_dir=closeness_approx_rank (s3,bucket,g,block,p=13.38,estimate_sample=50)\n",
    "        score_type='/norm_rank/'\n",
    "        \n",
    "    elif measurement=='avg_short_path':\n",
    "        g_dir=nx.average_shortest_path_length(g)\n",
    "        \n",
    "    elif measurement=='min_nodes':\n",
    "        g_dir=nx.minimum_node_cut(g)\n",
    "        \n",
    "        \n",
    "    elif measurement=='robustness_eff_loss':\n",
    "        attack_perc=0.01\n",
    "        g_dir=robustness_eff_loss(s3,bucket,g,block,attack_perc)\n",
    "        measurement=measurement+'_'+str(attack_perc*100)\n",
    "    \n",
    "        \n",
    "    # Safe graph processing to S3\n",
    "    \n",
    "    extraction_id=key.split('/')[1].split('_')[0]\n",
    "    key_out='graph_snapshots/'+extraction_id+'_connected/.data_transformations/'+measurement+score_type+str(block)+'.pkl'\n",
    "    pickle_byte_obj = pickle.dumps(g_dir) \n",
    "    response=s3.Object(bucket,key_out).put(Body=pickle_byte_obj)['ResponseMetadata']['HTTPStatusCode']\n",
    "    \n",
    "    \n",
    "    \n",
    "    return (block,response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "capacity_growth\n",
    "    Calculates how much has capacity grown (or decreased) for all nodes in a graph\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "weight: str\n",
    "    Node property that will be used to weight the calculation.\n",
    "    \n",
    "bucket: str\n",
    "    S3 bucket where data is stored\n",
    "\n",
    "g: NetworkX graph\n",
    "    Graph for which the calculation will be computed\n",
    "    \n",
    "block: int\n",
    "    Block number corresponding to the selected graph\n",
    "\n",
    "s3: S3 session object\n",
    "    S3 session object for the boto3 api\n",
    "    \n",
    "block_frame: int\n",
    "    The amount of blocks into the past that will be considered to calculate growth\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "g_dir: dir\n",
    "    Dictionary with measurment values for each node\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def capacity_growth (weight,bucket,g,block,s3,block_frame=3600):\n",
    "    \n",
    "    # Initialize g_dir items and min_block\n",
    "    \n",
    "    g_dir={}\n",
    "    min_block=block-block_frame\n",
    "    \n",
    "    \n",
    "    # Get graph nodes\n",
    "    nodes=list(g.nodes())\n",
    "    \n",
    "    # Load decisions DataFrame\n",
    "    \n",
    "    decisions_load = s3.Object(bucket_name=bucket, key='decisions_df.csv').get()\n",
    "    decisions_df=pd.read_csv(io.BytesIO(decisions_load['Body'].read()))\n",
    "    \n",
    "    with tqdm(total=len(nodes),disable=True) as pbar:\n",
    "        \n",
    "        for node in nodes:\n",
    "\n",
    "            # Find all channel creations and closerues in block frame\n",
    "            opens_blockframe_node0=decisions_df[(decisions_df['node0_id']==node) & (decisions_df['open_block']>=min_block) & (decisions_df['open_block']<=block)]['satoshis']\n",
    "            opens_blockframe_node1=decisions_df[(decisions_df['node1_id']==node) & (decisions_df['open_block']>=min_block) & (decisions_df['open_block']<=block)]['satoshis']\n",
    "            closes_blockframe_node0=decisions_df[(decisions_df['node0_id']==node) & (decisions_df['close_block']>=min_block) & (decisions_df['close_block']<=block)]['satoshis']\n",
    "            closes_blockframe_node1=decisions_df[(decisions_df['node1_id']==node) & (decisions_df['close_block']>=min_block) & (decisions_df['close_block']<=block)]['satoshis']\n",
    "\n",
    "            # Calculate growth by adding capacity created in block frame and subtracting capacity lost\n",
    "            if weight==1: #Unweighted calculation\n",
    "                gain=opens_blockframe_node0.count()+opens_blockframe_node1.count()\n",
    "                loss=closes_blockframe_node0.count()+closes_blockframe_node1.count()\n",
    "\n",
    "            else:\n",
    "                gain=opens_blockframe_node0.sum()+opens_blockframe_node1.sum()\n",
    "                loss=closes_blockframe_node0.sum()+closes_blockframe_node1.sum()\n",
    "\n",
    "            # Calculate growth and save to dir\n",
    "            net_growth=gain-loss\n",
    "            g_dir[node]=net_growth\n",
    "            pbar.update(1)\n",
    "        \n",
    "\n",
    "    return g_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEST: Weighted capacity function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block selected:532022\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8fb351046764ca6a674e682973588e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=683.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dic was saved correctly. Sample below:\n",
      "[(6038, 0), (5314, 0), (934, 0), (3023, 0), (3436, 3131000), (3310, 0), (422, 0), (1912, 0), (5154, 0), (4688, 0)]\n"
     ]
    }
   ],
   "source": [
    "# Define test parameters\n",
    "\n",
    "\n",
    "measurement='capacity_growth'\n",
    "weight='capacity'\n",
    "g_test,nodes_test,g_dic_test,block=load_graph_measurement(extraction_id,measurement,weight,blocks,extract_keys,test_ix=-1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3436, 3131000), (1331, 5500000), (6156, 520000), (4998, 4485183), (4527, 200000), (2757, 1677721), (3065, 1200000), (346, 19061712), (2724, 1491661), (2460, 100000), (2476, 9564725), (1893, 300000), (6418, 15737856), (6599, 93799), (4534, 805272), (5641, 97000), (2295, 1200000), (6832, 792562), (7711, 1500000), (1410, 2700000), (1514, 89141), (6296, 1267024), (7608, 2541273), (7631, 182031), (3271, 21417), (2674, 9193000), (3382, 1872962), (1220, 20000), (4580, 500000), (6215, 11315423), (6363, 10000), (4620, 7124495), (7673, 520550), (4639, 20000), (4426, 553341), (5738, 1319859), (7259, 4490000), (6924, 343070), (448, 174997), (227, 675364), (326, 2000), (5372, 77107), (4490, 10000), (2739, 50000), (2881, 20000), (4819, 20000), (4427, 500000), (1172, 80000), (6378, 100000), (1120, 1777721), (5634, 5000), (2300, 500000), (2973, 34003), (6249, 60000), (3, 27529), (7606, 743866), (415, 389026), (5601, 3390969), (5406, 1500000), (5495, 600000), (1257, 500000), (2512, 350000), (7073, 443002), (1082, 318000), (595, 200000), (7379, 800000), (7566, 132765), (6400, 2000000), (3846, 682031), (3283, 6681303), (6975, 40928), (3635, 900000), (1350, 28764), (6646, 740000), (869, 77130), (4259, 165826), (99, 500000), (3351, 1677721), (5242, 669000), (3107, 200000), (7533, 357886), (6465, 1646300), (6435, 60000), (5063, 656735), (1469, 8974725), (5797, 86000), (764, 188831), (5016, 500000), (4175, 1000000), (6748, 2000000), (5321, 7455151), (275, 2000000), (6553, 1000000), (4950, 172552), (4571, 1200000), (3224, 200000), (1182, 6379999), (4701, 1014003), (3030, 20000), (5976, 500000), (6657, 1950000), (2604, 300000), (1901, 1999999), (2686, 300000), (3181, 1400000), (285, 1581273), (2247, 810629), (5796, 579000), (1131, 143366), (3841, 420177), (2789, 400000), (3904, 2293799), (6757, 2000000), (4896, 550000), (2785, 80800), (4516, 7930183), (1239, 666741), (4297, 600000), (180, 400000), (6303, 1133341), (62, 1227139), (5235, 400000), (174, 20000), (3979, 182519), (7194, 100000), (5311, 500000), (3796, 165826), (3882, 2700000), (953, 540000), (4409, 100000), (7039, 20000), (6160, 7400000), (7250, 616350), (5429, 122962), (5583, 400000), (7696, 21000), (252, 10000000), (4192, 3000000), (5005, 400000), (5066, 384510), (5899, 9193000), (5124, 480000), (3996, 40928), (4899, 26591), (1910, 176716), (6787, 500000), (1631, 222273), (4043, 150000), (3589, 50000), (424, 100000), (2696, 700000), (6408, 2175926), (1603, 1097000), (2769, 500000), (4720, 6750000), (795, 100000), (5209, 1600000), (5393, 220000), (3844, 169884), (1813, 482976), (2806, 20000), (4474, 100000), (6112, 240000), (6568, 2350000), (1503, 134510), (619, 150000), (5654, 491000), (4723, 150705), (4928, 55381), (6075, 86000), (7205, 500000), (3808, 1000000), (7125, 2226296), (2654, 22334), (4813, 500000), (5227, 872816), (6063, 852625), (6023, 70000), (1714, 140000), (1333, 280000), (5599, 16637691), (5933, 100000), (420, 318000), (6267, 295300), (4776, 1677721), (2466, 120000), (2994, 500000), (4735, 500000), (3474, 900000), (7734, 614869), (6061, 140863)]\n"
     ]
    }
   ],
   "source": [
    "positive=[t for t in list(g_dic_test.items()) if t[1]>0]\n",
    "print(positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_growth(g_test,g_dic_test,block,channel_opens,channel_closures):\n",
    "\n",
    "    test_list=[]\n",
    "    error_nodes=[]\n",
    "\n",
    "\n",
    "    min_block=block-3600\n",
    "\n",
    "    relevant_opens=[t[0] for t in list(channel_opens.items()) if (t[0]>=min_block and t[0]<=block)]\n",
    "    relevant_closures=[t[0] for t in list(channel_closures.items()) if (t[0]>=min_block and t[0]<=block)]\n",
    "    relevant_blocks=sorted(list(set(relevant_opens).union(set(relevant_closures))))\n",
    "\n",
    "\n",
    "    # Get graph nodes\n",
    "    nodes=list(g_test.nodes())\n",
    "\n",
    "        # Calculate net change for each node in each of the relevant blocks\n",
    "    with tqdm(total=len(nodes)) as pbar:\n",
    "        for node in nodes:\n",
    "            net_change=0\n",
    "            for block in relevant_blocks:\n",
    "\n",
    "                # Create list of nodes involved in channel opens and count how many times a node appears\n",
    "\n",
    "                if weight==1:\n",
    "                    list_block_opens=[[t[0],t[1]] for t in channel_opens[block] if (t[0]==node or t[1]==node)]\n",
    "                    total_weights=len(list_block_opens)\n",
    "\n",
    "                else:\n",
    "                    list_block_opens=[[t[0],t[1],t[2][weight]] for t in channel_opens[block] if (t[0]==node or t[1]==node)]\n",
    "                    total_weights=np.array([l[2] for l in list_block_opens]).sum()\n",
    "\n",
    "\n",
    "                net_change+=total_weights\n",
    "\n",
    "\n",
    "                # Create list of nodes involved in channel closures and count how many times a node appears\n",
    "\n",
    "                if weight==1:\n",
    "                    list_block_closures=[[t[0],t[1]] for t in channel_closures[block] if (t[0]==node or t[1]==node)]\n",
    "                    total_weights=len(list_block_opens)\n",
    "\n",
    "                else:\n",
    "                    list_block_closures=[[t[0],t[1],t[2][weight]] for t in channel_closures[block] if (t[0]==node or t[1]==node)]\n",
    "                    total_weights=np.array([l[2] for l in list_block_closures]).sum()\n",
    "\n",
    "\n",
    "                net_change-=total_weights\n",
    "\n",
    "            #Retrieve recorded growth for node\n",
    "\n",
    "            recorded_growth=g_dic_test[node] \n",
    "\n",
    "            # Check if growth match and populate test_list accordingly\n",
    "            if net_change==recorded_growth:\n",
    "                test_list.append(1)\n",
    "            else:\n",
    "                test_list.append(0)\n",
    "                error_nodes.append((node,recorded_growth,net_growth))\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "        # Add up all passed tests\n",
    "        tests_passed=np.array(test_list).sum()\n",
    "\n",
    "        # Print out statements based on test results\n",
    "    if tests_passed==len(nodes):\n",
    "        print('Growth calculated correctly for all nodes')\n",
    "\n",
    "\n",
    "    else: \n",
    "        print('Growth failed to be correctly calculated for {} nodes'.format(len(list(g_test.nodes()))-tests_passed))\n",
    "        if len(error_nodes)>10:\n",
    "            print('Some nodes with errors (node,recorded,actual)')\n",
    "            print(error_nodes[:10])\n",
    "\n",
    "        else:\n",
    "            print('Some nodes with errors (node,recorded,actual)')\n",
    "            print(error_nodes[:len(error_nodes)])\n",
    "        \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe37b5c85394f6193b009e7a0b12ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=683.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Growth calculated correctly for all nodes\n"
     ]
    }
   ],
   "source": [
    "test_growth(g_test,g_dic_test,block,channel_opens,channel_closures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEST: Test for age calculation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bafd82b8e064c6caebc03e31e9cdd89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5745.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Age calculated correctly for all nodes\n",
      "Example age for nodes in block 617169\n",
      "[(5314, 112020), (934, 110767), (3023, 110767), (3452, 110322), (576, 110322), (3436, 109094), (3310, 109094), (4223, 109079), (422, 108849), (1912, 108849)]\n"
     ]
    }
   ],
   "source": [
    "test_ix=-100\n",
    "test_block=blocks[test_ix]\n",
    "measurement='age'\n",
    "g_key=graph_keys[test_ix]\n",
    "block,response_test=graph_measurement((g_key,'age',None,bucket))\n",
    "\n",
    "#Load graph\n",
    "\n",
    "response = s3.Object(bucket_name=bucket, key=g_key).get()\n",
    "g_test=pickle.loads(response['Body'].read())\n",
    "nodes_test=list(g_test.nodes())\n",
    "\n",
    "if response_test==200:\n",
    "    key_test='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/raw_score/'+str(test_block)+'.pkl'\n",
    "    g_age_test_load = s3.Object(bucket_name=bucket, key=key_test).get()\n",
    "    g_age_test = pickle.loads(g_age_test_load['Body'].read())\n",
    "    \n",
    "    test_list=[]\n",
    "    error_nodes=[]\n",
    "    # Test that each node age is calculated correctly by looking at the decisions_df\n",
    "    with tqdm(total=len(nodes_test)) as pbar:\n",
    "        \n",
    "        for n in nodes_test:\n",
    "            \n",
    "            # Find creation block by looking at node0 and node1 columns\n",
    "            firstseenas_node0=decisions_df[decisions_df['node0_id']==n]['open_block'].min()\n",
    "            firstseenas_node1=decisions_df[decisions_df['node1_id']==n]['open_block'].min()\n",
    "            \n",
    "            # Correct for nan values, in case node is not present in either column (make it infinite)\n",
    "            fs_list=[firstseenas_node0,firstseenas_node1]\n",
    "            fs_list=[np.inf if np.isnan(i) else i for i in fs_list]\n",
    "            \n",
    "            # Calculate age\n",
    "            actual_age=block-min(fs_list[0],fs_list[1])\n",
    "            recorded_age=g_age_test[n]\n",
    "            \n",
    "            # Check if ages match and populate test_list accordingly\n",
    "            if recorded_age==actual_age:\n",
    "                test_list.append(1)\n",
    "            else:\n",
    "                test_list.append(0)\n",
    "                error_nodes.append((n,recorded_age,actual_age))\n",
    "                \n",
    "            pbar.update(1)\n",
    "            \n",
    "        # Add up all passed tests\n",
    "        tests_passed=np.array(test_list).sum()\n",
    "\n",
    "    \n",
    "# Print out statements based on test results\n",
    "    if tests_passed==len(nodes_test):\n",
    "        print('Age calculated correctly for all nodes')\n",
    "   \n",
    "    \n",
    "    else: \n",
    "        print('Age failed to be correctly calculated for {} nodes'.format(len(list(g_test.nodes()))-tests_passed))\n",
    "        if len(error_nodes)>10:\n",
    "            print('Some nodes with errors (node,recorded_age,actual_age)')\n",
    "            print(error_nodes[:10])\n",
    "        \n",
    "        else:\n",
    "            print('Some nodes with errors (node,recorded_age,actual_age)')\n",
    "            print(error_nodes[:len(error_nodes)])\n",
    "        \n",
    "    \n",
    "    print('Example age for nodes in block {}'.format(test_block))\n",
    "    print(list(g_age_test.items())[:10])\n",
    "    \n",
    "\n",
    "else:\n",
    "    print('Age was not saved correctly')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Aproximate Node closeness rank**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "---OLD VERSION---\n",
    "def approx_node_closrank (bucket,key,block,prev_block,nodes,p=13.38,estimate_sample=50):\n",
    "                             \n",
    "    \n",
    "    # Download graph and extract nodes\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.resource('s3')\n",
    "    response = s3.Object(bucket_name=bucket, key=key).get()\n",
    "    g=pickle.loads(response['Body'].read())\n",
    "    g_nodes=list(g.nodes())\n",
    "    n=len(g_nodes)\n",
    "                             \n",
    "    # Estimate c_mid for graph\n",
    "    estimation_nodes=random.sample(g_nodes, estimate_sample)\n",
    "    c_mid=np.array([nx.closeness_centrality(g,n) for n in estimation_nodes]).mean()\n",
    "\n",
    "                             \n",
    "    # Calculate closeness centrality for selected nodes in block\n",
    "    \n",
    "    clo_list=np.array([nx.closeness_centrality(g,n) for n in nodes])\n",
    "    \n",
    "    # Aproximate ranking using formula from: https://arxiv.org/pdf/1706.02083.pdf\n",
    "    norm_rank=n+((1-n)/(1+np.power((clo_list/c_mid),p)))\n",
    "    norm_rank=list(norm_rank)\n",
    "    \n",
    "    # Create dictionary with ranking per node\n",
    "    g_dir={node:rank for node,rank in zip(nodes,norm_rank)}\n",
    "    \n",
    "    # Save to S3    \n",
    "    measurement='closeness_approx'\n",
    "    extraction_id=key.split('/')[1].split('_')[0]\n",
    "    key_out='graph_snapshots/'+extraction_id+'_connected/.data_transformations/'+measurement+'/norm_rank/'+block+'.pkl'\n",
    "    pickle_byte_obj = pickle.dumps(g_dir) \n",
    "    response=s3.Object(bucket,key_out).put(Body=pickle_byte_obj)['ResponseMetadata']['HTTPStatusCode']\n",
    "    \n",
    "    \n",
    "    return response\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closeness_approx_rank (s3,bucket,g,block,p=13.38,estimate_sample=50):\n",
    "    \n",
    "    g_nodes=list(g.nodes())\n",
    "    n=len(g_nodes)\n",
    "    \n",
    "    # Download decisions_df\n",
    "    decisions_load = s3.Object(bucket_name=bucket, key='decisions_df.csv').get()\n",
    "    decisions_df=pd.read_csv(io.BytesIO(decisions_load['Body'].read()))\n",
    "    \n",
    "    # Extract nodes involved in decisions from the block \n",
    "    node0_open=decisions_df[decisions_df['open_block']==block]['node0_id'].tolist()\n",
    "    node1_open=decisions_df[decisions_df['open_block']==block]['node1_id'].tolist()\n",
    "    node0_close=decisions_df[decisions_df['close_block']==block]['node0_id'].tolist()\n",
    "    node1_close=decisions_df[decisions_df['close_block']==block]['node1_id'].tolist()\n",
    "    \n",
    "    # Create list of nodes existing in previous graph\n",
    "    nodes=node0_open+node1_open+node0_close+node1_close\n",
    "    nodes=list(set(nodes).intersection(set(g_nodes)))\n",
    "    \n",
    "    # Create list of nodes and set closeness to 0 to nodes that are not present\n",
    "    missing_nodes=list(set(nodes).difference(set(g_nodes)))\n",
    "    missing_clo=list(np.zeros(len(missing_nodes)))\n",
    "                             \n",
    "    # Estimate c_mid for graph by averaging closeness for a sample of nodes\n",
    "    \n",
    "    if len(g_nodes)<=estimate_sample:\n",
    "        estimation_nodes=g_nodes\n",
    "    else:\n",
    "        estimation_nodes=random.sample(g_nodes, estimate_sample)\n",
    "    \n",
    "    c_mid=np.array([nx.closeness_centrality(g,n) for n in estimation_nodes]).mean()\n",
    "\n",
    "                             \n",
    "    # Calculate closeness centrality for selected nodes in block\n",
    "    clo_list=np.array([nx.closeness_centrality(g,n) for n in nodes])\n",
    "    \n",
    "    # Aproximate ranking using formula from: https://arxiv.org/pdf/1706.02083.pdf\n",
    "    norm_rank_array=(n+((1-n)/(1+np.power((clo_list/c_mid),p))))/n\n",
    "    norm_rank=list(norm_rank_array)+missing_clo\n",
    "    \n",
    "    # Update nodes list with missing nodes\n",
    "    nodes=nodes+missing_nodes\n",
    "    \n",
    "    # Create dictionary with ranking per node\n",
    "    g_dir={node:rank for node,rank in zip(nodes,norm_rank)}\n",
    "    \n",
    "    \n",
    "    return g_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Values for all nodes are between 0-1\n",
      "Test Block:518195\n",
      "Nodes in Block:316\n",
      "Standard deviation of measurement: 0.07186561837374655\n",
      "Time elapse: 1.0128343105316162\n",
      "[(5424, 0.8409218063257561), (2537, 0.9846530430732492)]\n"
     ]
    }
   ],
   "source": [
    "# TEST aprox node closeness rank\n",
    "\n",
    "# Select testing Block and extract key/block\n",
    "test_ix=random.choice(range(1,len(blocks))) \n",
    "measurement='closeness_approx_rank'\n",
    "g_key=graph_keys[test_ix]\n",
    "prev_key=graph_keys[test_ix-1]\n",
    "\n",
    "rand_block=blocks[test_ix-1]\n",
    "\n",
    "\n",
    "# Download graph associated to test\n",
    "g_test_key='graph_snapshots/'+str(extraction_id)+'_connected/'+str(rand_block)+'.gpickle'\n",
    "response = s3.Object(bucket_name=bucket, key=g_test_key).get()\n",
    "g_test=pickle.loads(response['Body'].read())\n",
    "nodes_test=list(g_test.nodes())\n",
    "\n",
    "# Run node closeness function\n",
    "start=time.time()\n",
    "block,response_test=graph_measurement((g_key,measurement,None,bucket,prev_key))\n",
    "end=time.time()\n",
    "\n",
    "\n",
    "# Test if function saved result correctly and download result\n",
    "if response_test==200:\n",
    "    key_test='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/norm_rank/'+str(block)+'.pkl'\n",
    "   \n",
    "    g_clo_test_load = s3.Object(bucket_name=bucket, key=key_test).get()\n",
    "    g_clo_test = pickle.loads(g_clo_test_load['Body'].read())\n",
    "    clo_values=[v for k,v in list(g_clo_test.items())]\n",
    "\n",
    "\n",
    "# Test if no. items in dictionary are equal to nodes in graph\n",
    "dic_items=list(g_clo_test.items())\n",
    "print(len(dic_items))\n",
    "\n",
    "\n",
    "\n",
    "# Test that values are >0 and <1\n",
    "range_test=[]\n",
    "for n,v in dic_items:\n",
    "    if v>=0 and v<=1:\n",
    "        range_test.append(1)\n",
    "    else:\n",
    "        range_test.append(0)\n",
    "\n",
    "range_test_passed=np.array(range_test).sum()        \n",
    "\n",
    "if range_test_passed==len(dic_items):\n",
    "    print('Values for all nodes are between 0-1')\n",
    "else:\n",
    "    print('Values for some nodes are outside the [0,1] range')\n",
    "    \n",
    "\n",
    "\n",
    "# Download result from function\n",
    "\n",
    "print('Test Block:{}'.format(rand_block))\n",
    "print('Nodes in Block:{}'.format(len(nodes_test)))\n",
    "print('Standard deviation of measurement: {}'.format(np.std(np.array(clo_values))))\n",
    "print('Time elapse: {}'.format(end-start))\n",
    "print(list(g_clo_test.items()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Aproximate Node closeness rank**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robustness_eff_loss(s3,bucket,g,block,attack_perc):\n",
    "    \n",
    "    \n",
    "    # Get global base efficiency by extracting sp from S3\n",
    "    \n",
    "    print(block)\n",
    "    avg_sp_key='graph_snapshots/1587447789_connected/.data_transformations/avg_short_path/raw_score/'+str(block)+'.pkl'\n",
    "    response = s3.Object(bucket_name=bucket, key=avg_sp_key).get()\n",
    "    avg_sp=pickle.loads(response['Body'].read())\n",
    "    init_efficiency=1/avg_sp\n",
    "    \n",
    "    # Calculate 1% of highest degree nodes\n",
    "    num_nodes=len(g.nodes())\n",
    "    num_top_nodes=max(1,int(attack_perc*num_nodes))\n",
    "    g_degrees=[deg for deg in g.degree()]\n",
    "    top_degrees=sorted(g_degrees,key=lambda y: y[1],reverse=True)[:num_top_nodes]\n",
    "    top_nodes=[n for n,d in top_degrees]\n",
    "    \n",
    "    # Remove top nodes \n",
    "    g_pruned=g.copy()\n",
    "    g_pruned.remove_nodes_from(top_nodes)\n",
    "    \n",
    "    # Re-calculate efficiency for pruened graph and robustness\n",
    "    final_efficiency=nx.global_efficiency(g_pruned)\n",
    "    robustness=final_efficiency/init_efficiency\n",
    "    \n",
    "    \n",
    "    return robustness\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph_snapshots/1587447789_connected/591968.gpickle\n",
      "591968\n"
     ]
    }
   ],
   "source": [
    "# TEST robustness_eff_loss\n",
    "\n",
    "# Select testing Block and extract key/block\n",
    "test_ix=random.choice(range(1,len(sample_keys))) \n",
    "\n",
    "\n",
    "measurement='robustness_eff_loss'\n",
    "g_test_key=sample_keys[test_ix]\n",
    "#prev_key=graph_keys[test_ix-1]\n",
    "\n",
    "print(g_test_key)\n",
    "\n",
    "\n",
    "# Download graph associated to test\n",
    "\n",
    "response = s3.Object(bucket_name=bucket, key=g_test_key).get()\n",
    "g_test=pickle.loads(response['Body'].read())\n",
    "nodes_test=list(g_test.nodes())\n",
    "rand_block=g_test.graph['block']\n",
    "\n",
    "# Run robustness_eff_loss\n",
    "start=time.time()\n",
    "block,response_test=graph_measurement((g_test_key,measurement,None,bucket,None))\n",
    "end=time.time()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph_snapshots/1587447789_connected/.data_transformations/robustness_eff_loss_1.0/raw_score/591968.pkl\n",
      "Test Block:591968\n",
      "Nodes in Block:5184\n",
      "Time elapse: 76.71983003616333\n",
      "Sample: 0.4836248242892463\n"
     ]
    }
   ],
   "source": [
    "# Test if function saved result correctly and download result\n",
    "if response_test==200:\n",
    "    key_test='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'_1.0/raw_score/'+str(block)+'.pkl'\n",
    "    print(key_test)\n",
    "    g_rob_test_load = s3.Object(bucket_name=bucket, key=key_test).get()\n",
    "    g_rob_test = pickle.loads(g_rob_test_load['Body'].read())\n",
    "    \n",
    "    \n",
    "\n",
    "print('Test Block:{}'.format(block))\n",
    "print('Nodes in Block:{}'.format(len(nodes_test)))\n",
    "print('Time elapse: {}'.format(end-start))   \n",
    "print('Sample: {}'.format(g_rob_test))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Property measurment for a couple of nodes in a graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "node_measurement\n",
    "    Performs selected graph measurment on specific nodes in graph\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "g : nx graph\n",
    "    NetworkX graph object over which the measurment will be performed\n",
    "\n",
    "measurement: string\n",
    "    Type of measurement to be performend in graph\n",
    "    \n",
    "node0: int\n",
    "    Node id for node 0\n",
    "\n",
    "node1: int\n",
    "    Node id for node 1\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "node_tuple: tuple\n",
    "    Tuple of the form (node0_mes,node1_mes)\n",
    "    \n",
    "    node0_mes: float\n",
    "        Graph measurement for node0\n",
    "    node1_mes: float\n",
    "        Graph measurement for node1\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "def node_measurement(g,measurement,node0,node1):\n",
    "    \n",
    "    measurement_input=(g,measurement,'capacity')\n",
    "    block,g_dir=graph_measurement(measurement_input)\n",
    "    \n",
    "    node0_mes=g_dir[node0]\n",
    "    node1_mes=g_dir[node1]\n",
    "        \n",
    "    # Update marginal values for node0 and node1\n",
    "        \n",
    "    if (g.has_node(node0)): #If connected component of marginal graph contains node0 find betweeness\n",
    "        node0_mes=g_dir[node0]\n",
    "    else: # else update with fixed value\n",
    "        node0_mes=0\n",
    "            \n",
    "    if (g.has_node(node1)): #If connected component of marginal graph contains node1 find betweeness\n",
    "        node1_mes=g_dir[node1]\n",
    "    else: # else update with fixed value\n",
    "        node1_mes=0\n",
    "    \n",
    "    return (node0_mes,node1_mes)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incremental_closeness(bucket,graph_keys,blocks,start_point):\n",
    "    \n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.resource('s3')\n",
    "    counter=0\n",
    "    extraction_id=graph_keys[0].split('/')[1].split('_')[0]\n",
    "    responses=[]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Channel closures\n",
    "    closure_file = s3.Object(bucket_name=bucket, key='channel_closures.p').get()\n",
    "    channel_closures = pickle.loads(closure_file['Body'].read())\n",
    "    \n",
    "    \n",
    "    # Channel openings \n",
    "    opens_file = s3.Object(bucket_name=bucket, key='channel_opens.p').get()\n",
    "    channel_opens = pickle.loads(opens_file['Body'].read())\n",
    "    snapshot_clo={}\n",
    "    \n",
    "    #Initialize graph with all nodes \n",
    "    lastgraph_block=blocks[-1]\n",
    "    last_key='graph_snapshots/'+str(extraction_id)+'/'+str(lastgraph_block)+'.gpickle'\n",
    "    response = s3.Object(bucket_name=bucket, key=last_key).get()\n",
    "    G_final=pickle.loads(response['Body'].read())\n",
    "    nodes_final=list(G_final.nodes())\n",
    "    G=nx.Graph()\n",
    "    G.add_nodes_from(nodes_final)\n",
    "    prev_clo=None\n",
    "    \n",
    "    if start_point>0:\n",
    "        # Get previous graph\n",
    "        inigraph_block=blocks[start_point-1]\n",
    "        ini_key='graph_snapshots/'+str(extraction_id)+'/'+str(inigraph_block)+'.gpickle'\n",
    "        response = s3.Object(bucket_name=bucket, key=ini_key).get()\n",
    "        G_ini=pickle.loads(response['Body'].read())\n",
    "        \n",
    "        # Get previous closeness centrality\n",
    "        measurement='incremental_closeness'\n",
    "        key_clo='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/raw_score/'+str(inigraph_block)+'.pkl'\n",
    "        response = s3.Object(bucket_name=bucket, key=key_clo).get()\n",
    "        prev_clo=pickle.loads(response['Body'].read())\n",
    "      \n",
    "        G.add_edges_from(list(G_ini.edges(data=True)))\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    with tqdm(total=len(range(start_point,len(graph_keys)))) as pbar:\n",
    "        for i  in range(start_point,len(graph_keys)):\n",
    "\n",
    "            \n",
    "            block=blocks[i]\n",
    "            new_edges=channel_opens[block]\n",
    "            closed_edges=channel_closures[block]\n",
    "        \n",
    "\n",
    "            # Incremental closeness calculation for OPENS\n",
    "\n",
    "            with tqdm(total=len(new_edges),disable=True) as pbar1:\n",
    "                for edge in new_edges:\n",
    "\n",
    "                    #If edge exists in previous graph closeness doesn't change\n",
    "                    if G.has_edge(edge[0],edge[1]):\n",
    "                        new_clo=prev_clo\n",
    "                        G.edges[edge[0],edge[1]]['no_channels']+=1\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        new_clo=nx.incremental_closeness_centrality(G,(edge[0],edge[1]),prev_cc=prev_clo,insertion=True)\n",
    "                        G.add_edges_from([edge])\n",
    "                \n",
    "\n",
    "                    prev_clo=new_clo\n",
    "                    pbar1.update(1)\n",
    "                \n",
    "            \n",
    "\n",
    "            # Incremental closeness calculation for CLOSES\n",
    "\n",
    "            with tqdm(total=len(closed_edges),disable=True) as pbar2:\n",
    "                for edge in closed_edges:\n",
    "\n",
    "                    # Verify if existing edges result from multiple channels, if so, only reduce capacity otherwise remove edge\n",
    "                    no_channels=G.edges[edge[0],edge[1]]['no_channels']\n",
    "\n",
    "                    #If edge exists in previous graph closeness doesn't change\n",
    "                    if no_channels>1:\n",
    "                        new_clo=prev_clo\n",
    "                        G.edges[edge[0],edge[1]]['no_channels']-=1\n",
    "\n",
    "                    else:                                    \n",
    "                        new_clo=nx.incremental_closeness_centrality(G,(edge[0],edge[1]),prev_cc=prev_clo,insertion=False)                   \n",
    "                        G.remove_edge(edge[0],edge[1])\n",
    "\n",
    "                    prev_clo=new_clo\n",
    "                    pbar2.update(1)\n",
    "                \n",
    "\n",
    "\n",
    "            # Safe outcome\n",
    "            g_dir=new_clo\n",
    "\n",
    "\n",
    "            # Safe graph processing to S3\n",
    "            \n",
    "            measurement='incremental_closeness'\n",
    "            key_out='graph_snapshots/'+extraction_id+'_connected/.data_transformations/'+measurement+'/raw_score/'+str(block)+'.pkl'\n",
    "            pickle_byte_obj = pickle.dumps(g_dir) \n",
    "            response=s3.Object(bucket,key_out).put(Body=pickle_byte_obj)['ResponseMetadata']['HTTPStatusCode']\n",
    "            #print((block,response))\n",
    "\n",
    "\n",
    "            # Loop updates\n",
    "            pbar1.close()\n",
    "            pbar2.close()\n",
    "            pbar.update(1)\n",
    "            responses.append(response)\n",
    "            \n",
    "        \n",
    "        output={b:res for b,res in zip(blocks,responses)}\n",
    "\n",
    "           \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit sigmoid for p by looking at sample of graphs\n",
    "# estimate mid closeness by samplig closeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sp_closeness(G):\n",
    "    \n",
    "    nodes=list(G.nodes())\n",
    "    n=len(nodes)\n",
    "    sp_matrix=np.zeros((len(nodes),len(nodes)))\n",
    "    sp_matrix=sp_matrix.astype(object)\n",
    "    print(sp_matrix.dtype)\n",
    "    \n",
    "    dic_clo={}\n",
    "    \n",
    "    \n",
    "    for i in range(n):\n",
    "\n",
    "        for j in range(i,n):\n",
    "            # Calculate delayed shortest path\n",
    "            input_tuple=(G,nodes[i],nodes[j])\n",
    "            sp=dask.delayed(len_shortest_path)(input_tuple)\n",
    "            #print(type(sp))\n",
    "            sp_matrix[i][j]=sp\n",
    "            sp_matrix[j][i]=sp\n",
    "        \n",
    "        sp_sum=np.array(dask.compute(*sp_matrix[i].tolist())).sum()\n",
    "        clo_i=n-1/sp_sum\n",
    "        dic_clo[i]=clo_i\n",
    "    return dic_clo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "start=time.time()\n",
    "dic_clo_test=sp_closeness(G_test)\n",
    "end=time.time()\n",
    "print(sp)\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print(list(dic_clo_test.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute in seconds: 10.438627481460571\n"
     ]
    }
   ],
   "source": [
    "snapshot_nodes=collection_measure(bucket,extract_keys,'node_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(snapshot_nodes.items())[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Baseline Current Betweeness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute in seconds: 8009.403836488724\n"
     ]
    }
   ],
   "source": [
    "# Calculate Baseline betweeness\n",
    "snapshot_bet=collection_measure(bucket,extract_keys,'current_betweeness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab210d796de43c4b30280d4e42d8068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=36535.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compute in seconds: 12896.576698541641\n"
     ]
    }
   ],
   "source": [
    "# Calculate Baseline unweighted betweeness\n",
    "snapshot_bet_uw=collection_measure(bucket,extract_keys,'current_betweeness_unweighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save to S3 succesful\n"
     ]
    }
   ],
   "source": [
    "# Save baseline current betweeness to S3: For old calculation of Betweeness\n",
    "response=pickle_save_s3(snapshot_bet,blocks,extraction_id,'snapshot_bet')\n",
    "if response['HTTPStatusCode']==200:\n",
    "    print('Save to S3 succesful')\n",
    "else:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to single file per graph format\n",
    "# Load large dic from S3\n",
    "key='graph_snapshots/1587447789_connected/.data_transformations/1587447789snapshot_bet-36536-508400-617297.pkl'\n",
    "response = s3.Object(bucket_name=bucket, key=key).get()\n",
    "snapshot_bet=pickle.loads(response['Body'].read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c48118dd15a4defb5af75a6461ab58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=36536.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Delayed store function\n",
    "\n",
    "responses=[]\n",
    "with tqdm(total=len(blocks)) as pbart:\n",
    "    for block in blocks:\n",
    "        dic=snapshot_bet[block]\n",
    "        #create save key\n",
    "        measurement='betweeness_curr_aprox'\n",
    "        key_out='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/raw_score/'+str(block)+'.pkl'\n",
    "\n",
    "        #run save function\n",
    "        input_tuple=(bucket,key_out,dic)\n",
    "        response=simple_psave_s3(input_tuple)\n",
    "        responses.append(response)\n",
    "        pbart.update(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "betweeness_curr_aprox was calculated for ALL 36536 blocks correctly\n",
      "Example of betweeness_curr_aprox saved for block 607627:\n",
      "[(5314, 2.7627499157415683e-05), (934, 3.5404153621536765e-05), (3023, 7.752852808574474e-15), (3452, 4.186574492807752e-06), (576, 2.7889615061953227e-16), (3436, 0.0013869522957864274), (3310, 1.6268942119472713e-15), (2378, 5.268038400591164e-15), (4223, 0.0007224518380271746), (422, 0.0005759993487047296)]\n",
      "Total entries: 5506\n",
      "Number of nodes match\n"
     ]
    }
   ],
   "source": [
    "# TEST all saved correctly\n",
    "#responses=np.array([r for b,r in snapshot_capgrowth.items()])\n",
    "\n",
    "measurement='betweeness_curr_aprox' \n",
    "# Retrieve saved items\n",
    "\n",
    "bet_cur_keys = [obj.key \n",
    "        for obj in s3.Bucket(name=bucket).objects.all()\n",
    "        if re.match(\".*\"+str(extraction_id)+\"_connected/\\.data_transformations/betweeness_curr_aprox/.*\\.pkl\",obj.key)]\n",
    "\n",
    "if len(bet_cur_keys)==len(blocks):\n",
    "    print('{} was calculated for ALL {} blocks correctly'.format(measurement,len(bet_cur_keys)))\n",
    "else:\n",
    "    print('Error saving to S3')\n",
    "    \n",
    "  \n",
    "rand_block=str(random.choice(blocks))    \n",
    "test_key='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/raw_score/'+rand_block+'.pkl'\n",
    "test_file = s3.Object(bucket_name=bucket, key=test_key).get()\n",
    "test_object = pickle.loads(test_file['Body'].read())\n",
    "print('Example of {} saved for block {}:'.format(measurement,rand_block))\n",
    "print(list(test_object.items())[:10])\n",
    "print('Total entries: {}'.format(len(list(test_object.items()))))\n",
    "\n",
    "# Get test graph\n",
    "g_test_key='graph_snapshots/'+str(extraction_id)+'_connected/'+str(rand_block)+'.gpickle'\n",
    "response = s3.Object(bucket_name=bucket, key=g_test_key).get()\n",
    "g_test=pickle.loads(response['Body'].read())\n",
    "\n",
    "# Run test function on test graph: Check that dic stored has same number of nodes than graph\n",
    "\n",
    "nodes_gtest=list(g_test.nodes())\n",
    "items_dict_test=list(test_object.items())\n",
    "\n",
    "if len(nodes_gtest)==len(items_dict_test):\n",
    "    print('Number of nodes match')\n",
    "else:\n",
    "    print('Number of nodes does NOT match')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Baseline Current Closeness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Baseline current closeness\n",
    "#snapshot_clo=collection_measure(bucket,extract_keys,'current_closeness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate incremental closeness\n",
    "snapshot_clo=incremental_closeness(bucket,extract_keys,blocks,1425+3073)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graph_snapshots/1587447789_connected/535029.gpickle'"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_keys[1402]\n",
    "\n",
    "#channel_opens[1407]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "key='graph_snapshots/1587447789_connected/'+str(blocks[-100])+'.gpickle'\n",
    "response = s3.Object(bucket_name=bucket, key=key).get()\n",
    "G_test=pickle.loads(response['Body'].read())\n",
    "\n",
    "node1=random.choice(list(G.nodes()))\n",
    "node2=random.choice(list(G.nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6306, 2470)\n",
      "0\n",
      "Compute in seconds: 9.965896606445312e-05\n"
     ]
    }
   ],
   "source": [
    "print((node1,node2))\n",
    "#len(G_test.nodes())\n",
    "start=time.time()\n",
    "sp=nx.shortest_path_length(G_test, source=node1, target=node1)\n",
    "end=time.time()\n",
    "print(sp)\n",
    "print('Compute in seconds: {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0.2, 2: 0.2, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0}\n",
      "[(1, 2)]\n",
      "{1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2, 5: 0.0, 6: 0.0}\n",
      "[(1, 2), (3, 4)]\n",
      "{1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2, 5: 0.2, 6: 0.2}\n",
      "[(1, 2), (3, 4), (5, 6)]\n",
      "{1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2, 5: 0.0, 6: 0.0}\n",
      "[(1, 2), (3, 4)]\n"
     ]
    }
   ],
   "source": [
    "G=nx.Graph()\n",
    "G.add_nodes_from([1,2,3,4,5,6])\n",
    "#G.add_edge(2,1)\n",
    "new_clo=nx.incremental_closeness_centrality(G,(2,1),prev_cc=None,insertion=True)\n",
    "G.add_edge(2,1)\n",
    "print(new_clo)\n",
    "print(G.edges())\n",
    "\n",
    "\n",
    "new_clo=nx.incremental_closeness_centrality(G,(3,4),prev_cc=new_clo,insertion=True)\n",
    "G.add_edge(3,4)\n",
    "print(new_clo)\n",
    "print(G.edges())\n",
    "\n",
    "\n",
    "new_clo=nx.incremental_closeness_centrality(G,(5,6),prev_cc=new_clo,insertion=True)\n",
    "G.add_edge(5,6)\n",
    "print(new_clo)\n",
    "print(G.edges())\n",
    "\n",
    "\n",
    "new_clo=nx.incremental_closeness_centrality(G,(5,6),prev_cc=new_clo,insertion=False)\n",
    "G.remove_edge(5,6)\n",
    "\n",
    "print(new_clo)\n",
    "print(G.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save to S3 succesful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec1dca6cf3d443abad6f463e8fad8fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1400.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "% of nodes with positive closeness per Block : 0.9888914970559514\n",
      "Total Blocks: 1400\n"
     ]
    }
   ],
   "source": [
    "# TEST all saved correctly\n",
    "responses=np.array([r for b,r in snapshot_clo.items()])\n",
    "\n",
    "if np.amax(responses)==np.amin(responses): #and len(responses) == len(extract_keys):\n",
    "    print('Save to S3 succesful')\n",
    "else:\n",
    "    print('Error saving to S3')\n",
    "\n",
    "range_test=[]\n",
    "no_blocks=1400\n",
    "    \n",
    "with tqdm(total=no_blocks) as pbar:\n",
    "    for i in range(no_blocks):\n",
    "\n",
    "        measurement='incremental_closeness'    \n",
    "        #rand_block=str(random.choice(blocks[:1200]))    \n",
    "        rand_block=str(blocks[i])\n",
    "\n",
    "        test_key='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/raw_score/'+rand_block+'.pkl'\n",
    "        test_file = s3.Object(bucket_name=bucket, key=test_key).get()\n",
    "        test_object = pickle.loads(test_file['Body'].read())\n",
    "\n",
    "        graph_key='graph_snapshots/'+str(extraction_id)+'_connected/'+str(rand_block)+'.gpickle'\n",
    "        graph_response = s3.Object(bucket_name=bucket, key=graph_key).get()\n",
    "        G=pickle.loads(graph_response['Body'].read())\n",
    "\n",
    "\n",
    "        #print('Example of {} saved for block {}:'.format(measurement,rand_block))\n",
    "        #print(list(test_object.items())[:100]) \n",
    "        #print('Total nodes graph: {}'.format(len(G.nodes())))\n",
    "\n",
    "        connected_nodes=[(b,cent) for b,cent in list(test_object.items()) if cent>0]\n",
    "        #print('Total entries: {}'.format(len(connected_nodes)))\n",
    "\n",
    "        node_cons=[]\n",
    "        for node in list(G.nodes()):\n",
    "\n",
    "            node_con=test_object[node]\n",
    "            \n",
    "          \n",
    "            if node_con>0:\n",
    "                node_cons.append(1)\n",
    "            else:\n",
    "                node_cons.append(0)\n",
    "            \n",
    "            \n",
    "\n",
    "        block_test=np.array(node_cons).sum()/len(G.nodes)          \n",
    "\n",
    "        '''\n",
    "        if block_test==1:\n",
    "            test=1\n",
    "        else:\n",
    "            test=0\n",
    "        '''\n",
    "\n",
    "        range_test.append(block_test)\n",
    "        #range_test.append(test)\n",
    "        pbar.update(1)\n",
    "\n",
    "print('% of nodes with positive closeness per Block : {}'.format(np.array(range_test).mean()))\n",
    "#print('Blocks with closeness for all nodes: {}'.format(np.array(range_test).sum()))\n",
    "print('Total Blocks: {}'.format(no_blocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9950617283950617"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(range_test)[1399]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Baseline Channels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute in seconds: 186.33308386802673\n"
     ]
    }
   ],
   "source": [
    "# Calculate Baseline degree\n",
    "snapshot_channels=collection_measure(bucket,extract_keys,'channels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST all saved correctly\n",
    "responses=np.array([r for b,r in snapshot_channels.items()])\n",
    "\n",
    "if np.amax(responses)==np.amin(responses) and len(responses) == len(extract_keys):\n",
    "    print('Save to S3 succesful')\n",
    "else:\n",
    "    print('Error saving to S3')\n",
    "    \n",
    "measurement='channels'    \n",
    "rand_block=str(random.choice(blocks))    \n",
    "test_key='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/raw_score/'+rand_block+'.pkl'\n",
    "test_file = s3.Object(bucket_name=bucket, key=test_key).get()\n",
    "test_object = pickle.loads(test_file['Body'].read())\n",
    "print('Example of {} saved for block {}:'.format(measurement,rand_block))\n",
    "print(list(test_object.items())[:10])    \n",
    "print('Total entries: {}'.format(len(list(test_object.items()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Baseline Capacity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute in seconds: 205.32206177711487\n"
     ]
    }
   ],
   "source": [
    "# Calculate Baseline degree\n",
    "snapshot_capacity=collection_measure(bucket,extract_keys,'capacity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save to S3 succesful\n",
      "Example of capacity saved for block 586042:\n",
      "[(6038, 8878679), (5314, 14026340), (934, 1171934), (3023, 1111934), (3452, 2063908), (576, 40000), (3436, 6948131), (3310, 100000), (2378, 400000), (4223, 24298841)]\n",
      "Total entries: 4920\n"
     ]
    }
   ],
   "source": [
    "# TEST all saved correctly\n",
    "responses=np.array([r for b,r in snapshot_capacity.items()])\n",
    "\n",
    "if np.amax(responses)==np.amin(responses) and len(responses) == len(extract_keys):\n",
    "    print('Save to S3 succesful')\n",
    "else:\n",
    "    print('Error saving to S3')\n",
    "    \n",
    "measurement='capacity'    \n",
    "rand_block=str(random.choice(blocks))    \n",
    "test_key='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/raw_score/'+rand_block+'.pkl'\n",
    "test_file = s3.Object(bucket_name=bucket, key=test_key).get()\n",
    "test_object = pickle.loads(test_file['Body'].read())\n",
    "print('Example of {} saved for block {}:'.format(measurement,rand_block))\n",
    "print(list(test_object.items())[:10])\n",
    "print('Total entries: {}'.format(len(list(test_object.items()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Baseline Age**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5b44ee0cd944909ff8de4320c225b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=36536.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compute in seconds: 511.2060635089874\n"
     ]
    }
   ],
   "source": [
    "# Calculate Baseline age\n",
    "snapshot_age=collection_measure(bucket,extract_keys,'age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save to S3 succesful\n",
      "Example of age saved for block 568893:\n",
      "[(6038, 63744), (5314, 63744), (934, 62491), (3023, 62491), (3452, 62046), (576, 62046), (3436, 60818), (3310, 60818), (2378, 60803), (4223, 60803)]\n",
      "Total entries: 3872\n"
     ]
    }
   ],
   "source": [
    "# TEST all saved correctly\n",
    "responses=np.array([r for b,r in snapshot_age.items()])\n",
    "\n",
    "if np.amax(responses)==np.amin(responses) and len(responses) == len(extract_keys):\n",
    "    print('Save to S3 succesful')\n",
    "else:\n",
    "    print('Error saving to S3')\n",
    "    \n",
    "measurement='age'    \n",
    "rand_block=str(random.choice(blocks))    \n",
    "test_key='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/raw_score/'+rand_block+'.pkl'\n",
    "test_file = s3.Object(bucket_name=bucket, key=test_key).get()\n",
    "test_object = pickle.loads(test_file['Body'].read())\n",
    "print('Example of {} saved for block {}:'.format(measurement,rand_block))\n",
    "print(list(test_object.items())[:10])\n",
    "print('Total entries: {}'.format(len(list(test_object.items()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Baseline Growth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Baseline capacity growth\n",
    "snapshot_capgrowth=collection_measure(bucket,extract_keys,'capacity_growth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save to S3 succesful\n",
      "Example of capacity_growth saved for block 603179:\n",
      "[(5314, 0), (934, 0), (3023, 0), (3452, 0), (576, 0), (3436, -12000), (3310, 0), (2378, 0), (4223, 500000), (422, 0)]\n",
      "Total entries: 5338\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba8263bf8cc47e490cfc7d960ea1f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5338.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Growth calculated correctly for all nodes\n"
     ]
    }
   ],
   "source": [
    "# TEST all saved correctly\n",
    "responses=np.array([r for b,r in snapshot_capgrowth.items()])\n",
    "\n",
    "if np.amax(responses)==np.amin(responses) and len(responses) == len(extract_keys):\n",
    "    print('Save to S3 succesful')\n",
    "else:\n",
    "    print('Error saving to S3')\n",
    "    \n",
    "measurement='capacity_growth'    \n",
    "rand_block=str(random.choice(blocks))    \n",
    "test_key='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/raw_score/'+rand_block+'.pkl'\n",
    "test_file = s3.Object(bucket_name=bucket, key=test_key).get()\n",
    "test_object = pickle.loads(test_file['Body'].read())\n",
    "print('Example of {} saved for block {}:'.format(measurement,rand_block))\n",
    "print(list(test_object.items())[:10])\n",
    "print('Total entries: {}'.format(len(list(test_object.items()))))\n",
    "\n",
    "# Get test graph\n",
    "g_test_key='graph_snapshots/'+str(extraction_id)+'_connected/'+str(rand_block)+'.gpickle'\n",
    "response = s3.Object(bucket_name=bucket, key=g_test_key).get()\n",
    "g_test=pickle.loads(response['Body'].read())\n",
    "\n",
    "# Run test function on test graph\n",
    "test_growth(g_test,test_object,int(rand_block),channel_opens,channel_closures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Baseline Closeness rank**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate closeness rank before making block decisions\n",
    "snapshot_clorank=collection_measure(bucket,extract_keys,'closeness_approx_rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "074fb9ab82bc426797cf3345fc1270d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=36535.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate closeness rank after making block decisions\n",
    "snapshot_clorank=collection_measure(bucket,extract_keys,'closeness_approx_rank_post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample Blocks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3654\n"
     ]
    }
   ],
   "source": [
    "sample_keys=[extract_keys[10*i] for i in range(int(len(extract_keys)/10)+1)]\n",
    "print(len(sample_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['graph_snapshots/1587447789_connected/508400.gpickle',\n",
       " 'graph_snapshots/1587447789_connected/509496.gpickle',\n",
       " 'graph_snapshots/1587447789_connected/511852.gpickle',\n",
       " 'graph_snapshots/1587447789_connected/513758.gpickle',\n",
       " 'graph_snapshots/1587447789_connected/514060.gpickle',\n",
       " 'graph_snapshots/1587447789_connected/514345.gpickle',\n",
       " 'graph_snapshots/1587447789_connected/514412.gpickle',\n",
       " 'graph_snapshots/1587447789_connected/514703.gpickle',\n",
       " 'graph_snapshots/1587447789_connected/514850.gpickle',\n",
       " 'graph_snapshots/1587447789_connected/515162.gpickle']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_keys[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Average Shortest path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_avg_shortpath=collection_measure(bucket,sample_keys,'avg_short_path')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Robustness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_robustness=collection_measure(bucket,sample_keys,'robustness_eff_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparissons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Compare property changes for nodes in a collection of graphs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###NEW \n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "collection_compare\n",
    "    Iterates over blocks to calculate marginal change in metric for nodes that made decisions (opens/closures)\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "blocks : list\n",
    "    List of blocks extracted when reading graphs\n",
    "    \n",
    "    \n",
    "dec_dic: dic\n",
    "    Dictionary with channel decisions (open or closure) per block\n",
    "    \n",
    "    \n",
    "graph_snapshots: list\n",
    "    List of delayed dask objects each pointing to a graph snapshot to be loaded from S3\n",
    "\n",
    "snapshots_base: dic\n",
    "    Dictionary of dictionaries containing the base measurments per block per node\n",
    "\n",
    "measurement: string\n",
    "    Name of the type of measurment that will applied to the graph\n",
    "    \n",
    "type_dec: string\n",
    "    The type of decisions that will be analyzed 'opens' or 'closures'\n",
    "    \n",
    "\n",
    "Returns\n",
    "-------\n",
    "futures: list\n",
    "    List of tuples of the form (dic_node0,dic_node1) where dic_node0/1 is a dictionary containing the marginal changes for node0/1 \n",
    "    for every decision in a given block. The dictionaries are future dask objects that still need to be explicitly computed. \n",
    "\n",
    "\"\"\"\n",
    "def collection_compare(blocks,dec_dic,graph_keys,snapshots_base,measurement,type_dec):\n",
    "\n",
    "    futures_list=[] # list to populate with futures per block\n",
    "    \n",
    "    with tqdm(total=len(range(1,len(graph_keys)))) as pbar:\n",
    "        for i in range(1,len(graph_keys)): # iterate through blocks\n",
    "\n",
    "            # extract information from parameters and construct input tuple to delayed function\n",
    "            block=blocks[i]\n",
    "            block_prev=blocks[i-1]\n",
    "            block_dec=dec_dic[block]\n",
    "            key=graph_keys[i-1] # Pass previous graph\n",
    "            block_base=snapshots_base[block_prev]\n",
    "            block_res=snapshots_base[block]\n",
    "            input_tuple=(block,g,block_dec,block_base,measurement,type_dec,block_res)\n",
    "            \n",
    "            # submit to delayed function and append to list\n",
    "            output_tuple=dask.delayed(graph_compare)(input_tuple)\n",
    "            futures_list.append(output_tuple)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # persist to disk and return\n",
    "    futures = dask.persist(*futures_list)\n",
    "    return futures\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "collection_compare\n",
    "    Iterates over blocks to calculate marginal change in metric for nodes that made decisions (opens/closures)\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "blocks : list\n",
    "    List of blocks extracted when reading graphs\n",
    "    \n",
    "    \n",
    "dec_dic: dic\n",
    "    Dictionary with channel decisions (open or closure) per block\n",
    "    \n",
    "    \n",
    "graph_snapshots: list\n",
    "    List of delayed dask objects each pointing to a graph snapshot to be loaded from S3\n",
    "\n",
    "snapshots_base: dic\n",
    "    Dictionary of dictionaries containing the base measurments per block per node\n",
    "\n",
    "measurement: string\n",
    "    Name of the type of measurment that will applied to the graph\n",
    "    \n",
    "type_dec: string\n",
    "    The type of decisions that will be analyzed 'opens' or 'closures'\n",
    "    \n",
    "\n",
    "Returns\n",
    "-------\n",
    "futures: list\n",
    "    List of tuples of the form (dic_node0,dic_node1) where dic_node0/1 is a dictionary containing the marginal changes for node0/1 \n",
    "    for every decision in a given block. The dictionaries are future dask objects that still need to be explicitly computed. \n",
    "\n",
    "\n",
    "def collection_compare(blocks,dec_dic,graph_snapshots,snapshots_base,measurement,type_dec):\n",
    "\n",
    "    futures_list=[] # list to populate with futures per block\n",
    "    \n",
    "    with tqdm(total=len(range(1,len(graph_snapshots)))) as pbar:\n",
    "        for i in range(1,len(graph_snapshots)): # iterate through blocks\n",
    "\n",
    "            # extract information from parameters and construct input tuple to delayed function\n",
    "            block=blocks[i]\n",
    "            block_prev=blocks[i-1]\n",
    "            block_dec=dec_dic[block]\n",
    "            g=graph_snapshots[i-1] # Pass previous graph\n",
    "            block_base=snapshots_base[block_prev]\n",
    "            block_res=snapshots_base[block]\n",
    "            input_tuple=(block,g,block_dec,block_base,measurement,type_dec,block_res)\n",
    "            \n",
    "            # submit to delayed function and append to list\n",
    "            output_tuple=dask.delayed(graph_compare)(input_tuple)\n",
    "            futures_list.append(output_tuple)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # persist to disk and return\n",
    "    futures = dask.persist(*futures_list)\n",
    "    return futures\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Compare property changes for nodes in a graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "graph_compare\n",
    "    Calculates marginal change in metric for node0, node1 make decisions (open/close channels) in a single block\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "input_tuple : tuple\n",
    "    \n",
    "    block: int\n",
    "        Block number\n",
    "    g: nx_graph \n",
    "        Graph snapshot (as dask delayed object)\n",
    "    block_dec: list\n",
    "        List of tuples in nx edge format (u,v,att_dic) for all the decisions (channel opens/closures) made in that block  \n",
    "    block_base: dic\n",
    "        Dictionary of base measurements for each node in the graph snapshot corresponding to that block\n",
    "    measurement: string\n",
    "        Name of measurement to be computed\n",
    "    type_dec: string\n",
    "    The type of decisions that will be analyzed 'opens' or 'closures'\n",
    "    \n",
    "    block_res: dic\n",
    "        Dictionary of base measurements for each node in the graph snapshot corresponding to the next block\n",
    "    \n",
    "\n",
    "Returns\n",
    "-------\n",
    "nodes_mar_dic: tuple\n",
    "    Tuples of the form (mar_node0_dic_i,mar_node0_dic_i) where each element in the tuple is a dictionary containing the marginal changes for node0/1 \n",
    "    for every node0 and node1 involved in a decision (channel open/closures) in the block.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def graph_compare(input_tuple):\n",
    "    \n",
    "    block=input_tuple[0]\n",
    "    key=input_tuple[1]\n",
    "    block_dec=input_tuple[2]\n",
    "    block_base=input_tuple[3]\n",
    "    measurement=input_tuple[4]\n",
    "    type_dec=input_tuple[5]\n",
    "    block_res=input_tuple[6]\n",
    "   \n",
    "    mar_node0_dic_i={} # dictionary to story function output\n",
    "    mar_node1_dic_i={} \n",
    "    \n",
    "    \n",
    "    # Load data\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.resource('s3')\n",
    "    response = s3.Object(bucket_name=bucket, key=key).get()\n",
    "    G=pickle.loads(response['Body'].read())\n",
    "    \n",
    "    ###########################---------------------\n",
    "    ##if decisiono \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # For each decision calculate marginal change in measure for node0 and node1\n",
    "    for edge in block_dec:\n",
    "        \n",
    "        # Extract info about channel\n",
    "        \n",
    "        node0=edge[0]\n",
    "        node1=edge[1]\n",
    "        channel_id=edge[2]['channel_id']\n",
    "        capacity=edge[2]['capacity']\n",
    "\n",
    "        \n",
    "        #Â Copy original graph\n",
    "        g_mar=G.copy()   \n",
    "        old_nodes=False\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Retrieve base measurement before channel if nodes existed, else define base measure as 0\n",
    "        if (g_mar.has_node(node0)):\n",
    "            node0_base=block_base[node0]\n",
    "            old_nodes=True\n",
    "        else:\n",
    "            node0_base=0\n",
    "            \n",
    "        if (g_mar.has_node(node1)):\n",
    "            node1_base=block_base[node1]\n",
    "            old_nodes=True\n",
    "        else:\n",
    "            node1_base=0\n",
    "        \n",
    "            \n",
    "        if old_nodes: # If at least one node is old (part of the connected graph)\n",
    "            \n",
    "            if type_dec=='mar_opens': # marginal calculation for opens\n",
    "                \n",
    "                \n",
    "                # Define and add edges and calculate betweeness if at least one of the nodes is in graph \n",
    "                edge_list=[edge]\n",
    "                \n",
    "                \n",
    "                # If channel exists increase capacity\n",
    "                \n",
    "                if g_mar.has_edge(node0,node1):\n",
    "                   \n",
    "                    g_mar.edges[node0,node1]['capacity']+=capacity\n",
    "                    g_mar.edges[node0,node1]['no_channels']+=1\n",
    "\n",
    "                else:\n",
    "                    g_mar.add_edges_from(edge_list)\n",
    "                \n",
    "                \n",
    "                g_mar_mes=node_measurement(g_mar,measurement,node0,node1)\n",
    "                \n",
    "                # Update measurement values after marginal change\n",
    "                node0_new_mes=g_mar_mes[0]\n",
    "                node1_new_mes=g_mar_mes[1]\n",
    "            \n",
    "            elif type_dec=='mar_closures': # marginal calculation for closes\n",
    "                \n",
    "                # Define and remove edges, define new connected graph and calculate betweeness \n",
    "                edge_list=[(node0,node1)]\n",
    "                \n",
    "                \n",
    "                # If channel exists decrease capacity\n",
    "                if g_mar.edges[node0,node1]['no_channels']>1:\n",
    "                    g_mar.edges[node0,node1]['capacity']-=capacity\n",
    "                    g_mar.edges[node0,node1]['no_channels']-=1\n",
    "                \n",
    "                else: \n",
    "                    g_mar.remove_edges_from(edge_list) \n",
    "                    connected_components=[c for c in nx.algorithms.components.connected_components(g_mar)]\n",
    "                    g_mar=g_mar.subgraph(connected_components[0]).copy()\n",
    "                    \n",
    "                g_mar_mes=node_measurment(g_mar,measurement,node0,node1)\n",
    "                node0_new_mes=g_mar_mes[0]\n",
    "                node1_new_mes=g_mar_mes[1]\n",
    "                \n",
    "            elif type_dec=='actual': # actual calculation for both opens and closures\n",
    "                \n",
    "                # Check individualy if in the graph for the resulting block the node is present (in the connected component, \n",
    "                # if not assign measurment to 0. \n",
    "                \n",
    "                try:\n",
    "                    node0_new_mes=block_res[node0]\n",
    "\n",
    "                except KeyError:\n",
    "                    node0_new_mes=0\n",
    "\n",
    "                try:\n",
    "                    node1_new_mes=block_res[node1]\n",
    "\n",
    "                except KeyError:\n",
    "                    node1_new_mes=0\n",
    "\n",
    "                \n",
    "                   \n",
    "            node0_mar=(node0_new_mes-node0_base)\n",
    "            node1_mar=(node1_new_mes-node1_base) \n",
    "        \n",
    "        \n",
    "        else: # If both nodes are new (outside of connected graph) their marginal decision outcome is 0\n",
    "            node0_mar=0\n",
    "            node1_mar=0\n",
    "\n",
    "        \n",
    "        # Update dictionary - new betweenness\n",
    "        mar_node0_dic_i[channel_id]=node0_mar\n",
    "        mar_node1_dic_i[channel_id]=node1_mar\n",
    "        \n",
    "    \n",
    "    return (mar_node0_dic_i,mar_node1_dic_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MARGINAL - CURRENT BETWEENESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Marginal current betweeness for opens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute marginal betweeness for channel openings\n",
    "\n",
    "futures_bet_maropen=collection_compare(blocks,channel_opens,graph_snapshots,snapshot_bet,measurement='current_betweeness',type_dec='mar_opens')\n",
    "start=time.time()\n",
    "bet_maropen_diclist = dask.compute(*futures_bet_maropen)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(bet_maropen_diclist)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to marginal current betweeness for opens into decisions Dataframe\n",
    "\n",
    "bet_maropen_channels=add_columns(bet_maropen_diclist,decisions_df,'bet_maropen_node0','bet_maropen_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with marginal current betweeness for opens: {}'.format(len(bet_maropen_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(bet_maropen_channels)].head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Marginal current betweeness for closures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute marginal betweeness for channel closures\n",
    "\n",
    "futures_bet_marclose=collection_compare(blocks,channel_closures,graph_snapshots,snapshot_bet,measurement='current_betweeness',type_dec='mar_closures')\n",
    "start=time.time()\n",
    "bet_marclose_diclist = dask.compute(*futures_bet_marclose)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(bet_marclose_diclist)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to marginal current betweeness for closures into decisions Dataframe\n",
    "\n",
    "bet_marclose_channels=add_columns(bet_marclose_diclist,decisions_df,'bet_marclose_node0','bet_marclose_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with marginal current betweeness for closures: {}'.format(len(bet_marclose_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(bet_marclose_channels)].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MARGINAL - CURRENT CLOSENESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Marginal current closeness for opens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute marginal betweeness for channel openings\n",
    "\n",
    "futures_clo_maropen=collection_compare(blocks,channel_opens,graph_snapshots,snapshot_clo,measurement='current_closeness',type_dec='mar_opens')\n",
    "start=time.time()\n",
    "clo_maropen_diclist = dask.compute(*futures_clo_maropen)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(clo_maropen_diclist)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to marginal current betweeness for opens into decisions Dataframe\n",
    "\n",
    "clo_maropen_channels=add_columns(clo_maropen_diclist,decisions_df,'clo_maropen_node0','clo_maropen_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with marginal current closeness for opens: {}'.format(len(clo_maropen_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(clo_maropen_channels)].head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Marginal current closeness for closures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute marginal current closeness for channel closures\n",
    "\n",
    "futures_clo_marclose=collection_compare(blocks,channel_closures,graph_snapshots,snapshot_clo,measurement='current_closeness',type_dec='mar_closures')\n",
    "start=time.time()\n",
    "clo_marclose_diclist = dask.compute(*futures_clo_marclose)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(clo_marclose_diclist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to marginal current betweeness for opens into decisions Dataframe\n",
    "\n",
    "clo_marclose_channels=add_columns(clo_marclose_diclist,decisions_df,'clo_marclose_node0','clo_marclose_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with marginal current closeness for closures: {}'.format(len(clo_marclose_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(clo_marclose_channels)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACTUAL - CURRENT BETWEENESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Actual current betweeness for opens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute actual current betweeness for channel opens\n",
    "\n",
    "futures_bet_actopen=collection_compare(blocks,channel_opens,graph_snapshots,snapshot_bet,measurement='current_betweeness',type_dec='actual')\n",
    "start=time.time()\n",
    "bet_actopen_diclist = dask.compute(*futures_bet_actopen)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(bet_actopen_diclist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to marginal current betweeness for opens into decisions Dataframe\n",
    "\n",
    "bet_actopen_channels=add_columns(bet_actopen_diclist,decisions_df,'bet_actopen_node0','bet_actopen_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with actual current betweeness for opens: {}'.format(len(bet_actopen_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(bet_actopen_channels)].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Actual current betweeness for closures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute actual current betweeness for channel closures\n",
    "\n",
    "futures_bet_actclose=collection_compare(blocks,channel_closures,graph_snapshots,snapshot_bet,measurement='current_betweeness',type_dec='actual')\n",
    "start=time.time()\n",
    "bet_actclose_diclist = dask.compute(*futures_bet_actclose)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(bet_actclose_diclist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to marginal current betweeness for opens into decisions Dataframe\n",
    "\n",
    "bet_actclose_channels=add_columns(bet_actclose_diclist,decisions_df,'bet_actclose_node0','bet_actclose_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with actual current betweeness for closures: {}'.format(len(bet_actclose_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(bet_actclose_channels)].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACTUAL - CURRENT CLOSENESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Actual current closeness for opens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute actual current closeness for channel opens\n",
    "\n",
    "futures_clo_actopen=collection_compare(blocks,channel_opens,graph_snapshots,snapshot_bet,measurement='current_closeness',type_dec='actual')\n",
    "start=time.time()\n",
    "clo_actopen_diclist = dask.compute(*futures_clo_actopen)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(bet_actopen_diclist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to marginal current closeness for opens into decisions Dataframe\n",
    "\n",
    "clo_actopen_channels=add_columns(clo_actopen_diclist,decisions_df,'clo_actopen_node0','clo_actopen_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with actual current closeness for opens: {}'.format(len(clo_actopen_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(clo_actopen_channels)].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Actual current closeness for closures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute actual current closeness for channel closures\n",
    "\n",
    "futures_clo_actclose=collection_compare(blocks,channel_closures,graph_snapshots,snapshot_bet,measurement='current_closeness',type_dec='actual')\n",
    "start=time.time()\n",
    "clo_actclose_diclist = dask.compute(*futures_clo_actclose)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(clo_actclose_diclist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to marginal current closeness for opens into decisions Dataframe\n",
    "\n",
    "clo_actclose_channels=add_columns(clo_actclose_diclist,decisions_df,'clo_actclose_node0','clo_actclose_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with actual current closeness for closures: {}'.format(len(clo_actclose_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(clo_actclose_channels)].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise stability \n",
    "\n",
    "- **Marginal betweenness (bet_mar_nodei)**: The % change between the betweenness centrality, for the node under analysis, given the graph from the previous block and the betweenness centrality of the resulting graph after enacting the decission (adding or removing a channel). Weighted current betweenness centrality is used for this measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Marginal betweenness for opens** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Why is length of Dataframe longer than the number of snapshots extracted? Could it be that some channels appear more than once in dataframe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Marginal betweenness for closures** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Actual change in betweenness (bet_act_nodei)**: The % change between the betweenness centrality, for the node under analysis, given the graph from the previous block and the betweenness centrality of the resulting graph after enacting **all** the decissions (adding or removing a channels) in the current block. Weighted current betweenness centrality is used for this measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Marginal betweeness pairwise stability (bet_mar_pairst/open/close)**: Evaluates if given the marginal graph that results from just enacting this decission is consistent with pairwise stability, from a betweenness perspective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MARGINAL - Add column with check for pairwise stability compatability using marginal outcomes\n",
    "\n",
    "# OPEN - Channel is opened if both nodes gain\n",
    "\n",
    "# Define function \n",
    "def bet_pairst_maropen(row):\n",
    "    if not math.isnan(row['bet_mar_node0']):\n",
    "        pairst=(row['bet_maropen_node0']>=0 and row['bet_maropen_node1']>=0)\n",
    "    else:\n",
    "        pairst=row['bet_maropen_node0']\n",
    "    return pairst\n",
    "\n",
    "# Apply function\n",
    "decisions_df['bet_mar_pairstopen']=decisions_df.apply(bet_pairst_maropen,axis=1)\n",
    "\n",
    "# CLOSE - Channel is closed if at least one node is better off\n",
    "\n",
    "# Define function \n",
    "def bet_pairst_marclose(row):\n",
    "    if not math.isnan(row['bet_marclose_node0']):\n",
    "        pairst=(row['bet_marclose_node0']>0 or row['bet_marclose_node1']>0)\n",
    "    else:\n",
    "        pairst=row['bet_marclose_node0']\n",
    "    return pairst\n",
    "\n",
    "# Apply function\n",
    "decisions_df['bet_mar_pairstclose']=decisions_df.apply(bet_pairst_marclose,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MARGINAL OPEN\n",
    "decisions_df[decisions_df['bet_mar_node0'].notnull()][['bet_mar_node0','bet_mar_node1','bet_mar_pairstopen']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MARGINAL CLOSE\n",
    "decisions_df[decisions_df['bet_marclose_node0'].notnull()][['bet_marclose_node0','bet_marclose_node1','bet_mar_pairstclose']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Actual betweeness pairwise stability (bet_act_pairstopen/close)**: Evaluates if given the marginal graph that results from all the decisions in the block is consitend with pairwise stability, from a betweenness perspective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTUAL - Add column with check for pairwise stability compatability using marginal outcomes\n",
    "\n",
    "# OPEN - Channel is opened if both nodes gain\n",
    "\n",
    "# Define function \n",
    "def bet_pairst_actopen(row):\n",
    "    if not math.isnan(row['bet_actopen_node0']):\n",
    "        pairst=(row['bet_actopen_node0']>=0 and row['bet_actopen_node1']>=0)\n",
    "    else:\n",
    "        pairst=row['bet_actopen_node0']\n",
    "    return pairst\n",
    "\n",
    "# Apply function\n",
    "decisions_df['bet_act_pairstopen']=decisions_df.apply(bet_pairst_actopen,axis=1)\n",
    "\n",
    "# CLOSE - Channel is closed if at least one node is better off\n",
    "\n",
    "# Define function \n",
    "def bet_pairst_actclose(row):\n",
    "    if not math.isnan(row['bet_actclose_node0']):\n",
    "        pairst=(row['bet_actclose_node0']>0 or row['bet_actclose_node1']>0)\n",
    "    else:\n",
    "        pairst=row['bet_actclose_node0']\n",
    "    return pairst\n",
    "\n",
    "# Apply function\n",
    "decisions_df['bet_act_pairstclose']=decisions_df.apply(bet_pairst_actclose,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ACTUAL OPEN\n",
    "decisions_df[decisions_df['bet_actopen_node0'].notnull()][['bet_actopen_node0','bet_actopen_node1','bet_act_pairstopen']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ACTUAL CLOSE\n",
    "decisions_df[decisions_df['bet_actclose_node0'].notnull()][['bet_actclose_node0','bet_actclose_node1','bet_act_pairstclose']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Updated DataFrame to S3\n",
    "\n",
    "# Create S3 resource and define values\n",
    "session = boto3.session.Session()\n",
    "s3 = session.resource('s3')\n",
    "csv_buffer = io.StringIO()\n",
    "\n",
    "# File path and name ([extraction_id]snapshot_bet-[no_blocks]-[start_block]-[end_block])\n",
    "key_decisions_df='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+str(extraction_id)+'decisions_df_bet-'+str(no_blocks)+'-'+str(start_block)+'-'+str(end_block)+'.csv'\n",
    "\n",
    "\n",
    "# Safe DataFrame\n",
    "decisions_df.to_csv(csv_buffer)\n",
    "s3.Object(bucket, key_decisions_df).put(Body=csv_buffer.getvalue())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Save\n",
    "decisions_df_test_load = s3.Object(bucket_name=bucket, key=key_decisions_df).get()\n",
    "decisions_df_test=pd.read_csv(io.BytesIO(decisions_df_test_load['Body'].read()),index_col=0)\n",
    "decisions_df_test==decisions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions_df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency\n",
    "- **Average betweeness per block (bet_effic)**: Average betweenness centrality for all the nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nash stability \n",
    "\n",
    "- **% Change with respect to not making decision (bet_binstat_deltai)**: The % change in betwewnness centrality, for the node under analysis, given the resulting graph after all of the decissions have been executed. \n",
    "- **Nash compatible - binary strategy (bet_binstat_nash)**: Returns true if given the other decissions enacted in the block not making decision would have NOT have resulted in higher betweenness centrality. This tells me if my strategy helped me be better off (took into account what others were doing)\n",
    "\n",
    "(Optional approaches - Check for tracktability)\n",
    "- **Nash compatible - close only strategy (bet_closestat_nash)**: Returns true if given the other decissions enacted in the block, closing any other channels would NOT have not resulted in higher betwneenness centrality. (NOTE: Check if there are combinatorial considerations, if so just look at closings up to x) \n",
    "- **Nash compatible - close/open (bet_allstat_nash)**: Returns true if given the other decissions enacted in the block, closing any other channels (with any node) or opening a channel with one of the round participants would NOT have not resulted in lower betwneenness centrality. (NOTE: To make it reasonable and constraint the strategy space only consider 'similar nodes' or with relationships in the past?).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Connectivity\n",
    "\n",
    "### Pairwise stability \n",
    "\n",
    "- **Marginal % change in connectivity (con_mar_deltai)**: The % change between the shortest path average, for the node under analysis, given the graph from the previous block and the shortest path average of the resulting graph after enacting the decission (adding or removing a channel). Weighted shortest path (_single_source_dijkstra_path_) is used for this measure.\n",
    "\n",
    "- **Actual % change in connectivity (con_act_deltai)**: The % change between the shortest path average, for the node under analysis, given the graph from the previous block and the shortest path average of the resulting graph after enacting **all** the decissions (adding or removing a channels) in the current block. Weighted shortest path (_single_source_dijkstra_path_) is used for this measure.\n",
    "\n",
    "- **Marginal connectivity pairwise stability (con_mar_pairstab)**: Evaluates if given the marginal graph that results from just enacting this decission is consistent with pairwise stability, from a connectivity perspective.\n",
    "\n",
    "- **Actual connectivity pairwise stability (con_act_pairstab)**: Evaluates if given the marginal graph that results from all the decisions in the block is consitend with pairwise stability, from a connectivity perspective.  \n",
    "\n",
    "\n",
    "\n",
    "### Nash stability \n",
    "\n",
    "- **% Change with respect to not making decision (con_binstat_deltai)**: The % change in shortest path average, for the node under analysis, given the resulting graph after all of the decissions have been executed. \n",
    "- **Nash compatible - binary strategy (con_binstat_nash)**: Returns true if given the other decissions enacted in the block not making decision would have NOT have resulted in higher shortest path average. NOTE: This indicates if the strategy selected made the node better off (took into account what others were doing)\n",
    "\n",
    "(Optional approaches - Check for tracktability)\n",
    "- **Nash compatible - close only strategy (con_closestat_nash)**: Returns true if given the other decissions enacted in the block, closing any other channels would NOT have not resulted in higher shortest path average. (NOTE: Check if there are combinatorial considerations, if so just look at closings up to x) \n",
    "- **Nash compatible - close/open (con_allstat_nash)**: Returns true if given the other decissions enacted in the block, closing any other channels (with any node) or opening a channel with one of the round participants would NOT have not resulted in lower shortest path average. (NOTE: To make it reasonable and constraint the strategy space only consider 'similar nodes' or with relationships in the past?).\n",
    "\n",
    "\n",
    "\n",
    "### Efficiency\n",
    "- **Average betweeness per block (bet_effic)**: Average shortest path average for all the nodes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
