{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LN - Data PP - Stability and efficiency calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import itertools\n",
    "#import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import io\n",
    "import random\n",
    "from itertools import islice\n",
    "import math\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "#from tqdm.notebook import trange\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "from time import sleep\n",
    "\n",
    "from dask_cloudprovider import FargateCluster\n",
    "from dask.distributed import Client\n",
    "import dask.array as da\n",
    "import dask\n",
    "dask.config.set({'distributed.scheduler.allowed-failures': 50}) \n",
    "\n",
    "\n",
    "import boto3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "\n",
    "bucket='ln-strategy-data'\n",
    "extraction_id=1587447789\n",
    "#extraction_id=1585344554"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection to AWS Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate s3 resource\n",
    "\n",
    "session = boto3.session.Session()\n",
    "s3 = session.resource('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fargate Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster = FargateCluster(n_workers=100,scheduler_timeout='20 minutes',image='dsrincon/dask-graph:nx-scipy-v1',scheduler_cpu=4096,scheduler_mem=16384,worker_mem=32768)\n",
    "cluster = FargateCluster(n_workers=20,scheduler_timeout='10 minutes',image='dsrincon/dask-graph:nx-scipy-v1',scheduler_cpu=4096,scheduler_mem=16384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "958b369f3b9445edb3d408cdeecb7104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>FargateCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n  â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/distributed/client.py:1079: VersionMismatchWarning: Mismatched versions found\n",
      "\n",
      "python\n",
      "+---------------------------+---------------+\n",
      "|                           | version       |\n",
      "+---------------------------+---------------+\n",
      "| client                    | 3.7.3.final.0 |\n",
      "| scheduler                 | 3.7.4.final.0 |\n",
      "| tcp://172.31.10.138:46825 | 3.7.4.final.0 |\n",
      "| tcp://172.31.11.169:42625 | 3.7.4.final.0 |\n",
      "| tcp://172.31.19.210:41527 | 3.7.4.final.0 |\n",
      "| tcp://172.31.2.56:33483   | 3.7.4.final.0 |\n",
      "| tcp://172.31.20.202:34557 | 3.7.4.final.0 |\n",
      "| tcp://172.31.21.138:37815 | 3.7.4.final.0 |\n",
      "| tcp://172.31.22.25:38343  | 3.7.4.final.0 |\n",
      "| tcp://172.31.26.50:41149  | 3.7.4.final.0 |\n",
      "| tcp://172.31.27.64:37437  | 3.7.4.final.0 |\n",
      "| tcp://172.31.29.168:42035 | 3.7.4.final.0 |\n",
      "| tcp://172.31.29.26:37969  | 3.7.4.final.0 |\n",
      "| tcp://172.31.29.41:44035  | 3.7.4.final.0 |\n",
      "| tcp://172.31.30.139:36575 | 3.7.4.final.0 |\n",
      "| tcp://172.31.32.196:40005 | 3.7.4.final.0 |\n",
      "| tcp://172.31.32.50:46365  | 3.7.4.final.0 |\n",
      "| tcp://172.31.33.2:45037   | 3.7.4.final.0 |\n",
      "| tcp://172.31.37.153:46491 | 3.7.4.final.0 |\n",
      "| tcp://172.31.39.211:38567 | 3.7.4.final.0 |\n",
      "| tcp://172.31.39.216:33667 | 3.7.4.final.0 |\n",
      "| tcp://172.31.42.156:41737 | 3.7.4.final.0 |\n",
      "| tcp://172.31.43.246:45123 | 3.7.4.final.0 |\n",
      "| tcp://172.31.44.110:40453 | 3.7.4.final.0 |\n",
      "| tcp://172.31.46.152:42649 | 3.7.4.final.0 |\n",
      "| tcp://172.31.48.168:36045 | 3.7.4.final.0 |\n",
      "| tcp://172.31.51.59:43903  | 3.7.4.final.0 |\n",
      "| tcp://172.31.54.245:43527 | 3.7.4.final.0 |\n",
      "| tcp://172.31.54.39:35449  | 3.7.4.final.0 |\n",
      "| tcp://172.31.55.130:39665 | 3.7.4.final.0 |\n",
      "| tcp://172.31.55.190:46667 | 3.7.4.final.0 |\n",
      "| tcp://172.31.57.14:45753  | 3.7.4.final.0 |\n",
      "| tcp://172.31.6.15:41805   | 3.7.4.final.0 |\n",
      "| tcp://172.31.6.2:43193    | 3.7.4.final.0 |\n",
      "| tcp://172.31.6.74:38641   | 3.7.4.final.0 |\n",
      "| tcp://172.31.62.243:39301 | 3.7.4.final.0 |\n",
      "| tcp://172.31.63.147:35601 | 3.7.4.final.0 |\n",
      "| tcp://172.31.63.253:34911 | 3.7.4.final.0 |\n",
      "| tcp://172.31.64.190:37619 | 3.7.4.final.0 |\n",
      "| tcp://172.31.65.154:37065 | 3.7.4.final.0 |\n",
      "| tcp://172.31.68.102:40449 | 3.7.4.final.0 |\n",
      "| tcp://172.31.69.125:42311 | 3.7.4.final.0 |\n",
      "| tcp://172.31.69.177:40741 | 3.7.4.final.0 |\n",
      "| tcp://172.31.7.73:37919   | 3.7.4.final.0 |\n",
      "| tcp://172.31.71.63:45235  | 3.7.4.final.0 |\n",
      "| tcp://172.31.71.80:37313  | 3.7.4.final.0 |\n",
      "| tcp://172.31.74.115:44043 | 3.7.4.final.0 |\n",
      "| tcp://172.31.75.78:42673  | 3.7.4.final.0 |\n",
      "| tcp://172.31.76.56:37845  | 3.7.4.final.0 |\n",
      "| tcp://172.31.77.253:40197 | 3.7.4.final.0 |\n",
      "| tcp://172.31.80.65:41545  | 3.7.4.final.0 |\n",
      "| tcp://172.31.82.10:40047  | 3.7.4.final.0 |\n",
      "| tcp://172.31.82.132:41415 | 3.7.4.final.0 |\n",
      "| tcp://172.31.83.182:37161 | 3.7.4.final.0 |\n",
      "| tcp://172.31.84.177:39739 | 3.7.4.final.0 |\n",
      "| tcp://172.31.84.96:42457  | 3.7.4.final.0 |\n",
      "| tcp://172.31.87.186:34607 | 3.7.4.final.0 |\n",
      "| tcp://172.31.9.236:33829  | 3.7.4.final.0 |\n",
      "| tcp://172.31.9.76:35817   | 3.7.4.final.0 |\n",
      "| tcp://172.31.90.166:46371 | 3.7.4.final.0 |\n",
      "| tcp://172.31.92.179:35827 | 3.7.4.final.0 |\n",
      "| tcp://172.31.92.6:37193   | 3.7.4.final.0 |\n",
      "+---------------------------+---------------+\n",
      "\n",
      "tornado\n",
      "+---------------------------+---------+\n",
      "|                           | version |\n",
      "+---------------------------+---------+\n",
      "| client                    | 6.0.3   |\n",
      "| scheduler                 | 6.0.4   |\n",
      "| tcp://172.31.10.138:46825 | 6.0.4   |\n",
      "| tcp://172.31.11.169:42625 | 6.0.4   |\n",
      "| tcp://172.31.19.210:41527 | 6.0.4   |\n",
      "| tcp://172.31.2.56:33483   | 6.0.4   |\n",
      "| tcp://172.31.20.202:34557 | 6.0.4   |\n",
      "| tcp://172.31.21.138:37815 | 6.0.4   |\n",
      "| tcp://172.31.22.25:38343  | 6.0.4   |\n",
      "| tcp://172.31.26.50:41149  | 6.0.4   |\n",
      "| tcp://172.31.27.64:37437  | 6.0.4   |\n",
      "| tcp://172.31.29.168:42035 | 6.0.4   |\n",
      "| tcp://172.31.29.26:37969  | 6.0.4   |\n",
      "| tcp://172.31.29.41:44035  | 6.0.4   |\n",
      "| tcp://172.31.30.139:36575 | 6.0.4   |\n",
      "| tcp://172.31.32.196:40005 | 6.0.4   |\n",
      "| tcp://172.31.32.50:46365  | 6.0.4   |\n",
      "| tcp://172.31.33.2:45037   | 6.0.4   |\n",
      "| tcp://172.31.37.153:46491 | 6.0.4   |\n",
      "| tcp://172.31.39.211:38567 | 6.0.4   |\n",
      "| tcp://172.31.39.216:33667 | 6.0.4   |\n",
      "| tcp://172.31.42.156:41737 | 6.0.4   |\n",
      "| tcp://172.31.43.246:45123 | 6.0.4   |\n",
      "| tcp://172.31.44.110:40453 | 6.0.4   |\n",
      "| tcp://172.31.46.152:42649 | 6.0.4   |\n",
      "| tcp://172.31.48.168:36045 | 6.0.4   |\n",
      "| tcp://172.31.51.59:43903  | 6.0.4   |\n",
      "| tcp://172.31.54.245:43527 | 6.0.4   |\n",
      "| tcp://172.31.54.39:35449  | 6.0.4   |\n",
      "| tcp://172.31.55.130:39665 | 6.0.4   |\n",
      "| tcp://172.31.55.190:46667 | 6.0.4   |\n",
      "| tcp://172.31.57.14:45753  | 6.0.4   |\n",
      "| tcp://172.31.6.15:41805   | 6.0.4   |\n",
      "| tcp://172.31.6.2:43193    | 6.0.4   |\n",
      "| tcp://172.31.6.74:38641   | 6.0.4   |\n",
      "| tcp://172.31.62.243:39301 | 6.0.4   |\n",
      "| tcp://172.31.63.147:35601 | 6.0.4   |\n",
      "| tcp://172.31.63.253:34911 | 6.0.4   |\n",
      "| tcp://172.31.64.190:37619 | 6.0.4   |\n",
      "| tcp://172.31.65.154:37065 | 6.0.4   |\n",
      "| tcp://172.31.68.102:40449 | 6.0.4   |\n",
      "| tcp://172.31.69.125:42311 | 6.0.4   |\n",
      "| tcp://172.31.69.177:40741 | 6.0.4   |\n",
      "| tcp://172.31.7.73:37919   | 6.0.4   |\n",
      "| tcp://172.31.71.63:45235  | 6.0.4   |\n",
      "| tcp://172.31.71.80:37313  | 6.0.4   |\n",
      "| tcp://172.31.74.115:44043 | 6.0.4   |\n",
      "| tcp://172.31.75.78:42673  | 6.0.4   |\n",
      "| tcp://172.31.76.56:37845  | 6.0.4   |\n",
      "| tcp://172.31.77.253:40197 | 6.0.4   |\n",
      "| tcp://172.31.80.65:41545  | 6.0.4   |\n",
      "| tcp://172.31.82.10:40047  | 6.0.4   |\n",
      "| tcp://172.31.82.132:41415 | 6.0.4   |\n",
      "| tcp://172.31.83.182:37161 | 6.0.4   |\n",
      "| tcp://172.31.84.177:39739 | 6.0.4   |\n",
      "| tcp://172.31.84.96:42457  | 6.0.4   |\n",
      "| tcp://172.31.87.186:34607 | 6.0.4   |\n",
      "| tcp://172.31.9.236:33829  | 6.0.4   |\n",
      "| tcp://172.31.9.76:35817   | 6.0.4   |\n",
      "| tcp://172.31.90.166:46371 | 6.0.4   |\n",
      "| tcp://172.31.92.179:35827 | 6.0.4   |\n",
      "| tcp://172.31.92.6:37193   | 6.0.4   |\n",
      "+---------------------------+---------+\n",
      "  warnings.warn(version_module.VersionMismatchWarning(msg[0][\"warning\"]))\n"
     ]
    }
   ],
   "source": [
    "client = Client(cluster)\n",
    "#cluster=Client('tcp://18.234.80.68:8786')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Write output to DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function write output to DataFrame\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "add_columns\n",
    "    Function that takes an output from a decision comparisson computation and adds it's results for nodes 1 and 0 in the main DataFrame\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "output_diclist : list\n",
    "    Dictionary of the form (node0_dic_i,node1_dic_i) where i runs for all of the blocks being compared. \n",
    "\n",
    "original_df: Pandas DataFrame\n",
    "    Original DataFrame containing the opening and closure information for each channel, with a column named 'short_channel_id' to denote \n",
    "    id of channel. \n",
    "\n",
    "column_name_node0: string\n",
    "    Name for column in dataframe where the results will be stored for node 0\n",
    "    \n",
    "column_name_node1: string\n",
    "    Name for column in dataframe where the results will be stored for node 1\n",
    "\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "no_changes: list\n",
    "    List with the 'short_channel_id' of the channels edited. \n",
    " \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def add_columns(output_diclist,original_df,column_name_node0,column_name_node1):\n",
    "\n",
    "\n",
    "    # Merge individual dictionaries into one for each node\n",
    "    node0_dic={}\n",
    "    node1_dic={}\n",
    "    for dic_tuple in output_diclist:\n",
    "        node0_dic.update(dic_tuple[0])\n",
    "        node1_dic.update(dic_tuple[1])\n",
    "    \n",
    "    # Add to DataFrame\n",
    "\n",
    "    # Create empty columns\n",
    "    original_df[column_name_node0]=np.nan\n",
    "    original_df[column_name_node1]=np.nan\n",
    "\n",
    "    # Populate df with values\n",
    "    original_df[column_name_node0]=original_df['short_channel_id'].map(node0_dic)\n",
    "    original_df[column_name_node1]=original_df['short_channel_id'].map(node1_dic)\n",
    "    \n",
    "    # Calculate values changed\n",
    "    rows_edited=(original_df[original_df[column_name_node0].notnull()]['short_channel_id']).tolist()\n",
    "    \n",
    "    return rows_edited\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Save python object to S3 using pickle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to write pickle data to S3 bucket\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "pickle_save_s3\n",
    "    Saves Python object to S3 as pickle object\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "obj : <any>\n",
    "    Python Object\n",
    "\n",
    "blocks: list\n",
    "    List of extracted blocks\n",
    "\n",
    "extraction_id: int\n",
    "    Number of block extraction\n",
    "    \n",
    "name: string\n",
    "    Name of object to add to filename in S3\n",
    "\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "response: int\n",
    "    HTTP response code from S3 \n",
    " \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def pickle_save_s3(obj,blocks,extraction_id,name):\n",
    "\n",
    "\n",
    "    # Define number of blocks\n",
    "    start_block=np.min(np.array(blocks))\n",
    "    end_block=np.max(np.array(blocks))\n",
    "    no_blocks=len(blocks)\n",
    "\n",
    "    # Load S3 and bucket details\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.resource('s3')\n",
    "\n",
    "    # File path and name ([extraction_id][name]-[no_blocks]-[start_block]-[end_block])\n",
    "    key='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+str(extraction_id)+name+'-'+str(no_blocks)+'-'+str(start_block)+'-'+str(end_block)+'.pkl'\n",
    "\n",
    "    # Create pickle object and send to S3\n",
    "    pickle_byte_obj = pickle.dumps(obj) \n",
    "    response=s3.Object(bucket,key).put(Body=pickle_byte_obj)\n",
    "    \n",
    "    return response['ResponseMetadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to write pickle data to S3 bucket\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "simple_psave_s3\n",
    "    Saves Python object to S3 as pickle object\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "input_tuple: tuple\n",
    "    \n",
    "    bucket: str\n",
    "        S3 bucket to save\n",
    "    key: str\n",
    "        S3 key to save\n",
    "    obj: obj\n",
    "        python object to save\n",
    "\n",
    "Returns\n",
    "-------\n",
    "response: int\n",
    "    HTTP response code from S3 \n",
    " \n",
    "\"\"\"\n",
    "def simple_psave_s3(input_tuple):\n",
    "    \n",
    "    bucket=input_tuple[0]\n",
    "    key=input_tuple[1]\n",
    "    obj=input_tuple[2]\n",
    "    \n",
    "    # Start S3 session\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.resource('s3')\n",
    "    \n",
    "    # Save to S3\n",
    "    pickle_byte_obj = pickle.dumps(obj) \n",
    "    response=s3.Object(bucket,key).put(Body=pickle_byte_obj)['ResponseMetadata']['HTTPStatusCode']\n",
    "    \n",
    "    return response\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Load single graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "load_graph\n",
    "    Loads networkX (pickle serialized) object from S3\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "key : str\n",
    "    Path in S3 bucket for individual pickled serialized networkX graph object \n",
    "\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "response: networkX graph\n",
    "    Graph object\n",
    " \n",
    "\"\"\"\n",
    "\n",
    "def load_graph(key):\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.resource('s3')\n",
    "    response = s3.Object(bucket_name=bucket, key=key).get()\n",
    "    G=pickle.loads(response['Body'].read())\n",
    "    \n",
    "    return G\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Run graph measurement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "load_graph_measurement:\n",
    "    Runs graph measurement for every node in a specific block and loads the created dictionary from S3\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "\n",
    "extraction_id: int\n",
    "    Timestamp of block extraction\n",
    "\n",
    "test_ix: int\n",
    "    Index of block to test. Can be negative to move backwards in array\n",
    "\n",
    "measurement: str\n",
    "    Type of graph measurement to perform\n",
    "\n",
    "weight: str\n",
    "    Node attribute to use for weighted calculations\n",
    "\n",
    "blocks: list\n",
    "    List of (ints) blocks extracted \n",
    "\n",
    "graph_keys: list\n",
    "    List of (str) graph paths in S3\n",
    "\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "g_test: NetworkX graph\n",
    "    NetworkX graph extracted for the given ix\n",
    "\n",
    "nodes_test: list\n",
    "    List of nodes in g_test\n",
    "\n",
    "g_dic_test: dict\n",
    "    Dictionary of graph measurment for each node in g_test\n",
    "\n",
    "block: int\n",
    "    Block selected for test\n",
    " \n",
    "\"\"\"\n",
    "\n",
    "def load_graph_measurement(extraction_id,measurement,weight,blocks,graph_keys,test_ix=None):\n",
    "\n",
    "    if test_ix==None: # If no index provided choose one at random\n",
    "        test_ix=random.choice(range(len(blocks)))\n",
    "        \n",
    "        \n",
    "    \n",
    "    # Define block and graph key\n",
    "    test_block=blocks[test_ix]\n",
    "    g_key=graph_keys[test_ix]\n",
    "    print('Block selected:{}'.format(test_block))\n",
    "    \n",
    "    # Load graph and nodes\n",
    "    response = s3.Object(bucket_name=bucket, key=g_key).get()\n",
    "    g_test=pickle.loads(response['Body'].read())\n",
    "    nodes_test=list(g_test.nodes())\n",
    "\n",
    "    # Run function\n",
    "    block,response_test=graph_measurement((g_key,measurement,weight,bucket))\n",
    "\n",
    "\n",
    "    if response_test==200:\n",
    "        # Load created dictionary with calculation from S3\n",
    "        \n",
    "        key_test='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/raw_score/'+str(test_block)+'.pkl'\n",
    "        g__test_load = s3.Object(bucket_name=bucket, key=key_test).get()\n",
    "        g_dic_test = pickle.loads(g__test_load['Body'].read())\n",
    "        print('Dic was saved correctly. Sample below:')\n",
    "        print(list(g_dic_test.items())[:10])\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        print('Measurement was not saved correctly')\n",
    "        return\n",
    "    \n",
    "    return g_test,nodes_test,g_dic_test,test_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load objects form S3\n",
    "# Dataframe\n",
    "\n",
    "decisions_load = s3.Object(bucket_name=bucket, key='decisions_df.csv').get()\n",
    "decisions_df=pd.read_csv(io.BytesIO(decisions_load['Body'].read()))\n",
    "\n",
    "# Channel closures\n",
    "closure_file = s3.Object(bucket_name=bucket, key='channel_closures.p').get()\n",
    "channel_closures = pickle.loads(closure_file['Body'].read())\n",
    "    \n",
    "    \n",
    "# Channel openings \n",
    "opens_file = s3.Object(bucket_name=bucket, key='channel_opens.p').get()\n",
    "channel_opens = pickle.loads(opens_file['Body'].read())\n",
    "\n",
    "    \n",
    "\n",
    "# Create list with graph keys\n",
    "\n",
    "#TODO: Save graphs as numpy array in single H5 file to reduce. Test if creating graphs takes longer than reading from S3\n",
    "\n",
    "# graph_dir='./data/graph_snapshots' - For local tests\n",
    "\n",
    "\n",
    "graph_keys = [obj.key \n",
    "        for obj in s3.Bucket(name=bucket).objects.all()\n",
    "        if re.match(\".*\"+str(extraction_id)+\"_connected/.*\\.gpickle\",obj.key)]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36536"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Blocks to be extracted and define graph\n",
    "\n",
    "\n",
    "# Base lists to be populated\n",
    "graph_snapshots=[]\n",
    "blocks=[]\n",
    "base_ix=6\n",
    "\n",
    "\n",
    "extract_keys=graph_keys[base_ix:] # Blocks below 6th index are <3 and affect some graph metrics\n",
    "\n",
    "for key in extract_keys: # Change to [700:] for full range\n",
    "    \n",
    "    # Create block list from file_names\n",
    "    block_i=int(key.split(\".\")[0].split(\"/\")[-1]) \n",
    "    blocks.append(block_i)\n",
    "    \n",
    "    #Extract graphs - UNCOMMENT TO have them out of function\n",
    "    #G=dask.delayed(load_graph)(key)\n",
    "    #graph_snapshots.append(G)\n",
    "    \n",
    "   \n",
    "start_block=np.min(np.array(blocks))\n",
    "end_block=np.max(np.array(blocks))\n",
    "no_blocks=len(blocks)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Extract nodes and calculate age of nodes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29bb147ec1c34b8292dda6e8a059fd81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7735.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Details extracted and save to S3 succesful\n"
     ]
    }
   ],
   "source": [
    "# Define nodes and calculate details\n",
    "\n",
    "nodes=list(set(decisions_df['node0_id'].tolist()).union(set(decisions_df['node1_id'].tolist())))\n",
    "\n",
    "# DETAIL_1:Calculate birth block\n",
    "\n",
    "# Create list of sets (node pairs )\n",
    "opens_list=sorted(list(channel_opens.items()))\n",
    "open_list_sets=[(opens[0],[{t[0],t[1]} for t in opens[1]]) for opens in opens_list]\n",
    "\n",
    "# Dic to store details per node \n",
    "node_details={}\n",
    "\n",
    "\n",
    "with tqdm(total=len(nodes)) as pbar:\n",
    "    for node in nodes:\n",
    "\n",
    "        for opens in open_list_sets:\n",
    "            if opens[1] and node in set.union(*opens[1]):\n",
    "                birth_block=opens[0]\n",
    "                node_details[node]={'birth_block':birth_block}\n",
    "                break\n",
    "    \n",
    "        pbar.update(1)\n",
    "        \n",
    "\n",
    "# SAVE to S3\n",
    "\n",
    "key='node_details.p'\n",
    "pickle_byte_obj = pickle.dumps(node_details) \n",
    "response=s3.Object(bucket,key).put(Body=pickle_byte_obj)['ResponseMetadata']\n",
    "\n",
    "if response['HTTPStatusCode']==200:\n",
    "    print('Details extracted and save to S3 succesful')\n",
    "else: \n",
    "    print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details extracted and save to S3 succesful\n"
     ]
    }
   ],
   "source": [
    "# TESTS\n",
    "len_nodes=len(nodes)\n",
    "len_details=len(list(node_details.items()))\n",
    "\n",
    "\n",
    "if len_nodes==len_details and response['HTTPStatusCode']==200:\n",
    "    print('Details extracted and save to S3 succesful')\n",
    "else:\n",
    "    print('Details for {} nodes were not extracted'.format(len_nodes-len_details))   \n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total graph keys:36542\n",
      "Number of blocks to be processed:94\n",
      "---Sample graph keys---\n",
      "graph_snapshots/1587447789_connected/505149.gpickle\n",
      "---Sample channel opens---\n",
      "[(2378, 4223, {'capacity': 400000, 'open_fee': 4557, 'dec_id': 58766, 'channel_id': '508090x1515x1', 'no_channels': 0})]\n",
      "---Sample channel closures---\n",
      "[(2643, 6038, {'close_type': 'force', 'dec_id': 26620, 'channel_id': '570913x720x1', 'capacity': 300000}), (6038, 5314, {'close_type': 'mutual', 'dec_id': 0, 'channel_id': '505149x622x0', 'capacity': 300000})]\n"
     ]
    }
   ],
   "source": [
    "# Test: extracted formats\n",
    "print(\"Number of total graph keys:{}\".format(len(graph_keys)))\n",
    "print(\"Number of blocks to be processed:{}\".format(len(extract_keys)))\n",
    "print(\"---Sample graph keys---\")\n",
    "print(graph_keys[0])\n",
    "print(\"---Sample channel opens---\")\n",
    "print(channel_opens[508090])\n",
    "print(\"---Sample channel closures---\")\n",
    "print(channel_closures[592638])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of DataFrame in Memory:64821774\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>short_channel_id</th>\n",
       "      <th>open_block</th>\n",
       "      <th>open_transaction</th>\n",
       "      <th>address</th>\n",
       "      <th>close_block</th>\n",
       "      <th>close_transaction</th>\n",
       "      <th>node0</th>\n",
       "      <th>node1</th>\n",
       "      <th>satoshis</th>\n",
       "      <th>...</th>\n",
       "      <th>close_fee</th>\n",
       "      <th>last_update</th>\n",
       "      <th>close_type</th>\n",
       "      <th>close_htlc_count</th>\n",
       "      <th>close_balance_a</th>\n",
       "      <th>close_balance_b</th>\n",
       "      <th>dec_id</th>\n",
       "      <th>node0_id</th>\n",
       "      <th>node1_id</th>\n",
       "      <th>node_pair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70648</th>\n",
       "      <td>1027</td>\n",
       "      <td>535029x2012x1</td>\n",
       "      <td>535029</td>\n",
       "      <td>d01928d350e1ba04d7335a91e6dd54f5dbf94859e0c59b...</td>\n",
       "      <td>bc1qszamn0la3yqrqhjj8yepdxkl9qlr84zfwgg9zrkccl...</td>\n",
       "      <td>535029.0</td>\n",
       "      <td>d01928d350e1ba04d7335a91e6dd54f5dbf94859e0c59b...</td>\n",
       "      <td>022a7809052db05fde648391a53aba82286e4a517cff1d...</td>\n",
       "      <td>031b71cbad0cb4e22141e45f16c83c332f755e1ba68195...</td>\n",
       "      <td>462124</td>\n",
       "      <td>...</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unused</td>\n",
       "      <td>0.0</td>\n",
       "      <td>275630.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1027</td>\n",
       "      <td>4091</td>\n",
       "      <td>3578</td>\n",
       "      <td>14637598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70625</th>\n",
       "      <td>1045</td>\n",
       "      <td>535177x446x1</td>\n",
       "      <td>535177</td>\n",
       "      <td>7376d5bc0c18bbff8f644d0827e759a1518b38e1e95a08...</td>\n",
       "      <td>bc1qauzljedtlva73ngg7suqketlvn5gnnuemxpeuevcqt...</td>\n",
       "      <td>535177.0</td>\n",
       "      <td>7376d5bc0c18bbff8f644d0827e759a1518b38e1e95a08...</td>\n",
       "      <td>02272bd12e59324d0f2b231fb88f134b57eb26dd100d2c...</td>\n",
       "      <td>031b71cbad0cb4e22141e45f16c83c332f755e1ba68195...</td>\n",
       "      <td>257307</td>\n",
       "      <td>...</td>\n",
       "      <td>767.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unused</td>\n",
       "      <td>0.0</td>\n",
       "      <td>218405.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1045</td>\n",
       "      <td>3604</td>\n",
       "      <td>3578</td>\n",
       "      <td>12895112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68629</th>\n",
       "      <td>2745</td>\n",
       "      <td>549037x2738x0</td>\n",
       "      <td>549037</td>\n",
       "      <td>b7128bbbe422b4f18fad71b091eed1f9e4b0d231be8117...</td>\n",
       "      <td>bc1q95fytjzs8f7fma2nf66gcva7c3w7hnkdwrkef9pu33...</td>\n",
       "      <td>549037.0</td>\n",
       "      <td>b7128bbbe422b4f18fad71b091eed1f9e4b0d231be8117...</td>\n",
       "      <td>028b892b15f5cabcea5165b236db0e36dc06553c323c84...</td>\n",
       "      <td>038b36a43c38f75cd15bb25394f1cd162f717df0055852...</td>\n",
       "      <td>400000</td>\n",
       "      <td>...</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>1.547494e+09</td>\n",
       "      <td>unused</td>\n",
       "      <td>0.0</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2745</td>\n",
       "      <td>1781</td>\n",
       "      <td>4968</td>\n",
       "      <td>8848008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68636</th>\n",
       "      <td>2744</td>\n",
       "      <td>549037x2737x0</td>\n",
       "      <td>549037</td>\n",
       "      <td>0825da5e96cd45fced3233ebe615721b687285839d3036...</td>\n",
       "      <td>bc1q5mqzhw5e42rfqh250zalwu47ru8gvz4g4k968me0mg...</td>\n",
       "      <td>549037.0</td>\n",
       "      <td>0825da5e96cd45fced3233ebe615721b687285839d3036...</td>\n",
       "      <td>02b95713bbe4609a337f3ca5aab3a75674083ddf5331a4...</td>\n",
       "      <td>038b36a43c38f75cd15bb25394f1cd162f717df0055852...</td>\n",
       "      <td>400000</td>\n",
       "      <td>...</td>\n",
       "      <td>1992.0</td>\n",
       "      <td>1.547503e+09</td>\n",
       "      <td>unused</td>\n",
       "      <td>0.0</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2744</td>\n",
       "      <td>210</td>\n",
       "      <td>4968</td>\n",
       "      <td>1043280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68461</th>\n",
       "      <td>40227</td>\n",
       "      <td>549489x1194x1</td>\n",
       "      <td>549489</td>\n",
       "      <td>58dafe493648fbdd69143c26e0cf8a66ae11a272c2739d...</td>\n",
       "      <td>bc1q25j5l6crv4mrjkjjjw4rzyv890cwwnyyw9dezcqs5x...</td>\n",
       "      <td>549489.0</td>\n",
       "      <td>58dafe493648fbdd69143c26e0cf8a66ae11a272c2739d...</td>\n",
       "      <td>02574ffa55d394b9326f6e5c15992cc0516b0d6e6a79a1...</td>\n",
       "      <td>03a5927b64b1ea8657d5b770d61a3e2d0554fdb5d56877...</td>\n",
       "      <td>2500000</td>\n",
       "      <td>...</td>\n",
       "      <td>2363.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unused</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8974.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40227</td>\n",
       "      <td>7558</td>\n",
       "      <td>4495</td>\n",
       "      <td>33973210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59540</th>\n",
       "      <td>37095</td>\n",
       "      <td>562592x1695x1</td>\n",
       "      <td>562592</td>\n",
       "      <td>06b4d9b3cfa10bd2cd33131d034a6b38c1651eee49018a...</td>\n",
       "      <td>bc1q9jnkm78y45kyasnu43p52gc8sqazwy4yfjtqzkwqx5...</td>\n",
       "      <td>562592.0</td>\n",
       "      <td>06b4d9b3cfa10bd2cd33131d034a6b38c1651eee49018a...</td>\n",
       "      <td>029b71b8186914267ea59cb081c43ad1aeb874b5a185a4...</td>\n",
       "      <td>03864ef025fde8fb587d989186ce6a4a186895ee44a926...</td>\n",
       "      <td>5000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2136.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unused</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4995006.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37095</td>\n",
       "      <td>3210</td>\n",
       "      <td>7259</td>\n",
       "      <td>23301390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55969</th>\n",
       "      <td>37213</td>\n",
       "      <td>564495x455x1</td>\n",
       "      <td>564495</td>\n",
       "      <td>8a5764e1f0cb659b687a0675cd88983526bd9213665986...</td>\n",
       "      <td>bc1qqgcvu4nl3vjm4vvr9r5l2f4ufppph5fn070fzscq4r...</td>\n",
       "      <td>564510.0</td>\n",
       "      <td>655c3d44c09055e9af9f8d13d55c99979e0b0b306230b9...</td>\n",
       "      <td>024655b768ef40951b20053a5c4b951606d4d86085d512...</td>\n",
       "      <td>0375a154b8f94eb0556566d60d96acc47f99f2f0d74ef9...</td>\n",
       "      <td>400000</td>\n",
       "      <td>...</td>\n",
       "      <td>4889.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unused</td>\n",
       "      <td>0.0</td>\n",
       "      <td>395111.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37213</td>\n",
       "      <td>4927</td>\n",
       "      <td>1014</td>\n",
       "      <td>4995978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56020</th>\n",
       "      <td>37209</td>\n",
       "      <td>564476x2292x0</td>\n",
       "      <td>564476</td>\n",
       "      <td>0e00d6dc5cf2232d15750bd3177c57521cdff678a5666a...</td>\n",
       "      <td>bc1qec377ms3a79e3v3pe8gfjrzp6syfqykeqkw2lsh86f...</td>\n",
       "      <td>564511.0</td>\n",
       "      <td>58fea309da14892858be78c8c45a7d06fa6796e77e51d8...</td>\n",
       "      <td>028303182c9885da93b3b25c9621d22cf34475e63c1239...</td>\n",
       "      <td>03820e3b7bdbf7ccafe67791088de15df162b352f3b7ba...</td>\n",
       "      <td>20000</td>\n",
       "      <td>...</td>\n",
       "      <td>2889.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unused</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17111.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37209</td>\n",
       "      <td>642</td>\n",
       "      <td>4037</td>\n",
       "      <td>2591754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55208</th>\n",
       "      <td>37256</td>\n",
       "      <td>564948x2100x0</td>\n",
       "      <td>564948</td>\n",
       "      <td>e1b4de87949168dafae980bc1f467b2bee878c2667383a...</td>\n",
       "      <td>bc1ql47jp0hprvcq4dz8y5dpape60vhf532uea2rv9v20q...</td>\n",
       "      <td>565057.0</td>\n",
       "      <td>af2a36eb0782958a3f2e7aacb89dfdb92defeaed44df3c...</td>\n",
       "      <td>02529db69fd2ebd3126fb66fafa234fc3544477a23d509...</td>\n",
       "      <td>02e63d3e5a2351cc8de6c63b0d0784d1940406c5addce4...</td>\n",
       "      <td>47882</td>\n",
       "      <td>...</td>\n",
       "      <td>3643.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>force</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44239.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37256</td>\n",
       "      <td>6568</td>\n",
       "      <td>5977</td>\n",
       "      <td>39256936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55207</th>\n",
       "      <td>37255</td>\n",
       "      <td>564948x2082x0</td>\n",
       "      <td>564948</td>\n",
       "      <td>eb12d66c34e8009c408fa56d948bf87baec4888caddd2d...</td>\n",
       "      <td>bc1q4ksyf7c7jphsmwdj8n944y0mypy7n28mts5tt9zrph...</td>\n",
       "      <td>565057.0</td>\n",
       "      <td>af557d74c148c434156edc034b0a94d97afcda20e8c2b6...</td>\n",
       "      <td>02e63d3e5a2351cc8de6c63b0d0784d1940406c5addce4...</td>\n",
       "      <td>039edc94987c8f3adc28dab455efc00dea876089a120f5...</td>\n",
       "      <td>47882</td>\n",
       "      <td>...</td>\n",
       "      <td>3643.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>force</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44239.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37255</td>\n",
       "      <td>5977</td>\n",
       "      <td>1392</td>\n",
       "      <td>8319984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55270</th>\n",
       "      <td>37251</td>\n",
       "      <td>564916x3340x1</td>\n",
       "      <td>564916</td>\n",
       "      <td>ed2406ab63e597108158b6ff9803340cf9b611c76dc3c4...</td>\n",
       "      <td>bc1qxczmyzxtgh88tufzutgtp73a5g9yvdk4vrlz2sqed7...</td>\n",
       "      <td>565057.0</td>\n",
       "      <td>7e2dac954ff73b1a733a0f4a70871fae268ec2a5639ce5...</td>\n",
       "      <td>022755c3ff4e5a1d71f573cda4b315887fc00a9e5c9ea9...</td>\n",
       "      <td>032c0db1094579ff83ee591b90e1212ec56d5cdc18cb43...</td>\n",
       "      <td>800000</td>\n",
       "      <td>...</td>\n",
       "      <td>3150.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unused</td>\n",
       "      <td>0.0</td>\n",
       "      <td>796850.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37251</td>\n",
       "      <td>289</td>\n",
       "      <td>4864</td>\n",
       "      <td>1405696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55271</th>\n",
       "      <td>37250</td>\n",
       "      <td>564916x3339x1</td>\n",
       "      <td>564916</td>\n",
       "      <td>17027dde573a87fc2367d523abca381dcdca5769f4203c...</td>\n",
       "      <td>bc1qevw8khrvswflp4724k2kva44snzeanhtwzetc4vu79...</td>\n",
       "      <td>565057.0</td>\n",
       "      <td>d31fd5531ac0b1c0a7179a28e5d84b98fbfa2e77150f86...</td>\n",
       "      <td>0204a2b95b4c208383d7f02e741a8bfd5b5b7e8bea8d15...</td>\n",
       "      <td>032c0db1094579ff83ee591b90e1212ec56d5cdc18cb43...</td>\n",
       "      <td>800000</td>\n",
       "      <td>...</td>\n",
       "      <td>3150.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unused</td>\n",
       "      <td>0.0</td>\n",
       "      <td>796850.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37250</td>\n",
       "      <td>1878</td>\n",
       "      <td>4864</td>\n",
       "      <td>9134592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55269</th>\n",
       "      <td>37252</td>\n",
       "      <td>564916x3341x1</td>\n",
       "      <td>564916</td>\n",
       "      <td>0179ad769af697a7ddb5d11c067085108ca9b15eb63c5b...</td>\n",
       "      <td>bc1q2nzl7k83wy0hl4g5talvzyd8ludf6rtfkphv6qhe2h...</td>\n",
       "      <td>565057.0</td>\n",
       "      <td>58ea49bfd3d4259a6918963e76ac23c19586eb2f7b038f...</td>\n",
       "      <td>032c0db1094579ff83ee591b90e1212ec56d5cdc18cb43...</td>\n",
       "      <td>036dfa466e991c038a98175697aa284b414d40e03ccf45...</td>\n",
       "      <td>800000</td>\n",
       "      <td>...</td>\n",
       "      <td>3150.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unused</td>\n",
       "      <td>0.0</td>\n",
       "      <td>796850.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37252</td>\n",
       "      <td>4864</td>\n",
       "      <td>6861</td>\n",
       "      <td>33371904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55391</th>\n",
       "      <td>37246</td>\n",
       "      <td>564811x328x1</td>\n",
       "      <td>564811</td>\n",
       "      <td>1c49be81c52d2efe9d4b2ba3e6af6a8a541c6691cf7809...</td>\n",
       "      <td>bc1q9p6vhhs9sx7ndsljmc58a04ynkpkg7x4l2gg83lhyz...</td>\n",
       "      <td>565058.0</td>\n",
       "      <td>67f73be09cf6fe08e58964e6fcca85affb5e15e9c4680a...</td>\n",
       "      <td>02e63d3e5a2351cc8de6c63b0d0784d1940406c5addce4...</td>\n",
       "      <td>03c436af41160a355fc1ed230a64f6a64bcbd2ae50f121...</td>\n",
       "      <td>300000</td>\n",
       "      <td>...</td>\n",
       "      <td>3044.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unused</td>\n",
       "      <td>0.0</td>\n",
       "      <td>296956.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37246</td>\n",
       "      <td>5977</td>\n",
       "      <td>5406</td>\n",
       "      <td>32311662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55386</th>\n",
       "      <td>37248</td>\n",
       "      <td>564814x1050x0</td>\n",
       "      <td>564814</td>\n",
       "      <td>8e2584580304029db1c4f8fc5e0dd50b1ab61581dc607b...</td>\n",
       "      <td>bc1qd652jjq5ptayg2v8vn09w8h0y7mjg6qnp0q3rv8r0g...</td>\n",
       "      <td>565058.0</td>\n",
       "      <td>0ebab776b8adc89aaa058bc81ef6bf27d7e155f90cc1b2...</td>\n",
       "      <td>02e63d3e5a2351cc8de6c63b0d0784d1940406c5addce4...</td>\n",
       "      <td>03c436af41160a355fc1ed230a64f6a64bcbd2ae50f121...</td>\n",
       "      <td>250000</td>\n",
       "      <td>...</td>\n",
       "      <td>3044.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37248</td>\n",
       "      <td>5977</td>\n",
       "      <td>5406</td>\n",
       "      <td>32311662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55180</th>\n",
       "      <td>37259</td>\n",
       "      <td>564954x354x0</td>\n",
       "      <td>564954</td>\n",
       "      <td>1f9b92e1946a4e3faf90c1cc3c650ae7f3c7ed16ca04b1...</td>\n",
       "      <td>bc1ql9sv0g070h7tk09qgkssr8x78f82h3xr2r4m0xgksk...</td>\n",
       "      <td>565059.0</td>\n",
       "      <td>ba40311c33c9c2fe41b6718bf08671fa4de565de51ff51...</td>\n",
       "      <td>0293f0c48a70777630be0538c07ffdbd89c15d2f889f3a...</td>\n",
       "      <td>0395033b252c6f40e3756984162d68174e2bd8060a129c...</td>\n",
       "      <td>40000</td>\n",
       "      <td>...</td>\n",
       "      <td>4014.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unused</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35986.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37259</td>\n",
       "      <td>435</td>\n",
       "      <td>4104</td>\n",
       "      <td>1785240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55548</th>\n",
       "      <td>37239</td>\n",
       "      <td>564757x69x1</td>\n",
       "      <td>564757</td>\n",
       "      <td>61b8a9809c2585b52202e49523a1fcff0e984beca89aed...</td>\n",
       "      <td>bc1q7f6mdhnfapz4wftu73vf750xzgaee2qx9ycdn9w62x...</td>\n",
       "      <td>565060.0</td>\n",
       "      <td>de202ef677c9264efa60355b78628383d62c98158ca624...</td>\n",
       "      <td>038ddc73e450a4bfb52992b35be8b0f52e1a1b4726ff31...</td>\n",
       "      <td>03abf6f44c355dec0d5aa155bdbdd6e0c8fefe318eff40...</td>\n",
       "      <td>3600000</td>\n",
       "      <td>...</td>\n",
       "      <td>3150.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mutual</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88098.0</td>\n",
       "      <td>3508751.0</td>\n",
       "      <td>37239</td>\n",
       "      <td>3186</td>\n",
       "      <td>6748</td>\n",
       "      <td>21499128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55128</th>\n",
       "      <td>37261</td>\n",
       "      <td>564997x765x0</td>\n",
       "      <td>564997</td>\n",
       "      <td>6e8f91721c845b1240a0d829c7ad877ca1fc47b4c7b5fb...</td>\n",
       "      <td>bc1q5k0lf79875t3pnjtqcvhk7u597a7kywhyvq8gwk5lx...</td>\n",
       "      <td>565060.0</td>\n",
       "      <td>eaf3cda88fe6024bd99584c33a4d43136603c6d1807487...</td>\n",
       "      <td>02600e4f3b1c93c930314ddf236129cb15d81d211bd80a...</td>\n",
       "      <td>02e84e47a22d19332b356a6a41906237eeacf37a71d39a...</td>\n",
       "      <td>29355</td>\n",
       "      <td>...</td>\n",
       "      <td>3482.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unused</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25873.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37261</td>\n",
       "      <td>6006</td>\n",
       "      <td>198</td>\n",
       "      <td>1189188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55675</th>\n",
       "      <td>37227</td>\n",
       "      <td>564662x1356x0</td>\n",
       "      <td>564662</td>\n",
       "      <td>bbeebf91e898a2e0cf1e87ff1cce24e393f7e574ac8152...</td>\n",
       "      <td>bc1quds603zgr3swuavmutfac5d76fzl6k7esd50rkddyl...</td>\n",
       "      <td>565228.0</td>\n",
       "      <td>4290b645005bd1cc12193356264c82de7a5050154a82ce...</td>\n",
       "      <td>0232f15308269dbeeb8c34e1f5d4d1c297497c7eefe85f...</td>\n",
       "      <td>032c9bab7bf5c3b17ad231c617315890861227a802b422...</td>\n",
       "      <td>200000</td>\n",
       "      <td>...</td>\n",
       "      <td>4425.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unused</td>\n",
       "      <td>0.0</td>\n",
       "      <td>195575.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37227</td>\n",
       "      <td>5458</td>\n",
       "      <td>1238</td>\n",
       "      <td>6757004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55779</th>\n",
       "      <td>37220</td>\n",
       "      <td>564600x2121x0</td>\n",
       "      <td>564600</td>\n",
       "      <td>002cad2950c24d372bfbf29b749400fe49079959a97233...</td>\n",
       "      <td>bc1q0fwjew3xf4gne825ekxnv608p32h7hg92vyx8vy67t...</td>\n",
       "      <td>565229.0</td>\n",
       "      <td>48533df44ed0c02467e8d8abfe8d06277404f2126a1a5d...</td>\n",
       "      <td>0232f15308269dbeeb8c34e1f5d4d1c297497c7eefe85f...</td>\n",
       "      <td>0324d8f66feaf3e003a12366502f509834ee355485d63d...</td>\n",
       "      <td>200000</td>\n",
       "      <td>...</td>\n",
       "      <td>3643.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>force</td>\n",
       "      <td>0.0</td>\n",
       "      <td>196357.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37220</td>\n",
       "      <td>5458</td>\n",
       "      <td>332</td>\n",
       "      <td>1812056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0 short_channel_id  open_block  \\\n",
       "70648        1027    535029x2012x1      535029   \n",
       "70625        1045     535177x446x1      535177   \n",
       "68629        2745    549037x2738x0      549037   \n",
       "68636        2744    549037x2737x0      549037   \n",
       "68461       40227    549489x1194x1      549489   \n",
       "59540       37095    562592x1695x1      562592   \n",
       "55969       37213     564495x455x1      564495   \n",
       "56020       37209    564476x2292x0      564476   \n",
       "55208       37256    564948x2100x0      564948   \n",
       "55207       37255    564948x2082x0      564948   \n",
       "55270       37251    564916x3340x1      564916   \n",
       "55271       37250    564916x3339x1      564916   \n",
       "55269       37252    564916x3341x1      564916   \n",
       "55391       37246     564811x328x1      564811   \n",
       "55386       37248    564814x1050x0      564814   \n",
       "55180       37259     564954x354x0      564954   \n",
       "55548       37239      564757x69x1      564757   \n",
       "55128       37261     564997x765x0      564997   \n",
       "55675       37227    564662x1356x0      564662   \n",
       "55779       37220    564600x2121x0      564600   \n",
       "\n",
       "                                        open_transaction  \\\n",
       "70648  d01928d350e1ba04d7335a91e6dd54f5dbf94859e0c59b...   \n",
       "70625  7376d5bc0c18bbff8f644d0827e759a1518b38e1e95a08...   \n",
       "68629  b7128bbbe422b4f18fad71b091eed1f9e4b0d231be8117...   \n",
       "68636  0825da5e96cd45fced3233ebe615721b687285839d3036...   \n",
       "68461  58dafe493648fbdd69143c26e0cf8a66ae11a272c2739d...   \n",
       "59540  06b4d9b3cfa10bd2cd33131d034a6b38c1651eee49018a...   \n",
       "55969  8a5764e1f0cb659b687a0675cd88983526bd9213665986...   \n",
       "56020  0e00d6dc5cf2232d15750bd3177c57521cdff678a5666a...   \n",
       "55208  e1b4de87949168dafae980bc1f467b2bee878c2667383a...   \n",
       "55207  eb12d66c34e8009c408fa56d948bf87baec4888caddd2d...   \n",
       "55270  ed2406ab63e597108158b6ff9803340cf9b611c76dc3c4...   \n",
       "55271  17027dde573a87fc2367d523abca381dcdca5769f4203c...   \n",
       "55269  0179ad769af697a7ddb5d11c067085108ca9b15eb63c5b...   \n",
       "55391  1c49be81c52d2efe9d4b2ba3e6af6a8a541c6691cf7809...   \n",
       "55386  8e2584580304029db1c4f8fc5e0dd50b1ab61581dc607b...   \n",
       "55180  1f9b92e1946a4e3faf90c1cc3c650ae7f3c7ed16ca04b1...   \n",
       "55548  61b8a9809c2585b52202e49523a1fcff0e984beca89aed...   \n",
       "55128  6e8f91721c845b1240a0d829c7ad877ca1fc47b4c7b5fb...   \n",
       "55675  bbeebf91e898a2e0cf1e87ff1cce24e393f7e574ac8152...   \n",
       "55779  002cad2950c24d372bfbf29b749400fe49079959a97233...   \n",
       "\n",
       "                                                 address  close_block  \\\n",
       "70648  bc1qszamn0la3yqrqhjj8yepdxkl9qlr84zfwgg9zrkccl...     535029.0   \n",
       "70625  bc1qauzljedtlva73ngg7suqketlvn5gnnuemxpeuevcqt...     535177.0   \n",
       "68629  bc1q95fytjzs8f7fma2nf66gcva7c3w7hnkdwrkef9pu33...     549037.0   \n",
       "68636  bc1q5mqzhw5e42rfqh250zalwu47ru8gvz4g4k968me0mg...     549037.0   \n",
       "68461  bc1q25j5l6crv4mrjkjjjw4rzyv890cwwnyyw9dezcqs5x...     549489.0   \n",
       "59540  bc1q9jnkm78y45kyasnu43p52gc8sqazwy4yfjtqzkwqx5...     562592.0   \n",
       "55969  bc1qqgcvu4nl3vjm4vvr9r5l2f4ufppph5fn070fzscq4r...     564510.0   \n",
       "56020  bc1qec377ms3a79e3v3pe8gfjrzp6syfqykeqkw2lsh86f...     564511.0   \n",
       "55208  bc1ql47jp0hprvcq4dz8y5dpape60vhf532uea2rv9v20q...     565057.0   \n",
       "55207  bc1q4ksyf7c7jphsmwdj8n944y0mypy7n28mts5tt9zrph...     565057.0   \n",
       "55270  bc1qxczmyzxtgh88tufzutgtp73a5g9yvdk4vrlz2sqed7...     565057.0   \n",
       "55271  bc1qevw8khrvswflp4724k2kva44snzeanhtwzetc4vu79...     565057.0   \n",
       "55269  bc1q2nzl7k83wy0hl4g5talvzyd8ludf6rtfkphv6qhe2h...     565057.0   \n",
       "55391  bc1q9p6vhhs9sx7ndsljmc58a04ynkpkg7x4l2gg83lhyz...     565058.0   \n",
       "55386  bc1qd652jjq5ptayg2v8vn09w8h0y7mjg6qnp0q3rv8r0g...     565058.0   \n",
       "55180  bc1ql9sv0g070h7tk09qgkssr8x78f82h3xr2r4m0xgksk...     565059.0   \n",
       "55548  bc1q7f6mdhnfapz4wftu73vf750xzgaee2qx9ycdn9w62x...     565060.0   \n",
       "55128  bc1q5k0lf79875t3pnjtqcvhk7u597a7kywhyvq8gwk5lx...     565060.0   \n",
       "55675  bc1quds603zgr3swuavmutfac5d76fzl6k7esd50rkddyl...     565228.0   \n",
       "55779  bc1q0fwjew3xf4gne825ekxnv608p32h7hg92vyx8vy67t...     565229.0   \n",
       "\n",
       "                                       close_transaction  \\\n",
       "70648  d01928d350e1ba04d7335a91e6dd54f5dbf94859e0c59b...   \n",
       "70625  7376d5bc0c18bbff8f644d0827e759a1518b38e1e95a08...   \n",
       "68629  b7128bbbe422b4f18fad71b091eed1f9e4b0d231be8117...   \n",
       "68636  0825da5e96cd45fced3233ebe615721b687285839d3036...   \n",
       "68461  58dafe493648fbdd69143c26e0cf8a66ae11a272c2739d...   \n",
       "59540  06b4d9b3cfa10bd2cd33131d034a6b38c1651eee49018a...   \n",
       "55969  655c3d44c09055e9af9f8d13d55c99979e0b0b306230b9...   \n",
       "56020  58fea309da14892858be78c8c45a7d06fa6796e77e51d8...   \n",
       "55208  af2a36eb0782958a3f2e7aacb89dfdb92defeaed44df3c...   \n",
       "55207  af557d74c148c434156edc034b0a94d97afcda20e8c2b6...   \n",
       "55270  7e2dac954ff73b1a733a0f4a70871fae268ec2a5639ce5...   \n",
       "55271  d31fd5531ac0b1c0a7179a28e5d84b98fbfa2e77150f86...   \n",
       "55269  58ea49bfd3d4259a6918963e76ac23c19586eb2f7b038f...   \n",
       "55391  67f73be09cf6fe08e58964e6fcca85affb5e15e9c4680a...   \n",
       "55386  0ebab776b8adc89aaa058bc81ef6bf27d7e155f90cc1b2...   \n",
       "55180  ba40311c33c9c2fe41b6718bf08671fa4de565de51ff51...   \n",
       "55548  de202ef677c9264efa60355b78628383d62c98158ca624...   \n",
       "55128  eaf3cda88fe6024bd99584c33a4d43136603c6d1807487...   \n",
       "55675  4290b645005bd1cc12193356264c82de7a5050154a82ce...   \n",
       "55779  48533df44ed0c02467e8d8abfe8d06277404f2126a1a5d...   \n",
       "\n",
       "                                                   node0  \\\n",
       "70648  022a7809052db05fde648391a53aba82286e4a517cff1d...   \n",
       "70625  02272bd12e59324d0f2b231fb88f134b57eb26dd100d2c...   \n",
       "68629  028b892b15f5cabcea5165b236db0e36dc06553c323c84...   \n",
       "68636  02b95713bbe4609a337f3ca5aab3a75674083ddf5331a4...   \n",
       "68461  02574ffa55d394b9326f6e5c15992cc0516b0d6e6a79a1...   \n",
       "59540  029b71b8186914267ea59cb081c43ad1aeb874b5a185a4...   \n",
       "55969  024655b768ef40951b20053a5c4b951606d4d86085d512...   \n",
       "56020  028303182c9885da93b3b25c9621d22cf34475e63c1239...   \n",
       "55208  02529db69fd2ebd3126fb66fafa234fc3544477a23d509...   \n",
       "55207  02e63d3e5a2351cc8de6c63b0d0784d1940406c5addce4...   \n",
       "55270  022755c3ff4e5a1d71f573cda4b315887fc00a9e5c9ea9...   \n",
       "55271  0204a2b95b4c208383d7f02e741a8bfd5b5b7e8bea8d15...   \n",
       "55269  032c0db1094579ff83ee591b90e1212ec56d5cdc18cb43...   \n",
       "55391  02e63d3e5a2351cc8de6c63b0d0784d1940406c5addce4...   \n",
       "55386  02e63d3e5a2351cc8de6c63b0d0784d1940406c5addce4...   \n",
       "55180  0293f0c48a70777630be0538c07ffdbd89c15d2f889f3a...   \n",
       "55548  038ddc73e450a4bfb52992b35be8b0f52e1a1b4726ff31...   \n",
       "55128  02600e4f3b1c93c930314ddf236129cb15d81d211bd80a...   \n",
       "55675  0232f15308269dbeeb8c34e1f5d4d1c297497c7eefe85f...   \n",
       "55779  0232f15308269dbeeb8c34e1f5d4d1c297497c7eefe85f...   \n",
       "\n",
       "                                                   node1  satoshis  ...  \\\n",
       "70648  031b71cbad0cb4e22141e45f16c83c332f755e1ba68195...    462124  ...   \n",
       "70625  031b71cbad0cb4e22141e45f16c83c332f755e1ba68195...    257307  ...   \n",
       "68629  038b36a43c38f75cd15bb25394f1cd162f717df0055852...    400000  ...   \n",
       "68636  038b36a43c38f75cd15bb25394f1cd162f717df0055852...    400000  ...   \n",
       "68461  03a5927b64b1ea8657d5b770d61a3e2d0554fdb5d56877...   2500000  ...   \n",
       "59540  03864ef025fde8fb587d989186ce6a4a186895ee44a926...   5000000  ...   \n",
       "55969  0375a154b8f94eb0556566d60d96acc47f99f2f0d74ef9...    400000  ...   \n",
       "56020  03820e3b7bdbf7ccafe67791088de15df162b352f3b7ba...     20000  ...   \n",
       "55208  02e63d3e5a2351cc8de6c63b0d0784d1940406c5addce4...     47882  ...   \n",
       "55207  039edc94987c8f3adc28dab455efc00dea876089a120f5...     47882  ...   \n",
       "55270  032c0db1094579ff83ee591b90e1212ec56d5cdc18cb43...    800000  ...   \n",
       "55271  032c0db1094579ff83ee591b90e1212ec56d5cdc18cb43...    800000  ...   \n",
       "55269  036dfa466e991c038a98175697aa284b414d40e03ccf45...    800000  ...   \n",
       "55391  03c436af41160a355fc1ed230a64f6a64bcbd2ae50f121...    300000  ...   \n",
       "55386  03c436af41160a355fc1ed230a64f6a64bcbd2ae50f121...    250000  ...   \n",
       "55180  0395033b252c6f40e3756984162d68174e2bd8060a129c...     40000  ...   \n",
       "55548  03abf6f44c355dec0d5aa155bdbdd6e0c8fefe318eff40...   3600000  ...   \n",
       "55128  02e84e47a22d19332b356a6a41906237eeacf37a71d39a...     29355  ...   \n",
       "55675  032c9bab7bf5c3b17ad231c617315890861227a802b422...    200000  ...   \n",
       "55779  0324d8f66feaf3e003a12366502f509834ee355485d63d...    200000  ...   \n",
       "\n",
       "      close_fee   last_update  close_type  close_htlc_count  close_balance_a  \\\n",
       "70648    1989.0           NaN      unused               0.0         275630.0   \n",
       "70625     767.0           NaN      unused               0.0         218405.0   \n",
       "68629    1991.0  1.547494e+09      unused               0.0         400000.0   \n",
       "68636    1992.0  1.547503e+09      unused               0.0         400000.0   \n",
       "68461    2363.0           NaN      unused               0.0           8974.0   \n",
       "59540    2136.0           NaN      unused               0.0        4995006.0   \n",
       "55969    4889.0           NaN      unused               0.0         395111.0   \n",
       "56020    2889.0           NaN      unused               0.0          17111.0   \n",
       "55208    3643.0           NaN       force               0.0          44239.0   \n",
       "55207    3643.0           NaN       force               0.0          44239.0   \n",
       "55270    3150.0           NaN      unused               0.0         796850.0   \n",
       "55271    3150.0           NaN      unused               0.0         796850.0   \n",
       "55269    3150.0           NaN      unused               0.0         796850.0   \n",
       "55391    3044.0           NaN      unused               0.0         296956.0   \n",
       "55386    3044.0           NaN     unknown               0.0              0.0   \n",
       "55180    4014.0           NaN      unused               0.0          35986.0   \n",
       "55548    3150.0           NaN      mutual               0.0          88098.0   \n",
       "55128    3482.0           NaN      unused               0.0          25873.0   \n",
       "55675    4425.0           NaN      unused               0.0         195575.0   \n",
       "55779    3643.0           NaN       force               0.0         196357.0   \n",
       "\n",
       "       close_balance_b dec_id  node0_id  node1_id  node_pair  \n",
       "70648              0.0   1027      4091      3578   14637598  \n",
       "70625              0.0   1045      3604      3578   12895112  \n",
       "68629              0.0   2745      1781      4968    8848008  \n",
       "68636              0.0   2744       210      4968    1043280  \n",
       "68461              0.0  40227      7558      4495   33973210  \n",
       "59540              0.0  37095      3210      7259   23301390  \n",
       "55969              0.0  37213      4927      1014    4995978  \n",
       "56020              0.0  37209       642      4037    2591754  \n",
       "55208              0.0  37256      6568      5977   39256936  \n",
       "55207              0.0  37255      5977      1392    8319984  \n",
       "55270              0.0  37251       289      4864    1405696  \n",
       "55271              0.0  37250      1878      4864    9134592  \n",
       "55269              0.0  37252      4864      6861   33371904  \n",
       "55391              0.0  37246      5977      5406   32311662  \n",
       "55386              0.0  37248      5977      5406   32311662  \n",
       "55180              0.0  37259       435      4104    1785240  \n",
       "55548        3508751.0  37239      3186      6748   21499128  \n",
       "55128              0.0  37261      6006       198    1189188  \n",
       "55675              0.0  37227      5458      1238    6757004  \n",
       "55779              0.0  37220      5458       332    1812056  \n",
       "\n",
       "[20 rows x 24 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort and visualize DataFrame\n",
    "\n",
    "decisions_df.sort_values(by=['open_block'],inplace=True,ascending=True)\n",
    "print('Size of DataFrame in Memory:{}'.format(sys.getsizeof(decisions_df)))\n",
    "# Check specific channel id\n",
    "#decisions_df[decisions_df['short_channel_id']=='513675x2245x0'].head()\n",
    "\n",
    "decisions_df.sort_values(by=['close_block'],ascending=True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'short_channel_id', 'open_block', 'open_transaction',\n",
       "       'address', 'close_block', 'close_transaction', 'node0', 'node1',\n",
       "       'satoshis', 'last_seen', 'open_time', 'open_fee', 'close_time',\n",
       "       'close_fee', 'last_update', 'close_type', 'close_htlc_count',\n",
       "       'close_balance_a', 'close_balance_b', 'dec_id', 'node0_id', 'node1_id',\n",
       "       'node_pair'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decisions_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TESTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Lazy Graph extract\n",
    "blocks_att=[]\n",
    "for i in range(len(graph_snapshots)):\n",
    "    graph_i=dask.compute(graph_snapshots[i])\n",
    "    block=graph_i.graph['block']\n",
    "    blocks_att.append(block)\n",
    "\n",
    "print(blocks_att)\n",
    "\n",
    "#graph_snapshots=dask.compute(*graph_snapshots)\n",
    "#block=graph_snapshots[0].graph['block']\n",
    "    \n",
    "#print(len(graph_snapshots[5]))\n",
    "#print(graph_snapshots[3].graph['block'])\n",
    "\n",
    "# Delayed testing\n",
    "#results = dask.compute(*futures)\n",
    "#graphs=dask.compute(*graph_snapshots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Comparative Analysis\n",
    "\n",
    "In order to understand the potential motivations behind each decision we analyze each decission (opening or closure of a channel) independently from the perspective of each of the participants in the decission, which we'll call the node under analysis. For each decission we extract or compute the following information: \n",
    "\n",
    "Betweenness centrality measures how central is a network to the flow of information in a network. In the case of the Lightning Network the higher the betweenness centrality of a node, the more transactions (messages) that are routed through it. In particular, we will use a measure of betweenness centrality defined in (Brandes and Fleischer 2005 - https://link.springer.com/chapter/10.1007/978-3-540-31856-9_44) that models infomation through a network, as electric current, efficiently and not only considering shortest path. This allows us to account for the fact that not all transactions travel through shortes path given that there are fee and capacity considerations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Measurments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Property measurement for a collection of graphs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEW\n",
    "# Function to calculate base measurement for every graph in snapshot\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "collection_measure\n",
    "    Iterates over graph snapshots and calculates measurement for every node in each of the graphs.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "g_snapshots : list\n",
    "    List of delayed nx graph elements contianing graph snapshots\n",
    "    \n",
    "\n",
    "measurement: string\n",
    "    Name of the type of measurment that will applied to the graph. (See graph_measurement function for options)\n",
    "\n",
    "\n",
    "\n",
    "Returns\n",
    "-------\n",
    "snapshot_mes_dic: dic\n",
    "    Dictionary with blocks as keys and dic of measurements as values\n",
    "\n",
    "\"\"\"\n",
    "def collection_measure(bucket,graph_keys,measurement):\n",
    "    \n",
    "    snapshot_mes_list=[]\n",
    "    \n",
    "    \n",
    "    \n",
    "    with tqdm(total=len(range(1,len(graph_keys)))) as pbar:\n",
    "        for i in (range(1,len(graph_keys))):\n",
    "            \n",
    "            key=graph_keys[i]\n",
    "            prev_key=graph_keys[i-1]\n",
    "\n",
    "            measurement_input=(key,measurement,'capacity',bucket,prev_key)\n",
    "\n",
    "            b_g_tuple=dask.delayed(graph_measurement)(measurement_input)\n",
    "            snapshot_mes_list.append(b_g_tuple)\n",
    "            pbar.update(1)\n",
    "\n",
    "    futures = dask.persist(*snapshot_mes_list)\n",
    "    \n",
    "    start=time.time()\n",
    "    snapshot_mes_list = dask.compute(*futures)\n",
    "    #snapshot_mes_list = dask.compute(*snapshot_mes_list)\n",
    "    snapshot_mes_dic={record[0]:record[1] for record in snapshot_mes_list}\n",
    "    end=time.time()\n",
    "    print('Compute in seconds: {}'.format(end-start))\n",
    "    \n",
    "    return snapshot_mes_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Property measurment for a single graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "graph_measurement\n",
    "    Performs specific graph measurement \n",
    "\n",
    "Parameters\n",
    "----------\n",
    "measurment_input: tuple\n",
    "    g : nx graph\n",
    "        NetworkX graph object over which measurment will be calculated for each node\n",
    "\n",
    "    measurment: string\n",
    "        Type of measurement to be performed on graph\n",
    "        \n",
    "    weight: string\n",
    "        Edge attribute to be used as weight\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "g_dir: dir\n",
    "    Dictionary with measurment values for each node\n",
    "\"\"\"\n",
    "\n",
    "def graph_measurement(measurement_input):\n",
    "    \n",
    "    # Extract inputs\n",
    "    key=measurement_input[0]\n",
    "    measurement=measurement_input[1]\n",
    "    weight=measurement_input[2]\n",
    "    bucket=measurement_input[3]\n",
    "    \n",
    "    \n",
    "    if len(measurement_input)>4:\n",
    "        prev_key=measurement_input[4]\n",
    "    \n",
    "    # Download graph\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.resource('s3')\n",
    "    response = s3.Object(bucket_name=bucket, key=key).get()\n",
    "    g=pickle.loads(response['Body'].read())\n",
    "    \n",
    "    # Extract Block\n",
    "    \n",
    "    block=g.graph['block']\n",
    "    score_type='/raw_score/'\n",
    "   \n",
    "    \n",
    "    if measurement=='current_betweeness_full':\n",
    "        g_dir=nx.algorithms.centrality.current_flow_betweenness_centrality(g,weight=weight)\n",
    "    \n",
    "    elif measurement=='current_betweeness':\n",
    "        g_dir=nx.algorithms.centrality.approximate_current_flow_betweenness_centrality(g,weight=weight)\n",
    "        \n",
    "    elif measurement=='current_betweeness_unweighted': # for unweighted current betweeness\n",
    "        g_dir=nx.algorithms.centrality.approximate_current_flow_betweenness_centrality(g)\n",
    "    \n",
    "    elif measurement=='current_closeness':\n",
    "        g_dir=nx.algorithms.centrality.current_flow_closeness_centrality(g,weight=weight)\n",
    "    \n",
    "    elif measurement=='closeness':\n",
    "        g_dir=nx.closeness_centrality(g)\n",
    "        \n",
    "    elif measurement=='clustering':\n",
    "        g_dir=nx.clustering(g,weight=weight)\n",
    "        \n",
    "    elif measurement=='node_count':\n",
    "        g_dir=len(g.nodes())\n",
    "        \n",
    "    elif measurement=='channels':\n",
    "        g_dir=dict(list(g.degree(g.nodes())))\n",
    "    \n",
    "    elif measurement=='capacity':\n",
    "        g_dir=dict(list(g.degree(g.nodes(),weight=weight)))\n",
    "        \n",
    "    elif measurement=='age': \n",
    "        \n",
    "        # Get node_details from S3 \n",
    "        opens_file = s3.Object(bucket_name=bucket, key='node_details.p').get()\n",
    "        node_details = pickle.loads(opens_file['Body'].read())\n",
    "        \n",
    "        # Create dic with node's age in blocks\n",
    "        g_dir={node:block-node_details[node]['birth_block'] for node in list(g.nodes())} \n",
    "        \n",
    "        \n",
    "    elif measurement=='capacity_growth':  \n",
    "        g_dir=capacity_growth (weight,bucket,g,block,s3,block_frame=3600)\n",
    "        \n",
    "    elif measurement=='closeness_approx_rank':\n",
    "        \n",
    "        # Re-select previous block\n",
    "        response = s3.Object(bucket_name=bucket, key=prev_key).get()\n",
    "        g=pickle.loads(response['Body'].read())\n",
    "        \n",
    "        g_dir=closeness_approx_rank (s3,bucket,g,block,p=13.38,estimate_sample=50)\n",
    "        score_type='/norm_rank/'\n",
    "        \n",
    "    elif measurement=='closeness_approx_rank_post': # Same measurement as above, just looking at the rank after block decisions happen\n",
    "        \n",
    "        g_dir=closeness_approx_rank (s3,bucket,g,block,p=13.38,estimate_sample=50)\n",
    "        score_type='/norm_rank/'\n",
    "        \n",
    "    elif measurement=='avg_short_path':\n",
    "        g_dir=nx.average_shortest_path_length(g)\n",
    "        \n",
    "    elif measurement=='min_nodes':\n",
    "        g_dir=nx.minimum_node_cut(g)\n",
    "        \n",
    "        \n",
    "    elif measurement=='robustness_eff_loss':\n",
    "        attack_perc=0.01\n",
    "        g_dir=robustness_eff_loss(s3,bucket,g,block,attack_perc)\n",
    "        measurement=measurement+'_'+str(attack_perc*100)\n",
    "    \n",
    "        \n",
    "    # Safe graph processing to S3\n",
    "    \n",
    "    extraction_id=key.split('/')[1].split('_')[0]\n",
    "    key_out='graph_snapshots/'+extraction_id+'_connected/.data_transformations/'+measurement+score_type+str(block)+'.pkl'\n",
    "    pickle_byte_obj = pickle.dumps(g_dir) \n",
    "    response=s3.Object(bucket,key_out).put(Body=pickle_byte_obj)['ResponseMetadata']['HTTPStatusCode']\n",
    "    \n",
    "    \n",
    "    \n",
    "    return (block,response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "capacity_growth\n",
    "    Calculates how much has capacity grown (or decreased) for all nodes in a graph\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "weight: str\n",
    "    Node property that will be used to weight the calculation.\n",
    "    \n",
    "bucket: str\n",
    "    S3 bucket where data is stored\n",
    "\n",
    "g: NetworkX graph\n",
    "    Graph for which the calculation will be computed\n",
    "    \n",
    "block: int\n",
    "    Block number corresponding to the selected graph\n",
    "\n",
    "s3: S3 session object\n",
    "    S3 session object for the boto3 api\n",
    "    \n",
    "block_frame: int\n",
    "    The amount of blocks into the past that will be considered to calculate growth\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "g_dir: dir\n",
    "    Dictionary with measurment values for each node\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def capacity_growth (weight,bucket,g,block,s3,block_frame=3600):\n",
    "    \n",
    "    # Initialize g_dir items and min_block\n",
    "    \n",
    "    g_dir={}\n",
    "    min_block=block-block_frame\n",
    "    \n",
    "    \n",
    "    # Get graph nodes\n",
    "    nodes=list(g.nodes())\n",
    "    \n",
    "    # Load decisions DataFrame\n",
    "    \n",
    "    decisions_load = s3.Object(bucket_name=bucket, key='decisions_df.csv').get()\n",
    "    decisions_df=pd.read_csv(io.BytesIO(decisions_load['Body'].read()))\n",
    "    \n",
    "    with tqdm(total=len(nodes),disable=True) as pbar:\n",
    "        \n",
    "        for node in nodes:\n",
    "\n",
    "            # Find all channel creations and closerues in block frame\n",
    "            opens_blockframe_node0=decisions_df[(decisions_df['node0_id']==node) & (decisions_df['open_block']>=min_block) & (decisions_df['open_block']<=block)]['satoshis']\n",
    "            opens_blockframe_node1=decisions_df[(decisions_df['node1_id']==node) & (decisions_df['open_block']>=min_block) & (decisions_df['open_block']<=block)]['satoshis']\n",
    "            closes_blockframe_node0=decisions_df[(decisions_df['node0_id']==node) & (decisions_df['close_block']>=min_block) & (decisions_df['close_block']<=block)]['satoshis']\n",
    "            closes_blockframe_node1=decisions_df[(decisions_df['node1_id']==node) & (decisions_df['close_block']>=min_block) & (decisions_df['close_block']<=block)]['satoshis']\n",
    "\n",
    "            # Calculate growth by adding capacity created in block frame and subtracting capacity lost\n",
    "            if weight==1: #Unweighted calculation\n",
    "                gain=opens_blockframe_node0.count()+opens_blockframe_node1.count()\n",
    "                loss=closes_blockframe_node0.count()+closes_blockframe_node1.count()\n",
    "\n",
    "            else:\n",
    "                gain=opens_blockframe_node0.sum()+opens_blockframe_node1.sum()\n",
    "                loss=closes_blockframe_node0.sum()+closes_blockframe_node1.sum()\n",
    "\n",
    "            # Calculate growth and save to dir\n",
    "            net_growth=gain-loss\n",
    "            g_dir[node]=net_growth\n",
    "            pbar.update(1)\n",
    "        \n",
    "\n",
    "    return g_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEST: Weighted capacity function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block selected:532022\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8fb351046764ca6a674e682973588e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=683.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dic was saved correctly. Sample below:\n",
      "[(6038, 0), (5314, 0), (934, 0), (3023, 0), (3436, 3131000), (3310, 0), (422, 0), (1912, 0), (5154, 0), (4688, 0)]\n"
     ]
    }
   ],
   "source": [
    "# Define test parameters\n",
    "\n",
    "\n",
    "measurement='capacity_growth'\n",
    "weight='capacity'\n",
    "g_test,nodes_test,g_dic_test,block=load_graph_measurement(extraction_id,measurement,weight,blocks,extract_keys,test_ix=-1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3436, 3131000), (1331, 5500000), (6156, 520000), (4998, 4485183), (4527, 200000), (2757, 1677721), (3065, 1200000), (346, 19061712), (2724, 1491661), (2460, 100000), (2476, 9564725), (1893, 300000), (6418, 15737856), (6599, 93799), (4534, 805272), (5641, 97000), (2295, 1200000), (6832, 792562), (7711, 1500000), (1410, 2700000), (1514, 89141), (6296, 1267024), (7608, 2541273), (7631, 182031), (3271, 21417), (2674, 9193000), (3382, 1872962), (1220, 20000), (4580, 500000), (6215, 11315423), (6363, 10000), (4620, 7124495), (7673, 520550), (4639, 20000), (4426, 553341), (5738, 1319859), (7259, 4490000), (6924, 343070), (448, 174997), (227, 675364), (326, 2000), (5372, 77107), (4490, 10000), (2739, 50000), (2881, 20000), (4819, 20000), (4427, 500000), (1172, 80000), (6378, 100000), (1120, 1777721), (5634, 5000), (2300, 500000), (2973, 34003), (6249, 60000), (3, 27529), (7606, 743866), (415, 389026), (5601, 3390969), (5406, 1500000), (5495, 600000), (1257, 500000), (2512, 350000), (7073, 443002), (1082, 318000), (595, 200000), (7379, 800000), (7566, 132765), (6400, 2000000), (3846, 682031), (3283, 6681303), (6975, 40928), (3635, 900000), (1350, 28764), (6646, 740000), (869, 77130), (4259, 165826), (99, 500000), (3351, 1677721), (5242, 669000), (3107, 200000), (7533, 357886), (6465, 1646300), (6435, 60000), (5063, 656735), (1469, 8974725), (5797, 86000), (764, 188831), (5016, 500000), (4175, 1000000), (6748, 2000000), (5321, 7455151), (275, 2000000), (6553, 1000000), (4950, 172552), (4571, 1200000), (3224, 200000), (1182, 6379999), (4701, 1014003), (3030, 20000), (5976, 500000), (6657, 1950000), (2604, 300000), (1901, 1999999), (2686, 300000), (3181, 1400000), (285, 1581273), (2247, 810629), (5796, 579000), (1131, 143366), (3841, 420177), (2789, 400000), (3904, 2293799), (6757, 2000000), (4896, 550000), (2785, 80800), (4516, 7930183), (1239, 666741), (4297, 600000), (180, 400000), (6303, 1133341), (62, 1227139), (5235, 400000), (174, 20000), (3979, 182519), (7194, 100000), (5311, 500000), (3796, 165826), (3882, 2700000), (953, 540000), (4409, 100000), (7039, 20000), (6160, 7400000), (7250, 616350), (5429, 122962), (5583, 400000), (7696, 21000), (252, 10000000), (4192, 3000000), (5005, 400000), (5066, 384510), (5899, 9193000), (5124, 480000), (3996, 40928), (4899, 26591), (1910, 176716), (6787, 500000), (1631, 222273), (4043, 150000), (3589, 50000), (424, 100000), (2696, 700000), (6408, 2175926), (1603, 1097000), (2769, 500000), (4720, 6750000), (795, 100000), (5209, 1600000), (5393, 220000), (3844, 169884), (1813, 482976), (2806, 20000), (4474, 100000), (6112, 240000), (6568, 2350000), (1503, 134510), (619, 150000), (5654, 491000), (4723, 150705), (4928, 55381), (6075, 86000), (7205, 500000), (3808, 1000000), (7125, 2226296), (2654, 22334), (4813, 500000), (5227, 872816), (6063, 852625), (6023, 70000), (1714, 140000), (1333, 280000), (5599, 16637691), (5933, 100000), (420, 318000), (6267, 295300), (4776, 1677721), (2466, 120000), (2994, 500000), (4735, 500000), (3474, 900000), (7734, 614869), (6061, 140863)]\n"
     ]
    }
   ],
   "source": [
    "positive=[t for t in list(g_dic_test.items()) if t[1]>0]\n",
    "print(positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_growth(g_test,g_dic_test,block,channel_opens,channel_closures):\n",
    "\n",
    "    test_list=[]\n",
    "    error_nodes=[]\n",
    "\n",
    "\n",
    "    min_block=block-3600\n",
    "\n",
    "    relevant_opens=[t[0] for t in list(channel_opens.items()) if (t[0]>=min_block and t[0]<=block)]\n",
    "    relevant_closures=[t[0] for t in list(channel_closures.items()) if (t[0]>=min_block and t[0]<=block)]\n",
    "    relevant_blocks=sorted(list(set(relevant_opens).union(set(relevant_closures))))\n",
    "\n",
    "\n",
    "    # Get graph nodes\n",
    "    nodes=list(g_test.nodes())\n",
    "\n",
    "        # Calculate net change for each node in each of the relevant blocks\n",
    "    with tqdm(total=len(nodes)) as pbar:\n",
    "        for node in nodes:\n",
    "            net_change=0\n",
    "            for block in relevant_blocks:\n",
    "\n",
    "                # Create list of nodes involved in channel opens and count how many times a node appears\n",
    "\n",
    "                if weight==1:\n",
    "                    list_block_opens=[[t[0],t[1]] for t in channel_opens[block] if (t[0]==node or t[1]==node)]\n",
    "                    total_weights=len(list_block_opens)\n",
    "\n",
    "                else:\n",
    "                    list_block_opens=[[t[0],t[1],t[2][weight]] for t in channel_opens[block] if (t[0]==node or t[1]==node)]\n",
    "                    total_weights=np.array([l[2] for l in list_block_opens]).sum()\n",
    "\n",
    "\n",
    "                net_change+=total_weights\n",
    "\n",
    "\n",
    "                # Create list of nodes involved in channel closures and count how many times a node appears\n",
    "\n",
    "                if weight==1:\n",
    "                    list_block_closures=[[t[0],t[1]] for t in channel_closures[block] if (t[0]==node or t[1]==node)]\n",
    "                    total_weights=len(list_block_opens)\n",
    "\n",
    "                else:\n",
    "                    list_block_closures=[[t[0],t[1],t[2][weight]] for t in channel_closures[block] if (t[0]==node or t[1]==node)]\n",
    "                    total_weights=np.array([l[2] for l in list_block_closures]).sum()\n",
    "\n",
    "\n",
    "                net_change-=total_weights\n",
    "\n",
    "            #Retrieve recorded growth for node\n",
    "\n",
    "            recorded_growth=g_dic_test[node] \n",
    "\n",
    "            # Check if growth match and populate test_list accordingly\n",
    "            if net_change==recorded_growth:\n",
    "                test_list.append(1)\n",
    "            else:\n",
    "                test_list.append(0)\n",
    "                error_nodes.append((node,recorded_growth,net_growth))\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "        # Add up all passed tests\n",
    "        tests_passed=np.array(test_list).sum()\n",
    "\n",
    "        # Print out statements based on test results\n",
    "    if tests_passed==len(nodes):\n",
    "        print('Growth calculated correctly for all nodes')\n",
    "\n",
    "\n",
    "    else: \n",
    "        print('Growth failed to be correctly calculated for {} nodes'.format(len(list(g_test.nodes()))-tests_passed))\n",
    "        if len(error_nodes)>10:\n",
    "            print('Some nodes with errors (node,recorded,actual)')\n",
    "            print(error_nodes[:10])\n",
    "\n",
    "        else:\n",
    "            print('Some nodes with errors (node,recorded,actual)')\n",
    "            print(error_nodes[:len(error_nodes)])\n",
    "        \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe37b5c85394f6193b009e7a0b12ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=683.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Growth calculated correctly for all nodes\n"
     ]
    }
   ],
   "source": [
    "test_growth(g_test,g_dic_test,block,channel_opens,channel_closures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEST: Test for age calculation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bafd82b8e064c6caebc03e31e9cdd89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5745.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Age calculated correctly for all nodes\n",
      "Example age for nodes in block 617169\n",
      "[(5314, 112020), (934, 110767), (3023, 110767), (3452, 110322), (576, 110322), (3436, 109094), (3310, 109094), (4223, 109079), (422, 108849), (1912, 108849)]\n"
     ]
    }
   ],
   "source": [
    "test_ix=-100\n",
    "test_block=blocks[test_ix]\n",
    "measurement='age'\n",
    "g_key=graph_keys[test_ix]\n",
    "block,response_test=graph_measurement((g_key,'age',None,bucket))\n",
    "\n",
    "#Load graph\n",
    "\n",
    "response = s3.Object(bucket_name=bucket, key=g_key).get()\n",
    "g_test=pickle.loads(response['Body'].read())\n",
    "nodes_test=list(g_test.nodes())\n",
    "\n",
    "if response_test==200:\n",
    "    key_test='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/raw_score/'+str(test_block)+'.pkl'\n",
    "    g_age_test_load = s3.Object(bucket_name=bucket, key=key_test).get()\n",
    "    g_age_test = pickle.loads(g_age_test_load['Body'].read())\n",
    "    \n",
    "    test_list=[]\n",
    "    error_nodes=[]\n",
    "    # Test that each node age is calculated correctly by looking at the decisions_df\n",
    "    with tqdm(total=len(nodes_test)) as pbar:\n",
    "        \n",
    "        for n in nodes_test:\n",
    "            \n",
    "            # Find creation block by looking at node0 and node1 columns\n",
    "            firstseenas_node0=decisions_df[decisions_df['node0_id']==n]['open_block'].min()\n",
    "            firstseenas_node1=decisions_df[decisions_df['node1_id']==n]['open_block'].min()\n",
    "            \n",
    "            # Correct for nan values, in case node is not present in either column (make it infinite)\n",
    "            fs_list=[firstseenas_node0,firstseenas_node1]\n",
    "            fs_list=[np.inf if np.isnan(i) else i for i in fs_list]\n",
    "            \n",
    "            # Calculate age\n",
    "            actual_age=block-min(fs_list[0],fs_list[1])\n",
    "            recorded_age=g_age_test[n]\n",
    "            \n",
    "            # Check if ages match and populate test_list accordingly\n",
    "            if recorded_age==actual_age:\n",
    "                test_list.append(1)\n",
    "            else:\n",
    "                test_list.append(0)\n",
    "                error_nodes.append((n,recorded_age,actual_age))\n",
    "                \n",
    "            pbar.update(1)\n",
    "            \n",
    "        # Add up all passed tests\n",
    "        tests_passed=np.array(test_list).sum()\n",
    "\n",
    "    \n",
    "# Print out statements based on test results\n",
    "    if tests_passed==len(nodes_test):\n",
    "        print('Age calculated correctly for all nodes')\n",
    "   \n",
    "    \n",
    "    else: \n",
    "        print('Age failed to be correctly calculated for {} nodes'.format(len(list(g_test.nodes()))-tests_passed))\n",
    "        if len(error_nodes)>10:\n",
    "            print('Some nodes with errors (node,recorded_age,actual_age)')\n",
    "            print(error_nodes[:10])\n",
    "        \n",
    "        else:\n",
    "            print('Some nodes with errors (node,recorded_age,actual_age)')\n",
    "            print(error_nodes[:len(error_nodes)])\n",
    "        \n",
    "    \n",
    "    print('Example age for nodes in block {}'.format(test_block))\n",
    "    print(list(g_age_test.items())[:10])\n",
    "    \n",
    "\n",
    "else:\n",
    "    print('Age was not saved correctly')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Aproximate Node closeness rank**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "---OLD VERSION---\n",
    "def approx_node_closrank (bucket,key,block,prev_block,nodes,p=13.38,estimate_sample=50):\n",
    "                             \n",
    "    \n",
    "    # Download graph and extract nodes\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.resource('s3')\n",
    "    response = s3.Object(bucket_name=bucket, key=key).get()\n",
    "    g=pickle.loads(response['Body'].read())\n",
    "    g_nodes=list(g.nodes())\n",
    "    n=len(g_nodes)\n",
    "                             \n",
    "    # Estimate c_mid for graph\n",
    "    estimation_nodes=random.sample(g_nodes, estimate_sample)\n",
    "    c_mid=np.array([nx.closeness_centrality(g,n) for n in estimation_nodes]).mean()\n",
    "\n",
    "                             \n",
    "    # Calculate closeness centrality for selected nodes in block\n",
    "    \n",
    "    clo_list=np.array([nx.closeness_centrality(g,n) for n in nodes])\n",
    "    \n",
    "    # Aproximate ranking using formula from: https://arxiv.org/pdf/1706.02083.pdf\n",
    "    norm_rank=n+((1-n)/(1+np.power((clo_list/c_mid),p)))\n",
    "    norm_rank=list(norm_rank)\n",
    "    \n",
    "    # Create dictionary with ranking per node\n",
    "    g_dir={node:rank for node,rank in zip(nodes,norm_rank)}\n",
    "    \n",
    "    # Save to S3    \n",
    "    measurement='closeness_approx'\n",
    "    extraction_id=key.split('/')[1].split('_')[0]\n",
    "    key_out='graph_snapshots/'+extraction_id+'_connected/.data_transformations/'+measurement+'/norm_rank/'+block+'.pkl'\n",
    "    pickle_byte_obj = pickle.dumps(g_dir) \n",
    "    response=s3.Object(bucket,key_out).put(Body=pickle_byte_obj)['ResponseMetadata']['HTTPStatusCode']\n",
    "    \n",
    "    \n",
    "    return response\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closeness_approx_rank (s3,bucket,g,block,p=13.38,estimate_sample=50):\n",
    "    \n",
    "    g_nodes=list(g.nodes())\n",
    "    n=len(g_nodes)\n",
    "    \n",
    "    # Download decisions_df\n",
    "    decisions_load = s3.Object(bucket_name=bucket, key='decisions_df.csv').get()\n",
    "    decisions_df=pd.read_csv(io.BytesIO(decisions_load['Body'].read()))\n",
    "    \n",
    "    # Extract nodes involved in decisions from the block \n",
    "    node0_open=decisions_df[decisions_df['open_block']==block]['node0_id'].tolist()\n",
    "    node1_open=decisions_df[decisions_df['open_block']==block]['node1_id'].tolist()\n",
    "    node0_close=decisions_df[decisions_df['close_block']==block]['node0_id'].tolist()\n",
    "    node1_close=decisions_df[decisions_df['close_block']==block]['node1_id'].tolist()\n",
    "    \n",
    "    # Create list of nodes existing in previous graph\n",
    "    nodes=node0_open+node1_open+node0_close+node1_close\n",
    "    nodes=list(set(nodes).intersection(set(g_nodes)))\n",
    "    \n",
    "    # Create list of nodes and set closeness to 0 to nodes that are not present\n",
    "    missing_nodes=list(set(nodes).difference(set(g_nodes)))\n",
    "    missing_clo=list(np.zeros(len(missing_nodes)))\n",
    "                             \n",
    "    # Estimate c_mid for graph by averaging closeness for a sample of nodes\n",
    "    \n",
    "    if len(g_nodes)<=estimate_sample:\n",
    "        estimation_nodes=g_nodes\n",
    "    else:\n",
    "        estimation_nodes=random.sample(g_nodes, estimate_sample)\n",
    "    \n",
    "    c_mid=np.array([nx.closeness_centrality(g,n) for n in estimation_nodes]).mean()\n",
    "\n",
    "                             \n",
    "    # Calculate closeness centrality for selected nodes in block\n",
    "    clo_list=np.array([nx.closeness_centrality(g,n) for n in nodes])\n",
    "    \n",
    "    # Aproximate ranking using formula from: https://arxiv.org/pdf/1706.02083.pdf\n",
    "    norm_rank_array=(n+((1-n)/(1+np.power((clo_list/c_mid),p))))/n\n",
    "    norm_rank=list(norm_rank_array)+missing_clo\n",
    "    \n",
    "    # Update nodes list with missing nodes\n",
    "    nodes=nodes+missing_nodes\n",
    "    \n",
    "    # Create dictionary with ranking per node\n",
    "    g_dir={node:rank for node,rank in zip(nodes,norm_rank)}\n",
    "    \n",
    "    \n",
    "    return g_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Values for all nodes are between 0-1\n",
      "Test Block:518195\n",
      "Nodes in Block:316\n",
      "Standard deviation of measurement: 0.07186561837374655\n",
      "Time elapse: 1.0128343105316162\n",
      "[(5424, 0.8409218063257561), (2537, 0.9846530430732492)]\n"
     ]
    }
   ],
   "source": [
    "# TEST aprox node closeness rank\n",
    "\n",
    "# Select testing Block and extract key/block\n",
    "test_ix=random.choice(range(1,len(blocks))) \n",
    "measurement='closeness_approx_rank'\n",
    "g_key=graph_keys[test_ix]\n",
    "prev_key=graph_keys[test_ix-1]\n",
    "\n",
    "rand_block=blocks[test_ix-1]\n",
    "\n",
    "\n",
    "# Download graph associated to test\n",
    "g_test_key='graph_snapshots/'+str(extraction_id)+'_connected/'+str(rand_block)+'.gpickle'\n",
    "response = s3.Object(bucket_name=bucket, key=g_test_key).get()\n",
    "g_test=pickle.loads(response['Body'].read())\n",
    "nodes_test=list(g_test.nodes())\n",
    "\n",
    "# Run node closeness function\n",
    "start=time.time()\n",
    "block,response_test=graph_measurement((g_key,measurement,None,bucket,prev_key))\n",
    "end=time.time()\n",
    "\n",
    "\n",
    "# Test if function saved result correctly and download result\n",
    "if response_test==200:\n",
    "    key_test='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/norm_rank/'+str(block)+'.pkl'\n",
    "   \n",
    "    g_clo_test_load = s3.Object(bucket_name=bucket, key=key_test).get()\n",
    "    g_clo_test = pickle.loads(g_clo_test_load['Body'].read())\n",
    "    clo_values=[v for k,v in list(g_clo_test.items())]\n",
    "\n",
    "\n",
    "# Test if no. items in dictionary are equal to nodes in graph\n",
    "dic_items=list(g_clo_test.items())\n",
    "print(len(dic_items))\n",
    "\n",
    "\n",
    "\n",
    "# Test that values are >0 and <1\n",
    "range_test=[]\n",
    "for n,v in dic_items:\n",
    "    if v>=0 and v<=1:\n",
    "        range_test.append(1)\n",
    "    else:\n",
    "        range_test.append(0)\n",
    "\n",
    "range_test_passed=np.array(range_test).sum()        \n",
    "\n",
    "if range_test_passed==len(dic_items):\n",
    "    print('Values for all nodes are between 0-1')\n",
    "else:\n",
    "    print('Values for some nodes are outside the [0,1] range')\n",
    "    \n",
    "\n",
    "\n",
    "# Download result from function\n",
    "\n",
    "print('Test Block:{}'.format(rand_block))\n",
    "print('Nodes in Block:{}'.format(len(nodes_test)))\n",
    "print('Standard deviation of measurement: {}'.format(np.std(np.array(clo_values))))\n",
    "print('Time elapse: {}'.format(end-start))\n",
    "print(list(g_clo_test.items()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Aproximate Node closeness rank**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robustness_eff_loss(s3,bucket,g,block,attack_perc):\n",
    "    \n",
    "    \n",
    "    # Get global base efficiency by extracting sp from S3\n",
    "    \n",
    "    print(block)\n",
    "    avg_sp_key='graph_snapshots/1587447789_connected/.data_transformations/avg_short_path/raw_score/'+str(block)+'.pkl'\n",
    "    response = s3.Object(bucket_name=bucket, key=avg_sp_key).get()\n",
    "    avg_sp=pickle.loads(response['Body'].read())\n",
    "    init_efficiency=1/avg_sp\n",
    "    \n",
    "    # Calculate 1% of highest degree nodes\n",
    "    num_nodes=len(g.nodes())\n",
    "    num_top_nodes=max(1,int(attack_perc*num_nodes))\n",
    "    g_degrees=[deg for deg in g.degree()]\n",
    "    top_degrees=sorted(g_degrees,key=lambda y: y[1],reverse=True)[:num_top_nodes]\n",
    "    top_nodes=[n for n,d in top_degrees]\n",
    "    \n",
    "    # Remove top nodes \n",
    "    g_pruned=g.copy()\n",
    "    g_pruned.remove_nodes_from(top_nodes)\n",
    "    \n",
    "    # Re-calculate efficiency for pruened graph and robustness\n",
    "    final_efficiency=nx.global_efficiency(g_pruned)\n",
    "    robustness=final_efficiency/init_efficiency\n",
    "    \n",
    "    \n",
    "    return robustness\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph_snapshots/1587447789_connected/591968.gpickle\n",
      "591968\n"
     ]
    }
   ],
   "source": [
    "# TEST robustness_eff_loss\n",
    "\n",
    "# Select testing Block and extract key/block\n",
    "test_ix=random.choice(range(1,len(sample_keys))) \n",
    "\n",
    "\n",
    "measurement='robustness_eff_loss'\n",
    "g_test_key=sample_keys[test_ix]\n",
    "#prev_key=graph_keys[test_ix-1]\n",
    "\n",
    "print(g_test_key)\n",
    "\n",
    "\n",
    "# Download graph associated to test\n",
    "\n",
    "response = s3.Object(bucket_name=bucket, key=g_test_key).get()\n",
    "g_test=pickle.loads(response['Body'].read())\n",
    "nodes_test=list(g_test.nodes())\n",
    "rand_block=g_test.graph['block']\n",
    "\n",
    "# Run robustness_eff_loss\n",
    "start=time.time()\n",
    "block,response_test=graph_measurement((g_test_key,measurement,None,bucket,None))\n",
    "end=time.time()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph_snapshots/1587447789_connected/.data_transformations/robustness_eff_loss_1.0/raw_score/591968.pkl\n",
      "Test Block:591968\n",
      "Nodes in Block:5184\n",
      "Time elapse: 76.71983003616333\n",
      "Sample: 0.4836248242892463\n"
     ]
    }
   ],
   "source": [
    "# Test if function saved result correctly and download result\n",
    "if response_test==200:\n",
    "    key_test='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'_1.0/raw_score/'+str(block)+'.pkl'\n",
    "    print(key_test)\n",
    "    g_rob_test_load = s3.Object(bucket_name=bucket, key=key_test).get()\n",
    "    g_rob_test = pickle.loads(g_rob_test_load['Body'].read())\n",
    "    \n",
    "    \n",
    "\n",
    "print('Test Block:{}'.format(block))\n",
    "print('Nodes in Block:{}'.format(len(nodes_test)))\n",
    "print('Time elapse: {}'.format(end-start))   \n",
    "print('Sample: {}'.format(g_rob_test))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Property measurment for a couple of nodes in a graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "node_measurement\n",
    "    Performs selected graph measurment on specific nodes in graph\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "g : nx graph\n",
    "    NetworkX graph object over which the measurment will be performed\n",
    "\n",
    "measurement: string\n",
    "    Type of measurement to be performend in graph\n",
    "    \n",
    "node0: int\n",
    "    Node id for node 0\n",
    "\n",
    "node1: int\n",
    "    Node id for node 1\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "node_tuple: tuple\n",
    "    Tuple of the form (node0_mes,node1_mes)\n",
    "    \n",
    "    node0_mes: float\n",
    "        Graph measurement for node0\n",
    "    node1_mes: float\n",
    "        Graph measurement for node1\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "def node_measurement(g,measurement,node0,node1):\n",
    "    \n",
    "    measurement_input=(g,measurement,'capacity')\n",
    "    block,g_dir=graph_measurement(measurement_input)\n",
    "    \n",
    "    node0_mes=g_dir[node0]\n",
    "    node1_mes=g_dir[node1]\n",
    "        \n",
    "    # Update marginal values for node0 and node1\n",
    "        \n",
    "    if (g.has_node(node0)): #If connected component of marginal graph contains node0 find betweeness\n",
    "        node0_mes=g_dir[node0]\n",
    "    else: # else update with fixed value\n",
    "        node0_mes=0\n",
    "            \n",
    "    if (g.has_node(node1)): #If connected component of marginal graph contains node1 find betweeness\n",
    "        node1_mes=g_dir[node1]\n",
    "    else: # else update with fixed value\n",
    "        node1_mes=0\n",
    "    \n",
    "    return (node0_mes,node1_mes)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incremental_closeness(bucket,graph_keys,blocks,start_point):\n",
    "    \n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.resource('s3')\n",
    "    counter=0\n",
    "    extraction_id=graph_keys[0].split('/')[1].split('_')[0]\n",
    "    responses=[]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Channel closures\n",
    "    closure_file = s3.Object(bucket_name=bucket, key='channel_closures.p').get()\n",
    "    channel_closures = pickle.loads(closure_file['Body'].read())\n",
    "    \n",
    "    \n",
    "    # Channel openings \n",
    "    opens_file = s3.Object(bucket_name=bucket, key='channel_opens.p').get()\n",
    "    channel_opens = pickle.loads(opens_file['Body'].read())\n",
    "    snapshot_clo={}\n",
    "    \n",
    "    #Initialize graph with all nodes \n",
    "    lastgraph_block=blocks[-1]\n",
    "    last_key='graph_snapshots/'+str(extraction_id)+'/'+str(lastgraph_block)+'.gpickle'\n",
    "    response = s3.Object(bucket_name=bucket, key=last_key).get()\n",
    "    G_final=pickle.loads(response['Body'].read())\n",
    "    nodes_final=list(G_final.nodes())\n",
    "    G=nx.Graph()\n",
    "    G.add_nodes_from(nodes_final)\n",
    "    prev_clo=None\n",
    "    \n",
    "    if start_point>0:\n",
    "        # Get previous graph\n",
    "        inigraph_block=blocks[start_point-1]\n",
    "        ini_key='graph_snapshots/'+str(extraction_id)+'/'+str(inigraph_block)+'.gpickle'\n",
    "        response = s3.Object(bucket_name=bucket, key=ini_key).get()\n",
    "        G_ini=pickle.loads(response['Body'].read())\n",
    "        \n",
    "        # Get previous closeness centrality\n",
    "        measurement='incremental_closeness'\n",
    "        key_clo='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/raw_score/'+str(inigraph_block)+'.pkl'\n",
    "        response = s3.Object(bucket_name=bucket, key=key_clo).get()\n",
    "        prev_clo=pickle.loads(response['Body'].read())\n",
    "      \n",
    "        G.add_edges_from(list(G_ini.edges(data=True)))\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    with tqdm(total=len(range(start_point,len(graph_keys)))) as pbar:\n",
    "        for i  in range(start_point,len(graph_keys)):\n",
    "\n",
    "            \n",
    "            block=blocks[i]\n",
    "            new_edges=channel_opens[block]\n",
    "            closed_edges=channel_closures[block]\n",
    "        \n",
    "\n",
    "            # Incremental closeness calculation for OPENS\n",
    "\n",
    "            with tqdm(total=len(new_edges),disable=True) as pbar1:\n",
    "                for edge in new_edges:\n",
    "\n",
    "                    #If edge exists in previous graph closeness doesn't change\n",
    "                    if G.has_edge(edge[0],edge[1]):\n",
    "                        new_clo=prev_clo\n",
    "                        G.edges[edge[0],edge[1]]['no_channels']+=1\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        new_clo=nx.incremental_closeness_centrality(G,(edge[0],edge[1]),prev_cc=prev_clo,insertion=True)\n",
    "                        G.add_edges_from([edge])\n",
    "                \n",
    "\n",
    "                    prev_clo=new_clo\n",
    "                    pbar1.update(1)\n",
    "                \n",
    "            \n",
    "\n",
    "            # Incremental closeness calculation for CLOSES\n",
    "\n",
    "            with tqdm(total=len(closed_edges),disable=True) as pbar2:\n",
    "                for edge in closed_edges:\n",
    "\n",
    "                    # Verify if existing edges result from multiple channels, if so, only reduce capacity otherwise remove edge\n",
    "                    no_channels=G.edges[edge[0],edge[1]]['no_channels']\n",
    "\n",
    "                    #If edge exists in previous graph closeness doesn't change\n",
    "                    if no_channels>1:\n",
    "                        new_clo=prev_clo\n",
    "                        G.edges[edge[0],edge[1]]['no_channels']-=1\n",
    "\n",
    "                    else:                                    \n",
    "                        new_clo=nx.incremental_closeness_centrality(G,(edge[0],edge[1]),prev_cc=prev_clo,insertion=False)                   \n",
    "                        G.remove_edge(edge[0],edge[1])\n",
    "\n",
    "                    prev_clo=new_clo\n",
    "                    pbar2.update(1)\n",
    "                \n",
    "\n",
    "\n",
    "            # Safe outcome\n",
    "            g_dir=new_clo\n",
    "\n",
    "\n",
    "            # Safe graph processing to S3\n",
    "            \n",
    "            measurement='incremental_closeness'\n",
    "            key_out='graph_snapshots/'+extraction_id+'_connected/.data_transformations/'+measurement+'/raw_score/'+str(block)+'.pkl'\n",
    "            pickle_byte_obj = pickle.dumps(g_dir) \n",
    "            response=s3.Object(bucket,key_out).put(Body=pickle_byte_obj)['ResponseMetadata']['HTTPStatusCode']\n",
    "            #print((block,response))\n",
    "\n",
    "\n",
    "            # Loop updates\n",
    "            pbar1.close()\n",
    "            pbar2.close()\n",
    "            pbar.update(1)\n",
    "            responses.append(response)\n",
    "            \n",
    "        \n",
    "        output={b:res for b,res in zip(blocks,responses)}\n",
    "\n",
    "           \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit sigmoid for p by looking at sample of graphs\n",
    "# estimate mid closeness by samplig closeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sp_closeness(G):\n",
    "    \n",
    "    nodes=list(G.nodes())\n",
    "    n=len(nodes)\n",
    "    sp_matrix=np.zeros((len(nodes),len(nodes)))\n",
    "    sp_matrix=sp_matrix.astype(object)\n",
    "    print(sp_matrix.dtype)\n",
    "    \n",
    "    dic_clo={}\n",
    "    \n",
    "    \n",
    "    for i in range(n):\n",
    "\n",
    "        for j in range(i,n):\n",
    "            # Calculate delayed shortest path\n",
    "            input_tuple=(G,nodes[i],nodes[j])\n",
    "            sp=dask.delayed(len_shortest_path)(input_tuple)\n",
    "            #print(type(sp))\n",
    "            sp_matrix[i][j]=sp\n",
    "            sp_matrix[j][i]=sp\n",
    "        \n",
    "        sp_sum=np.array(dask.compute(*sp_matrix[i].tolist())).sum()\n",
    "        clo_i=n-1/sp_sum\n",
    "        dic_clo[i]=clo_i\n",
    "    return dic_clo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "start=time.time()\n",
    "dic_clo_test=sp_closeness(G_test)\n",
    "end=time.time()\n",
    "print(sp)\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print(list(dic_clo_test.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute in seconds: 10.438627481460571\n"
     ]
    }
   ],
   "source": [
    "snapshot_nodes=collection_measure(bucket,extract_keys,'node_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(snapshot_nodes.items())[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Baseline Current Betweeness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute in seconds: 8009.403836488724\n"
     ]
    }
   ],
   "source": [
    "# Calculate Baseline betweeness\n",
    "snapshot_bet=collection_measure(bucket,extract_keys,'current_betweeness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab210d796de43c4b30280d4e42d8068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=36535.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compute in seconds: 12896.576698541641\n"
     ]
    }
   ],
   "source": [
    "# Calculate Baseline unweighted betweeness\n",
    "snapshot_bet_uw=collection_measure(bucket,extract_keys,'current_betweeness_unweighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save to S3 succesful\n"
     ]
    }
   ],
   "source": [
    "# Save baseline current betweeness to S3: For old calculation of Betweeness\n",
    "response=pickle_save_s3(snapshot_bet,blocks,extraction_id,'snapshot_bet')\n",
    "if response['HTTPStatusCode']==200:\n",
    "    print('Save to S3 succesful')\n",
    "else:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to single file per graph format\n",
    "# Load large dic from S3\n",
    "key='graph_snapshots/1587447789_connected/.data_transformations/1587447789snapshot_bet-36536-508400-617297.pkl'\n",
    "response = s3.Object(bucket_name=bucket, key=key).get()\n",
    "snapshot_bet=pickle.loads(response['Body'].read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c48118dd15a4defb5af75a6461ab58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=36536.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Delayed store function\n",
    "\n",
    "responses=[]\n",
    "with tqdm(total=len(blocks)) as pbart:\n",
    "    for block in blocks:\n",
    "        dic=snapshot_bet[block]\n",
    "        #create save key\n",
    "        measurement='betweeness_curr_aprox'\n",
    "        key_out='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/raw_score/'+str(block)+'.pkl'\n",
    "\n",
    "        #run save function\n",
    "        input_tuple=(bucket,key_out,dic)\n",
    "        response=simple_psave_s3(input_tuple)\n",
    "        responses.append(response)\n",
    "        pbart.update(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "betweeness_curr_aprox was calculated for ALL 36536 blocks correctly\n",
      "Example of betweeness_curr_aprox saved for block 607627:\n",
      "[(5314, 2.7627499157415683e-05), (934, 3.5404153621536765e-05), (3023, 7.752852808574474e-15), (3452, 4.186574492807752e-06), (576, 2.7889615061953227e-16), (3436, 0.0013869522957864274), (3310, 1.6268942119472713e-15), (2378, 5.268038400591164e-15), (4223, 0.0007224518380271746), (422, 0.0005759993487047296)]\n",
      "Total entries: 5506\n",
      "Number of nodes match\n"
     ]
    }
   ],
   "source": [
    "# TEST all saved correctly\n",
    "#responses=np.array([r for b,r in snapshot_capgrowth.items()])\n",
    "\n",
    "measurement='betweeness_curr_aprox' \n",
    "# Retrieve saved items\n",
    "\n",
    "bet_cur_keys = [obj.key \n",
    "        for obj in s3.Bucket(name=bucket).objects.all()\n",
    "        if re.match(\".*\"+str(extraction_id)+\"_connected/\\.data_transformations/betweeness_curr_aprox/.*\\.pkl\",obj.key)]\n",
    "\n",
    "if len(bet_cur_keys)==len(blocks):\n",
    "    print('{} was calculated for ALL {} blocks correctly'.format(measurement,len(bet_cur_keys)))\n",
    "else:\n",
    "    print('Error saving to S3')\n",
    "    \n",
    "  \n",
    "rand_block=str(random.choice(blocks))    \n",
    "test_key='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/raw_score/'+rand_block+'.pkl'\n",
    "test_file = s3.Object(bucket_name=bucket, key=test_key).get()\n",
    "test_object = pickle.loads(test_file['Body'].read())\n",
    "print('Example of {} saved for block {}:'.format(measurement,rand_block))\n",
    "print(list(test_object.items())[:10])\n",
    "print('Total entries: {}'.format(len(list(test_object.items()))))\n",
    "\n",
    "# Get test graph\n",
    "g_test_key='graph_snapshots/'+str(extraction_id)+'_connected/'+str(rand_block)+'.gpickle'\n",
    "response = s3.Object(bucket_name=bucket, key=g_test_key).get()\n",
    "g_test=pickle.loads(response['Body'].read())\n",
    "\n",
    "# Run test function on test graph: Check that dic stored has same number of nodes than graph\n",
    "\n",
    "nodes_gtest=list(g_test.nodes())\n",
    "items_dict_test=list(test_object.items())\n",
    "\n",
    "if len(nodes_gtest)==len(items_dict_test):\n",
    "    print('Number of nodes match')\n",
    "else:\n",
    "    print('Number of nodes does NOT match')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Baseline Current Closeness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Baseline current closeness\n",
    "#snapshot_clo=collection_measure(bucket,extract_keys,'current_closeness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate incremental closeness\n",
    "snapshot_clo=incremental_closeness(bucket,extract_keys,blocks,1425+3073)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graph_snapshots/1587447789_connected/535029.gpickle'"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_keys[1402]\n",
    "\n",
    "#channel_opens[1407]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "key='graph_snapshots/1587447789_connected/'+str(blocks[-100])+'.gpickle'\n",
    "response = s3.Object(bucket_name=bucket, key=key).get()\n",
    "G_test=pickle.loads(response['Body'].read())\n",
    "\n",
    "node1=random.choice(list(G.nodes()))\n",
    "node2=random.choice(list(G.nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6306, 2470)\n",
      "0\n",
      "Compute in seconds: 9.965896606445312e-05\n"
     ]
    }
   ],
   "source": [
    "print((node1,node2))\n",
    "#len(G_test.nodes())\n",
    "start=time.time()\n",
    "sp=nx.shortest_path_length(G_test, source=node1, target=node1)\n",
    "end=time.time()\n",
    "print(sp)\n",
    "print('Compute in seconds: {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0.2, 2: 0.2, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0}\n",
      "[(1, 2)]\n",
      "{1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2, 5: 0.0, 6: 0.0}\n",
      "[(1, 2), (3, 4)]\n",
      "{1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2, 5: 0.2, 6: 0.2}\n",
      "[(1, 2), (3, 4), (5, 6)]\n",
      "{1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2, 5: 0.0, 6: 0.0}\n",
      "[(1, 2), (3, 4)]\n"
     ]
    }
   ],
   "source": [
    "G=nx.Graph()\n",
    "G.add_nodes_from([1,2,3,4,5,6])\n",
    "#G.add_edge(2,1)\n",
    "new_clo=nx.incremental_closeness_centrality(G,(2,1),prev_cc=None,insertion=True)\n",
    "G.add_edge(2,1)\n",
    "print(new_clo)\n",
    "print(G.edges())\n",
    "\n",
    "\n",
    "new_clo=nx.incremental_closeness_centrality(G,(3,4),prev_cc=new_clo,insertion=True)\n",
    "G.add_edge(3,4)\n",
    "print(new_clo)\n",
    "print(G.edges())\n",
    "\n",
    "\n",
    "new_clo=nx.incremental_closeness_centrality(G,(5,6),prev_cc=new_clo,insertion=True)\n",
    "G.add_edge(5,6)\n",
    "print(new_clo)\n",
    "print(G.edges())\n",
    "\n",
    "\n",
    "new_clo=nx.incremental_closeness_centrality(G,(5,6),prev_cc=new_clo,insertion=False)\n",
    "G.remove_edge(5,6)\n",
    "\n",
    "print(new_clo)\n",
    "print(G.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save to S3 succesful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec1dca6cf3d443abad6f463e8fad8fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1400.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "% of nodes with positive closeness per Block : 0.9888914970559514\n",
      "Total Blocks: 1400\n"
     ]
    }
   ],
   "source": [
    "# TEST all saved correctly\n",
    "responses=np.array([r for b,r in snapshot_clo.items()])\n",
    "\n",
    "if np.amax(responses)==np.amin(responses): #and len(responses) == len(extract_keys):\n",
    "    print('Save to S3 succesful')\n",
    "else:\n",
    "    print('Error saving to S3')\n",
    "\n",
    "range_test=[]\n",
    "no_blocks=1400\n",
    "    \n",
    "with tqdm(total=no_blocks) as pbar:\n",
    "    for i in range(no_blocks):\n",
    "\n",
    "        measurement='incremental_closeness'    \n",
    "        #rand_block=str(random.choice(blocks[:1200]))    \n",
    "        rand_block=str(blocks[i])\n",
    "\n",
    "        test_key='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/raw_score/'+rand_block+'.pkl'\n",
    "        test_file = s3.Object(bucket_name=bucket, key=test_key).get()\n",
    "        test_object = pickle.loads(test_file['Body'].read())\n",
    "\n",
    "        graph_key='graph_snapshots/'+str(extraction_id)+'_connected/'+str(rand_block)+'.gpickle'\n",
    "        graph_response = s3.Object(bucket_name=bucket, key=graph_key).get()\n",
    "        G=pickle.loads(graph_response['Body'].read())\n",
    "\n",
    "\n",
    "        #print('Example of {} saved for block {}:'.format(measurement,rand_block))\n",
    "        #print(list(test_object.items())[:100]) \n",
    "        #print('Total nodes graph: {}'.format(len(G.nodes())))\n",
    "\n",
    "        connected_nodes=[(b,cent) for b,cent in list(test_object.items()) if cent>0]\n",
    "        #print('Total entries: {}'.format(len(connected_nodes)))\n",
    "\n",
    "        node_cons=[]\n",
    "        for node in list(G.nodes()):\n",
    "\n",
    "            node_con=test_object[node]\n",
    "            \n",
    "          \n",
    "            if node_con>0:\n",
    "                node_cons.append(1)\n",
    "            else:\n",
    "                node_cons.append(0)\n",
    "            \n",
    "            \n",
    "\n",
    "        block_test=np.array(node_cons).sum()/len(G.nodes)          \n",
    "\n",
    "        '''\n",
    "        if block_test==1:\n",
    "            test=1\n",
    "        else:\n",
    "            test=0\n",
    "        '''\n",
    "\n",
    "        range_test.append(block_test)\n",
    "        #range_test.append(test)\n",
    "        pbar.update(1)\n",
    "\n",
    "print('% of nodes with positive closeness per Block : {}'.format(np.array(range_test).mean()))\n",
    "#print('Blocks with closeness for all nodes: {}'.format(np.array(range_test).sum()))\n",
    "print('Total Blocks: {}'.format(no_blocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9950617283950617"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(range_test)[1399]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Baseline Channels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute in seconds: 186.33308386802673\n"
     ]
    }
   ],
   "source": [
    "# Calculate Baseline degree\n",
    "snapshot_channels=collection_measure(bucket,extract_keys,'channels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST all saved correctly\n",
    "responses=np.array([r for b,r in snapshot_channels.items()])\n",
    "\n",
    "if np.amax(responses)==np.amin(responses) and len(responses) == len(extract_keys):\n",
    "    print('Save to S3 succesful')\n",
    "else:\n",
    "    print('Error saving to S3')\n",
    "    \n",
    "measurement='channels'    \n",
    "rand_block=str(random.choice(blocks))    \n",
    "test_key='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/raw_score/'+rand_block+'.pkl'\n",
    "test_file = s3.Object(bucket_name=bucket, key=test_key).get()\n",
    "test_object = pickle.loads(test_file['Body'].read())\n",
    "print('Example of {} saved for block {}:'.format(measurement,rand_block))\n",
    "print(list(test_object.items())[:10])    \n",
    "print('Total entries: {}'.format(len(list(test_object.items()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Baseline Capacity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute in seconds: 205.32206177711487\n"
     ]
    }
   ],
   "source": [
    "# Calculate Baseline degree\n",
    "snapshot_capacity=collection_measure(bucket,extract_keys,'capacity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save to S3 succesful\n",
      "Example of capacity saved for block 586042:\n",
      "[(6038, 8878679), (5314, 14026340), (934, 1171934), (3023, 1111934), (3452, 2063908), (576, 40000), (3436, 6948131), (3310, 100000), (2378, 400000), (4223, 24298841)]\n",
      "Total entries: 4920\n"
     ]
    }
   ],
   "source": [
    "# TEST all saved correctly\n",
    "responses=np.array([r for b,r in snapshot_capacity.items()])\n",
    "\n",
    "if np.amax(responses)==np.amin(responses) and len(responses) == len(extract_keys):\n",
    "    print('Save to S3 succesful')\n",
    "else:\n",
    "    print('Error saving to S3')\n",
    "    \n",
    "measurement='capacity'    \n",
    "rand_block=str(random.choice(blocks))    \n",
    "test_key='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/raw_score/'+rand_block+'.pkl'\n",
    "test_file = s3.Object(bucket_name=bucket, key=test_key).get()\n",
    "test_object = pickle.loads(test_file['Body'].read())\n",
    "print('Example of {} saved for block {}:'.format(measurement,rand_block))\n",
    "print(list(test_object.items())[:10])\n",
    "print('Total entries: {}'.format(len(list(test_object.items()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Baseline Age**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5b44ee0cd944909ff8de4320c225b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=36536.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compute in seconds: 511.2060635089874\n"
     ]
    }
   ],
   "source": [
    "# Calculate Baseline age\n",
    "snapshot_age=collection_measure(bucket,extract_keys,'age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save to S3 succesful\n",
      "Example of age saved for block 568893:\n",
      "[(6038, 63744), (5314, 63744), (934, 62491), (3023, 62491), (3452, 62046), (576, 62046), (3436, 60818), (3310, 60818), (2378, 60803), (4223, 60803)]\n",
      "Total entries: 3872\n"
     ]
    }
   ],
   "source": [
    "# TEST all saved correctly\n",
    "responses=np.array([r for b,r in snapshot_age.items()])\n",
    "\n",
    "if np.amax(responses)==np.amin(responses) and len(responses) == len(extract_keys):\n",
    "    print('Save to S3 succesful')\n",
    "else:\n",
    "    print('Error saving to S3')\n",
    "    \n",
    "measurement='age'    \n",
    "rand_block=str(random.choice(blocks))    \n",
    "test_key='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/raw_score/'+rand_block+'.pkl'\n",
    "test_file = s3.Object(bucket_name=bucket, key=test_key).get()\n",
    "test_object = pickle.loads(test_file['Body'].read())\n",
    "print('Example of {} saved for block {}:'.format(measurement,rand_block))\n",
    "print(list(test_object.items())[:10])\n",
    "print('Total entries: {}'.format(len(list(test_object.items()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Baseline Growth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Baseline capacity growth\n",
    "snapshot_capgrowth=collection_measure(bucket,extract_keys,'capacity_growth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save to S3 succesful\n",
      "Example of capacity_growth saved for block 603179:\n",
      "[(5314, 0), (934, 0), (3023, 0), (3452, 0), (576, 0), (3436, -12000), (3310, 0), (2378, 0), (4223, 500000), (422, 0)]\n",
      "Total entries: 5338\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba8263bf8cc47e490cfc7d960ea1f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5338.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Growth calculated correctly for all nodes\n"
     ]
    }
   ],
   "source": [
    "# TEST all saved correctly\n",
    "responses=np.array([r for b,r in snapshot_capgrowth.items()])\n",
    "\n",
    "if np.amax(responses)==np.amin(responses) and len(responses) == len(extract_keys):\n",
    "    print('Save to S3 succesful')\n",
    "else:\n",
    "    print('Error saving to S3')\n",
    "    \n",
    "measurement='capacity_growth'    \n",
    "rand_block=str(random.choice(blocks))    \n",
    "test_key='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/raw_score/'+rand_block+'.pkl'\n",
    "test_file = s3.Object(bucket_name=bucket, key=test_key).get()\n",
    "test_object = pickle.loads(test_file['Body'].read())\n",
    "print('Example of {} saved for block {}:'.format(measurement,rand_block))\n",
    "print(list(test_object.items())[:10])\n",
    "print('Total entries: {}'.format(len(list(test_object.items()))))\n",
    "\n",
    "# Get test graph\n",
    "g_test_key='graph_snapshots/'+str(extraction_id)+'_connected/'+str(rand_block)+'.gpickle'\n",
    "response = s3.Object(bucket_name=bucket, key=g_test_key).get()\n",
    "g_test=pickle.loads(response['Body'].read())\n",
    "\n",
    "# Run test function on test graph\n",
    "test_growth(g_test,test_object,int(rand_block),channel_opens,channel_closures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Baseline Closeness rank**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate closeness rank before making block decisions\n",
    "snapshot_clorank=collection_measure(bucket,extract_keys,'closeness_approx_rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "074fb9ab82bc426797cf3345fc1270d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=36535.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate closeness rank after making block decisions\n",
    "snapshot_clorank=collection_measure(bucket,extract_keys,'closeness_approx_rank_post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample Blocks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3654\n"
     ]
    }
   ],
   "source": [
    "sample_keys=[extract_keys[10*i] for i in range(int(len(extract_keys)/10)+1)]\n",
    "print(len(sample_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['graph_snapshots/1587447789_connected/508400.gpickle',\n",
       " 'graph_snapshots/1587447789_connected/509496.gpickle',\n",
       " 'graph_snapshots/1587447789_connected/511852.gpickle',\n",
       " 'graph_snapshots/1587447789_connected/513758.gpickle',\n",
       " 'graph_snapshots/1587447789_connected/514060.gpickle',\n",
       " 'graph_snapshots/1587447789_connected/514345.gpickle',\n",
       " 'graph_snapshots/1587447789_connected/514412.gpickle',\n",
       " 'graph_snapshots/1587447789_connected/514703.gpickle',\n",
       " 'graph_snapshots/1587447789_connected/514850.gpickle',\n",
       " 'graph_snapshots/1587447789_connected/515162.gpickle']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_keys[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Average Shortest path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_avg_shortpath=collection_measure(bucket,sample_keys,'avg_short_path')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Robustness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_robustness=collection_measure(bucket,sample_keys,'robustness_eff_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparissons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Compare property changes for nodes in a collection of graphs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###NEW \n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "collection_compare\n",
    "    Iterates over blocks to calculate marginal change in metric for nodes that made decisions (opens/closures)\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "blocks : list\n",
    "    List of blocks extracted when reading graphs\n",
    "    \n",
    "    \n",
    "dec_dic: dic\n",
    "    Dictionary with channel decisions (open or closure) per block\n",
    "    \n",
    "    \n",
    "graph_snapshots: list\n",
    "    List of delayed dask objects each pointing to a graph snapshot to be loaded from S3\n",
    "\n",
    "snapshots_base: dic\n",
    "    Dictionary of dictionaries containing the base measurments per block per node\n",
    "\n",
    "measurement: string\n",
    "    Name of the type of measurment that will applied to the graph\n",
    "    \n",
    "type_dec: string\n",
    "    The type of decisions that will be analyzed 'opens' or 'closures'\n",
    "    \n",
    "\n",
    "Returns\n",
    "-------\n",
    "futures: list\n",
    "    List of tuples of the form (dic_node0,dic_node1) where dic_node0/1 is a dictionary containing the marginal changes for node0/1 \n",
    "    for every decision in a given block. The dictionaries are future dask objects that still need to be explicitly computed. \n",
    "\n",
    "\"\"\"\n",
    "def collection_compare(blocks,dec_dic,graph_keys,snapshots_base,measurement,type_dec):\n",
    "\n",
    "    futures_list=[] # list to populate with futures per block\n",
    "    \n",
    "    with tqdm(total=len(range(1,len(graph_keys)))) as pbar:\n",
    "        for i in range(1,len(graph_keys)): # iterate through blocks\n",
    "\n",
    "            # extract information from parameters and construct input tuple to delayed function\n",
    "            block=blocks[i]\n",
    "            block_prev=blocks[i-1]\n",
    "            block_dec=dec_dic[block]\n",
    "            key=graph_keys[i-1] # Pass previous graph\n",
    "            block_base=snapshots_base[block_prev]\n",
    "            block_res=snapshots_base[block]\n",
    "            input_tuple=(block,g,block_dec,block_base,measurement,type_dec,block_res)\n",
    "            \n",
    "            # submit to delayed function and append to list\n",
    "            output_tuple=dask.delayed(graph_compare)(input_tuple)\n",
    "            futures_list.append(output_tuple)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # persist to disk and return\n",
    "    futures = dask.persist(*futures_list)\n",
    "    return futures\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "collection_compare\n",
    "    Iterates over blocks to calculate marginal change in metric for nodes that made decisions (opens/closures)\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "blocks : list\n",
    "    List of blocks extracted when reading graphs\n",
    "    \n",
    "    \n",
    "dec_dic: dic\n",
    "    Dictionary with channel decisions (open or closure) per block\n",
    "    \n",
    "    \n",
    "graph_snapshots: list\n",
    "    List of delayed dask objects each pointing to a graph snapshot to be loaded from S3\n",
    "\n",
    "snapshots_base: dic\n",
    "    Dictionary of dictionaries containing the base measurments per block per node\n",
    "\n",
    "measurement: string\n",
    "    Name of the type of measurment that will applied to the graph\n",
    "    \n",
    "type_dec: string\n",
    "    The type of decisions that will be analyzed 'opens' or 'closures'\n",
    "    \n",
    "\n",
    "Returns\n",
    "-------\n",
    "futures: list\n",
    "    List of tuples of the form (dic_node0,dic_node1) where dic_node0/1 is a dictionary containing the marginal changes for node0/1 \n",
    "    for every decision in a given block. The dictionaries are future dask objects that still need to be explicitly computed. \n",
    "\n",
    "\n",
    "def collection_compare(blocks,dec_dic,graph_snapshots,snapshots_base,measurement,type_dec):\n",
    "\n",
    "    futures_list=[] # list to populate with futures per block\n",
    "    \n",
    "    with tqdm(total=len(range(1,len(graph_snapshots)))) as pbar:\n",
    "        for i in range(1,len(graph_snapshots)): # iterate through blocks\n",
    "\n",
    "            # extract information from parameters and construct input tuple to delayed function\n",
    "            block=blocks[i]\n",
    "            block_prev=blocks[i-1]\n",
    "            block_dec=dec_dic[block]\n",
    "            g=graph_snapshots[i-1] # Pass previous graph\n",
    "            block_base=snapshots_base[block_prev]\n",
    "            block_res=snapshots_base[block]\n",
    "            input_tuple=(block,g,block_dec,block_base,measurement,type_dec,block_res)\n",
    "            \n",
    "            # submit to delayed function and append to list\n",
    "            output_tuple=dask.delayed(graph_compare)(input_tuple)\n",
    "            futures_list.append(output_tuple)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # persist to disk and return\n",
    "    futures = dask.persist(*futures_list)\n",
    "    return futures\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Compare property changes for nodes in a graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "graph_compare\n",
    "    Calculates marginal change in metric for node0, node1 make decisions (open/close channels) in a single block\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "input_tuple : tuple\n",
    "    \n",
    "    block: int\n",
    "        Block number\n",
    "    g: nx_graph \n",
    "        Graph snapshot (as dask delayed object)\n",
    "    block_dec: list\n",
    "        List of tuples in nx edge format (u,v,att_dic) for all the decisions (channel opens/closures) made in that block  \n",
    "    block_base: dic\n",
    "        Dictionary of base measurements for each node in the graph snapshot corresponding to that block\n",
    "    measurement: string\n",
    "        Name of measurement to be computed\n",
    "    type_dec: string\n",
    "    The type of decisions that will be analyzed 'opens' or 'closures'\n",
    "    \n",
    "    block_res: dic\n",
    "        Dictionary of base measurements for each node in the graph snapshot corresponding to the next block\n",
    "    \n",
    "\n",
    "Returns\n",
    "-------\n",
    "nodes_mar_dic: tuple\n",
    "    Tuples of the form (mar_node0_dic_i,mar_node0_dic_i) where each element in the tuple is a dictionary containing the marginal changes for node0/1 \n",
    "    for every node0 and node1 involved in a decision (channel open/closures) in the block.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def graph_compare(input_tuple):\n",
    "    \n",
    "    block=input_tuple[0]\n",
    "    key=input_tuple[1]\n",
    "    block_dec=input_tuple[2]\n",
    "    block_base=input_tuple[3]\n",
    "    measurement=input_tuple[4]\n",
    "    type_dec=input_tuple[5]\n",
    "    block_res=input_tuple[6]\n",
    "   \n",
    "    mar_node0_dic_i={} # dictionary to story function output\n",
    "    mar_node1_dic_i={} \n",
    "    \n",
    "    \n",
    "    # Load data\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.resource('s3')\n",
    "    response = s3.Object(bucket_name=bucket, key=key).get()\n",
    "    G=pickle.loads(response['Body'].read())\n",
    "    \n",
    "    ###########################---------------------\n",
    "    ##if decisiono \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # For each decision calculate marginal change in measure for node0 and node1\n",
    "    for edge in block_dec:\n",
    "        \n",
    "        # Extract info about channel\n",
    "        \n",
    "        node0=edge[0]\n",
    "        node1=edge[1]\n",
    "        channel_id=edge[2]['channel_id']\n",
    "        capacity=edge[2]['capacity']\n",
    "\n",
    "        \n",
    "        #Â Copy original graph\n",
    "        g_mar=G.copy()   \n",
    "        old_nodes=False\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Retrieve base measurement before channel if nodes existed, else define base measure as 0\n",
    "        if (g_mar.has_node(node0)):\n",
    "            node0_base=block_base[node0]\n",
    "            old_nodes=True\n",
    "        else:\n",
    "            node0_base=0\n",
    "            \n",
    "        if (g_mar.has_node(node1)):\n",
    "            node1_base=block_base[node1]\n",
    "            old_nodes=True\n",
    "        else:\n",
    "            node1_base=0\n",
    "        \n",
    "            \n",
    "        if old_nodes: # If at least one node is old (part of the connected graph)\n",
    "            \n",
    "            if type_dec=='mar_opens': # marginal calculation for opens\n",
    "                \n",
    "                \n",
    "                # Define and add edges and calculate betweeness if at least one of the nodes is in graph \n",
    "                edge_list=[edge]\n",
    "                \n",
    "                \n",
    "                # If channel exists increase capacity\n",
    "                \n",
    "                if g_mar.has_edge(node0,node1):\n",
    "                   \n",
    "                    g_mar.edges[node0,node1]['capacity']+=capacity\n",
    "                    g_mar.edges[node0,node1]['no_channels']+=1\n",
    "\n",
    "                else:\n",
    "                    g_mar.add_edges_from(edge_list)\n",
    "                \n",
    "                \n",
    "                g_mar_mes=node_measurement(g_mar,measurement,node0,node1)\n",
    "                \n",
    "                # Update measurement values after marginal change\n",
    "                node0_new_mes=g_mar_mes[0]\n",
    "                node1_new_mes=g_mar_mes[1]\n",
    "            \n",
    "            elif type_dec=='mar_closures': # marginal calculation for closes\n",
    "                \n",
    "                # Define and remove edges, define new connected graph and calculate betweeness \n",
    "                edge_list=[(node0,node1)]\n",
    "                \n",
    "                \n",
    "                # If channel exists decrease capacity\n",
    "                if g_mar.edges[node0,node1]['no_channels']>1:\n",
    "                    g_mar.edges[node0,node1]['capacity']-=capacity\n",
    "                    g_mar.edges[node0,node1]['no_channels']-=1\n",
    "                \n",
    "                else: \n",
    "                    g_mar.remove_edges_from(edge_list) \n",
    "                    connected_components=[c for c in nx.algorithms.components.connected_components(g_mar)]\n",
    "                    g_mar=g_mar.subgraph(connected_components[0]).copy()\n",
    "                    \n",
    "                g_mar_mes=node_measurment(g_mar,measurement,node0,node1)\n",
    "                node0_new_mes=g_mar_mes[0]\n",
    "                node1_new_mes=g_mar_mes[1]\n",
    "                \n",
    "            elif type_dec=='actual': # actual calculation for both opens and closures\n",
    "                \n",
    "                # Check individualy if in the graph for the resulting block the node is present (in the connected component, \n",
    "                # if not assign measurment to 0. \n",
    "                \n",
    "                try:\n",
    "                    node0_new_mes=block_res[node0]\n",
    "\n",
    "                except KeyError:\n",
    "                    node0_new_mes=0\n",
    "\n",
    "                try:\n",
    "                    node1_new_mes=block_res[node1]\n",
    "\n",
    "                except KeyError:\n",
    "                    node1_new_mes=0\n",
    "\n",
    "                \n",
    "                   \n",
    "            node0_mar=(node0_new_mes-node0_base)\n",
    "            node1_mar=(node1_new_mes-node1_base) \n",
    "        \n",
    "        \n",
    "        else: # If both nodes are new (outside of connected graph) their marginal decision outcome is 0\n",
    "            node0_mar=0\n",
    "            node1_mar=0\n",
    "\n",
    "        \n",
    "        # Update dictionary - new betweenness\n",
    "        mar_node0_dic_i[channel_id]=node0_mar\n",
    "        mar_node1_dic_i[channel_id]=node1_mar\n",
    "        \n",
    "    \n",
    "    return (mar_node0_dic_i,mar_node1_dic_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MARGINAL - CURRENT BETWEENESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Marginal current betweeness for opens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute marginal betweeness for channel openings\n",
    "\n",
    "futures_bet_maropen=collection_compare(blocks,channel_opens,graph_snapshots,snapshot_bet,measurement='current_betweeness',type_dec='mar_opens')\n",
    "start=time.time()\n",
    "bet_maropen_diclist = dask.compute(*futures_bet_maropen)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(bet_maropen_diclist)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to marginal current betweeness for opens into decisions Dataframe\n",
    "\n",
    "bet_maropen_channels=add_columns(bet_maropen_diclist,decisions_df,'bet_maropen_node0','bet_maropen_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with marginal current betweeness for opens: {}'.format(len(bet_maropen_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(bet_maropen_channels)].head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Marginal current betweeness for closures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute marginal betweeness for channel closures\n",
    "\n",
    "futures_bet_marclose=collection_compare(blocks,channel_closures,graph_snapshots,snapshot_bet,measurement='current_betweeness',type_dec='mar_closures')\n",
    "start=time.time()\n",
    "bet_marclose_diclist = dask.compute(*futures_bet_marclose)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(bet_marclose_diclist)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to marginal current betweeness for closures into decisions Dataframe\n",
    "\n",
    "bet_marclose_channels=add_columns(bet_marclose_diclist,decisions_df,'bet_marclose_node0','bet_marclose_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with marginal current betweeness for closures: {}'.format(len(bet_marclose_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(bet_marclose_channels)].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MARGINAL - CURRENT CLOSENESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Marginal current closeness for opens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute marginal betweeness for channel openings\n",
    "\n",
    "futures_clo_maropen=collection_compare(blocks,channel_opens,graph_snapshots,snapshot_clo,measurement='current_closeness',type_dec='mar_opens')\n",
    "start=time.time()\n",
    "clo_maropen_diclist = dask.compute(*futures_clo_maropen)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(clo_maropen_diclist)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to marginal current betweeness for opens into decisions Dataframe\n",
    "\n",
    "clo_maropen_channels=add_columns(clo_maropen_diclist,decisions_df,'clo_maropen_node0','clo_maropen_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with marginal current closeness for opens: {}'.format(len(clo_maropen_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(clo_maropen_channels)].head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Marginal current closeness for closures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute marginal current closeness for channel closures\n",
    "\n",
    "futures_clo_marclose=collection_compare(blocks,channel_closures,graph_snapshots,snapshot_clo,measurement='current_closeness',type_dec='mar_closures')\n",
    "start=time.time()\n",
    "clo_marclose_diclist = dask.compute(*futures_clo_marclose)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(clo_marclose_diclist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to marginal current betweeness for opens into decisions Dataframe\n",
    "\n",
    "clo_marclose_channels=add_columns(clo_marclose_diclist,decisions_df,'clo_marclose_node0','clo_marclose_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with marginal current closeness for closures: {}'.format(len(clo_marclose_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(clo_marclose_channels)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACTUAL - CURRENT BETWEENESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Actual current betweeness for opens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute actual current betweeness for channel opens\n",
    "\n",
    "futures_bet_actopen=collection_compare(blocks,channel_opens,graph_snapshots,snapshot_bet,measurement='current_betweeness',type_dec='actual')\n",
    "start=time.time()\n",
    "bet_actopen_diclist = dask.compute(*futures_bet_actopen)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(bet_actopen_diclist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to marginal current betweeness for opens into decisions Dataframe\n",
    "\n",
    "bet_actopen_channels=add_columns(bet_actopen_diclist,decisions_df,'bet_actopen_node0','bet_actopen_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with actual current betweeness for opens: {}'.format(len(bet_actopen_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(bet_actopen_channels)].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Actual current betweeness for closures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute actual current betweeness for channel closures\n",
    "\n",
    "futures_bet_actclose=collection_compare(blocks,channel_closures,graph_snapshots,snapshot_bet,measurement='current_betweeness',type_dec='actual')\n",
    "start=time.time()\n",
    "bet_actclose_diclist = dask.compute(*futures_bet_actclose)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(bet_actclose_diclist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to marginal current betweeness for opens into decisions Dataframe\n",
    "\n",
    "bet_actclose_channels=add_columns(bet_actclose_diclist,decisions_df,'bet_actclose_node0','bet_actclose_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with actual current betweeness for closures: {}'.format(len(bet_actclose_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(bet_actclose_channels)].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACTUAL - CURRENT CLOSENESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Actual current closeness for opens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute actual current closeness for channel opens\n",
    "\n",
    "futures_clo_actopen=collection_compare(blocks,channel_opens,graph_snapshots,snapshot_bet,measurement='current_closeness',type_dec='actual')\n",
    "start=time.time()\n",
    "clo_actopen_diclist = dask.compute(*futures_clo_actopen)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(bet_actopen_diclist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to marginal current closeness for opens into decisions Dataframe\n",
    "\n",
    "clo_actopen_channels=add_columns(clo_actopen_diclist,decisions_df,'clo_actopen_node0','clo_actopen_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with actual current closeness for opens: {}'.format(len(clo_actopen_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(clo_actopen_channels)].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Actual current closeness for closures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute actual current closeness for channel closures\n",
    "\n",
    "futures_clo_actclose=collection_compare(blocks,channel_closures,graph_snapshots,snapshot_bet,measurement='current_closeness',type_dec='actual')\n",
    "start=time.time()\n",
    "clo_actclose_diclist = dask.compute(*futures_clo_actclose)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(clo_actclose_diclist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to marginal current closeness for opens into decisions Dataframe\n",
    "\n",
    "clo_actclose_channels=add_columns(clo_actclose_diclist,decisions_df,'clo_actclose_node0','clo_actclose_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with actual current closeness for closures: {}'.format(len(clo_actclose_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(clo_actclose_channels)].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise stability \n",
    "\n",
    "- **Marginal betweenness (bet_mar_nodei)**: The % change between the betweenness centrality, for the node under analysis, given the graph from the previous block and the betweenness centrality of the resulting graph after enacting the decission (adding or removing a channel). Weighted current betweenness centrality is used for this measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Marginal betweenness for opens** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Why is length of Dataframe longer than the number of snapshots extracted? Could it be that some channels appear more than once in dataframe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Marginal betweenness for closures** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Actual change in betweenness (bet_act_nodei)**: The % change between the betweenness centrality, for the node under analysis, given the graph from the previous block and the betweenness centrality of the resulting graph after enacting **all** the decissions (adding or removing a channels) in the current block. Weighted current betweenness centrality is used for this measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Marginal betweeness pairwise stability (bet_mar_pairst/open/close)**: Evaluates if given the marginal graph that results from just enacting this decission is consistent with pairwise stability, from a betweenness perspective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MARGINAL - Add column with check for pairwise stability compatability using marginal outcomes\n",
    "\n",
    "# OPEN - Channel is opened if both nodes gain\n",
    "\n",
    "# Define function \n",
    "def bet_pairst_maropen(row):\n",
    "    if not math.isnan(row['bet_mar_node0']):\n",
    "        pairst=(row['bet_maropen_node0']>=0 and row['bet_maropen_node1']>=0)\n",
    "    else:\n",
    "        pairst=row['bet_maropen_node0']\n",
    "    return pairst\n",
    "\n",
    "# Apply function\n",
    "decisions_df['bet_mar_pairstopen']=decisions_df.apply(bet_pairst_maropen,axis=1)\n",
    "\n",
    "# CLOSE - Channel is closed if at least one node is better off\n",
    "\n",
    "# Define function \n",
    "def bet_pairst_marclose(row):\n",
    "    if not math.isnan(row['bet_marclose_node0']):\n",
    "        pairst=(row['bet_marclose_node0']>0 or row['bet_marclose_node1']>0)\n",
    "    else:\n",
    "        pairst=row['bet_marclose_node0']\n",
    "    return pairst\n",
    "\n",
    "# Apply function\n",
    "decisions_df['bet_mar_pairstclose']=decisions_df.apply(bet_pairst_marclose,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MARGINAL OPEN\n",
    "decisions_df[decisions_df['bet_mar_node0'].notnull()][['bet_mar_node0','bet_mar_node1','bet_mar_pairstopen']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MARGINAL CLOSE\n",
    "decisions_df[decisions_df['bet_marclose_node0'].notnull()][['bet_marclose_node0','bet_marclose_node1','bet_mar_pairstclose']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Actual betweeness pairwise stability (bet_act_pairstopen/close)**: Evaluates if given the marginal graph that results from all the decisions in the block is consitend with pairwise stability, from a betweenness perspective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTUAL - Add column with check for pairwise stability compatability using marginal outcomes\n",
    "\n",
    "# OPEN - Channel is opened if both nodes gain\n",
    "\n",
    "# Define function \n",
    "def bet_pairst_actopen(row):\n",
    "    if not math.isnan(row['bet_actopen_node0']):\n",
    "        pairst=(row['bet_actopen_node0']>=0 and row['bet_actopen_node1']>=0)\n",
    "    else:\n",
    "        pairst=row['bet_actopen_node0']\n",
    "    return pairst\n",
    "\n",
    "# Apply function\n",
    "decisions_df['bet_act_pairstopen']=decisions_df.apply(bet_pairst_actopen,axis=1)\n",
    "\n",
    "# CLOSE - Channel is closed if at least one node is better off\n",
    "\n",
    "# Define function \n",
    "def bet_pairst_actclose(row):\n",
    "    if not math.isnan(row['bet_actclose_node0']):\n",
    "        pairst=(row['bet_actclose_node0']>0 or row['bet_actclose_node1']>0)\n",
    "    else:\n",
    "        pairst=row['bet_actclose_node0']\n",
    "    return pairst\n",
    "\n",
    "# Apply function\n",
    "decisions_df['bet_act_pairstclose']=decisions_df.apply(bet_pairst_actclose,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ACTUAL OPEN\n",
    "decisions_df[decisions_df['bet_actopen_node0'].notnull()][['bet_actopen_node0','bet_actopen_node1','bet_act_pairstopen']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ACTUAL CLOSE\n",
    "decisions_df[decisions_df['bet_actclose_node0'].notnull()][['bet_actclose_node0','bet_actclose_node1','bet_act_pairstclose']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Updated DataFrame to S3\n",
    "\n",
    "# Create S3 resource and define values\n",
    "session = boto3.session.Session()\n",
    "s3 = session.resource('s3')\n",
    "csv_buffer = io.StringIO()\n",
    "\n",
    "# File path and name ([extraction_id]snapshot_bet-[no_blocks]-[start_block]-[end_block])\n",
    "key_decisions_df='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+str(extraction_id)+'decisions_df_bet-'+str(no_blocks)+'-'+str(start_block)+'-'+str(end_block)+'.csv'\n",
    "\n",
    "\n",
    "# Safe DataFrame\n",
    "decisions_df.to_csv(csv_buffer)\n",
    "s3.Object(bucket, key_decisions_df).put(Body=csv_buffer.getvalue())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Save\n",
    "decisions_df_test_load = s3.Object(bucket_name=bucket, key=key_decisions_df).get()\n",
    "decisions_df_test=pd.read_csv(io.BytesIO(decisions_df_test_load['Body'].read()),index_col=0)\n",
    "decisions_df_test==decisions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions_df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency\n",
    "- **Average betweeness per block (bet_effic)**: Average betweenness centrality for all the nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nash stability \n",
    "\n",
    "- **% Change with respect to not making decision (bet_binstat_deltai)**: The % change in betwewnness centrality, for the node under analysis, given the resulting graph after all of the decissions have been executed. \n",
    "- **Nash compatible - binary strategy (bet_binstat_nash)**: Returns true if given the other decissions enacted in the block not making decision would have NOT have resulted in higher betweenness centrality. This tells me if my strategy helped me be better off (took into account what others were doing)\n",
    "\n",
    "(Optional approaches - Check for tracktability)\n",
    "- **Nash compatible - close only strategy (bet_closestat_nash)**: Returns true if given the other decissions enacted in the block, closing any other channels would NOT have not resulted in higher betwneenness centrality. (NOTE: Check if there are combinatorial considerations, if so just look at closings up to x) \n",
    "- **Nash compatible - close/open (bet_allstat_nash)**: Returns true if given the other decissions enacted in the block, closing any other channels (with any node) or opening a channel with one of the round participants would NOT have not resulted in lower betwneenness centrality. (NOTE: To make it reasonable and constraint the strategy space only consider 'similar nodes' or with relationships in the past?).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Connectivity\n",
    "\n",
    "### Pairwise stability \n",
    "\n",
    "- **Marginal % change in connectivity (con_mar_deltai)**: The % change between the shortest path average, for the node under analysis, given the graph from the previous block and the shortest path average of the resulting graph after enacting the decission (adding or removing a channel). Weighted shortest path (_single_source_dijkstra_path_) is used for this measure.\n",
    "\n",
    "- **Actual % change in connectivity (con_act_deltai)**: The % change between the shortest path average, for the node under analysis, given the graph from the previous block and the shortest path average of the resulting graph after enacting **all** the decissions (adding or removing a channels) in the current block. Weighted shortest path (_single_source_dijkstra_path_) is used for this measure.\n",
    "\n",
    "- **Marginal connectivity pairwise stability (con_mar_pairstab)**: Evaluates if given the marginal graph that results from just enacting this decission is consistent with pairwise stability, from a connectivity perspective.\n",
    "\n",
    "- **Actual connectivity pairwise stability (con_act_pairstab)**: Evaluates if given the marginal graph that results from all the decisions in the block is consitend with pairwise stability, from a connectivity perspective.  \n",
    "\n",
    "\n",
    "\n",
    "### Nash stability \n",
    "\n",
    "- **% Change with respect to not making decision (con_binstat_deltai)**: The % change in shortest path average, for the node under analysis, given the resulting graph after all of the decissions have been executed. \n",
    "- **Nash compatible - binary strategy (con_binstat_nash)**: Returns true if given the other decissions enacted in the block not making decision would have NOT have resulted in higher shortest path average. NOTE: This indicates if the strategy selected made the node better off (took into account what others were doing)\n",
    "\n",
    "(Optional approaches - Check for tracktability)\n",
    "- **Nash compatible - close only strategy (con_closestat_nash)**: Returns true if given the other decissions enacted in the block, closing any other channels would NOT have not resulted in higher shortest path average. (NOTE: Check if there are combinatorial considerations, if so just look at closings up to x) \n",
    "- **Nash compatible - close/open (con_allstat_nash)**: Returns true if given the other decissions enacted in the block, closing any other channels (with any node) or opening a channel with one of the round participants would NOT have not resulted in lower shortest path average. (NOTE: To make it reasonable and constraint the strategy space only consider 'similar nodes' or with relationships in the past?).\n",
    "\n",
    "\n",
    "\n",
    "### Efficiency\n",
    "- **Average betweeness per block (bet_effic)**: Average shortest path average for all the nodes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
