{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LN - Data PP - Stability and efficiency calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import itertools\n",
    "#import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import io\n",
    "import random\n",
    "from itertools import islice\n",
    "import math\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "#from tqdm.notebook import trange\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "from time import sleep\n",
    "\n",
    "from dask_cloudprovider import FargateCluster\n",
    "from dask.distributed import Client\n",
    "import dask.array as da\n",
    "import dask\n",
    "dask.config.set({'distributed.scheduler.allowed-failures': 50}) \n",
    "\n",
    "\n",
    "import boto3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "\n",
    "bucket='ln-strategy-data'\n",
    "extraction_id=1587447789\n",
    "#extraction_id=1585344554"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection to AWS Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate s3 resource\n",
    "\n",
    "session = boto3.session.Session()\n",
    "s3 = session.resource('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fargate Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster = FargateCluster(n_workers=100,scheduler_timeout='20 minutes',image='dsrincon/dask-graph:nx-scipy-v1',scheduler_cpu=4096,scheduler_mem=16384,worker_mem=32768)\n",
    "cluster = FargateCluster(n_workers=20,scheduler_timeout='20 minutes',image='dsrincon/dask-graph:nx-scipy-v1',scheduler_cpu=4096,scheduler_mem=16384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "728fa4b6018945ae83ddeaccbee9fef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>FargateCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/distributed/client.py:1079: VersionMismatchWarning: Mismatched versions found\n",
      "\n",
      "python\n",
      "+---------------------------+---------------+\n",
      "|                           | version       |\n",
      "+---------------------------+---------------+\n",
      "| client                    | 3.7.3.final.0 |\n",
      "| scheduler                 | 3.7.4.final.0 |\n",
      "| tcp://172.31.13.209:46357 | 3.7.4.final.0 |\n",
      "| tcp://172.31.18.116:39467 | 3.7.4.final.0 |\n",
      "| tcp://172.31.19.161:33289 | 3.7.4.final.0 |\n",
      "| tcp://172.31.19.30:38457  | 3.7.4.final.0 |\n",
      "| tcp://172.31.19.71:44779  | 3.7.4.final.0 |\n",
      "| tcp://172.31.22.77:44093  | 3.7.4.final.0 |\n",
      "| tcp://172.31.26.198:37485 | 3.7.4.final.0 |\n",
      "| tcp://172.31.28.105:35895 | 3.7.4.final.0 |\n",
      "| tcp://172.31.28.135:37931 | 3.7.4.final.0 |\n",
      "| tcp://172.31.31.189:44025 | 3.7.4.final.0 |\n",
      "| tcp://172.31.34.108:35797 | 3.7.4.final.0 |\n",
      "| tcp://172.31.43.151:43143 | 3.7.4.final.0 |\n",
      "| tcp://172.31.49.33:46463  | 3.7.4.final.0 |\n",
      "| tcp://172.31.50.102:35397 | 3.7.4.final.0 |\n",
      "| tcp://172.31.6.230:35675  | 3.7.4.final.0 |\n",
      "| tcp://172.31.65.50:37545  | 3.7.4.final.0 |\n",
      "| tcp://172.31.77.127:41603 | 3.7.4.final.0 |\n",
      "| tcp://172.31.79.94:38975  | 3.7.4.final.0 |\n",
      "| tcp://172.31.83.53:45495  | 3.7.4.final.0 |\n",
      "| tcp://172.31.83.83:41817  | 3.7.4.final.0 |\n",
      "+---------------------------+---------------+\n",
      "\n",
      "tornado\n",
      "+---------------------------+---------+\n",
      "|                           | version |\n",
      "+---------------------------+---------+\n",
      "| client                    | 6.0.3   |\n",
      "| scheduler                 | 6.0.4   |\n",
      "| tcp://172.31.13.209:46357 | 6.0.4   |\n",
      "| tcp://172.31.18.116:39467 | 6.0.4   |\n",
      "| tcp://172.31.19.161:33289 | 6.0.4   |\n",
      "| tcp://172.31.19.30:38457  | 6.0.4   |\n",
      "| tcp://172.31.19.71:44779  | 6.0.4   |\n",
      "| tcp://172.31.22.77:44093  | 6.0.4   |\n",
      "| tcp://172.31.26.198:37485 | 6.0.4   |\n",
      "| tcp://172.31.28.105:35895 | 6.0.4   |\n",
      "| tcp://172.31.28.135:37931 | 6.0.4   |\n",
      "| tcp://172.31.31.189:44025 | 6.0.4   |\n",
      "| tcp://172.31.34.108:35797 | 6.0.4   |\n",
      "| tcp://172.31.43.151:43143 | 6.0.4   |\n",
      "| tcp://172.31.49.33:46463  | 6.0.4   |\n",
      "| tcp://172.31.50.102:35397 | 6.0.4   |\n",
      "| tcp://172.31.6.230:35675  | 6.0.4   |\n",
      "| tcp://172.31.65.50:37545  | 6.0.4   |\n",
      "| tcp://172.31.77.127:41603 | 6.0.4   |\n",
      "| tcp://172.31.79.94:38975  | 6.0.4   |\n",
      "| tcp://172.31.83.53:45495  | 6.0.4   |\n",
      "| tcp://172.31.83.83:41817  | 6.0.4   |\n",
      "+---------------------------+---------+\n",
      "  warnings.warn(version_module.VersionMismatchWarning(msg[0][\"warning\"]))\n"
     ]
    }
   ],
   "source": [
    "client = Client(cluster)\n",
    "#cluster=Client('tcp://3.214.224.107:8786')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Write output to DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function write output to DataFrame\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "add_columns\n",
    "    Function that takes an output from a decision comparisson computation and adds it's results for nodes 1 and 0 in the main DataFrame\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "output_diclist : list\n",
    "    Dictionary of the form (node0_dic_i,node1_dic_i) where i runs for all of the blocks being compared. \n",
    "\n",
    "original_df: Pandas DataFrame\n",
    "    Original DataFrame containing the opening and closure information for each channel, with a column named 'short_channel_id' to denote \n",
    "    id of channel. \n",
    "\n",
    "column_name_node0: string\n",
    "    Name for column in dataframe where the results will be stored for node 0\n",
    "    \n",
    "column_name_node1: string\n",
    "    Name for column in dataframe where the results will be stored for node 1\n",
    "\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "no_changes: list\n",
    "    List with the 'short_channel_id' of the channels edited. \n",
    " \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def add_columns(output_diclist,original_df,column_name_node0,column_name_node1):\n",
    "\n",
    "\n",
    "    # Merge individual dictionaries into one for each node\n",
    "    node0_dic={}\n",
    "    node1_dic={}\n",
    "    for dic_tuple in output_diclist:\n",
    "        node0_dic.update(dic_tuple[0])\n",
    "        node1_dic.update(dic_tuple[1])\n",
    "    \n",
    "    # Add to DataFrame\n",
    "\n",
    "    # Create empty columns\n",
    "    original_df[column_name_node0]=np.nan\n",
    "    original_df[column_name_node1]=np.nan\n",
    "\n",
    "    # Populate df with values\n",
    "    original_df[column_name_node0]=original_df['short_channel_id'].map(node0_dic)\n",
    "    original_df[column_name_node1]=original_df['short_channel_id'].map(node1_dic)\n",
    "    \n",
    "    # Calculate values changed\n",
    "    rows_edited=(original_df[original_df[column_name_node0].notnull()]['short_channel_id']).tolist()\n",
    "    \n",
    "    return rows_edited\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Save python object to S3 using pickle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to write pickle data to S3 bucket\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "pickle_save_s3\n",
    "    Saves Python object to S3 as pickle object\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "obj : <any>\n",
    "    Python Object\n",
    "\n",
    "blocks: list\n",
    "    List of extracted blocks\n",
    "\n",
    "extraction_id: int\n",
    "    Number of block extraction\n",
    "    \n",
    "name: string\n",
    "    Name of object to add to filename in S3\n",
    "\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "response: int\n",
    "    HTTP response code from S3 \n",
    " \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def pickle_save_s3(obj,blocks,extraction_id,name):\n",
    "\n",
    "\n",
    "    # Define number of blocks\n",
    "    start_block=np.min(np.array(blocks))\n",
    "    end_block=np.max(np.array(blocks))\n",
    "    no_blocks=len(blocks)\n",
    "\n",
    "    # Load S3 and bucket details\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.resource('s3')\n",
    "\n",
    "    # File path and name ([extraction_id][name]-[no_blocks]-[start_block]-[end_block])\n",
    "    key='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+str(extraction_id)+name+'-'+str(no_blocks)+'-'+str(start_block)+'-'+str(end_block)+'.pkl'\n",
    "\n",
    "    # Create pickle object and send to S3\n",
    "    pickle_byte_obj = pickle.dumps(obj) \n",
    "    response=s3.Object(bucket,key).put(Body=pickle_byte_obj)\n",
    "    \n",
    "    return response['ResponseMetadata']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Load single graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "load_graph\n",
    "    Loads networkX (pickle serialized) object from S3\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "key : str\n",
    "    Path in S3 bucket for individual pickled serialized networkX graph object \n",
    "\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "response: networkX graph\n",
    "    Graph object\n",
    " \n",
    "\"\"\"\n",
    "\n",
    "def load_graph(key):\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.resource('s3')\n",
    "    response = s3.Object(bucket_name=bucket, key=key).get()\n",
    "    G=pickle.loads(response['Body'].read())\n",
    "    \n",
    "    return G\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load objects form S3\n",
    "# Dataframe\n",
    "\n",
    "decisions_load = s3.Object(bucket_name=bucket, key='decisions_df.csv').get()\n",
    "decisions_df=pd.read_csv(io.BytesIO(decisions_load['Body'].read()))\n",
    "\n",
    "# Channel closures\n",
    "closure_file = s3.Object(bucket_name=bucket, key='channel_closures.p').get()\n",
    "channel_closures = pickle.loads(closure_file['Body'].read())\n",
    "    \n",
    "    \n",
    "# Channel openings \n",
    "opens_file = s3.Object(bucket_name=bucket, key='channel_opens.p').get()\n",
    "channel_opens = pickle.loads(opens_file['Body'].read())\n",
    "\n",
    "    \n",
    "\n",
    "# Create list with graph keys\n",
    "\n",
    "#TODO: Save graphs as numpy array in single H5 file to reduce. Test if creating graphs takes longer than reading from S3\n",
    "\n",
    "# graph_dir='./data/graph_snapshots' - For local tests\n",
    "\n",
    "\n",
    "graph_keys = [obj.key \n",
    "        for obj in s3.Bucket(name=bucket).objects.all()\n",
    "        if re.match(\".*\"+str(extraction_id)+\"_connected/.*\\.gpickle\",obj.key)]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Blocks to be extracted and define graph\n",
    "\n",
    "\n",
    "# Base lists to be populated\n",
    "graph_snapshots=[]\n",
    "blocks=[]\n",
    "\n",
    "\n",
    "extract_keys=graph_keys[6:] # Blocks below 6th index are <3 and affect some graph metrics\n",
    "\n",
    "for key in extract_keys: # Change to [700:] for full range\n",
    "    \n",
    "    # Create block list from file_names\n",
    "    block_i=int(key.split(\".\")[0].split(\"/\")[-1]) \n",
    "    blocks.append(block_i)\n",
    "    \n",
    "    #Extract graphs - UNCOMMENT TO have them out of function\n",
    "    #G=dask.delayed(load_graph)(key)\n",
    "    #graph_snapshots.append(G)\n",
    "    \n",
    "   \n",
    "start_block=np.min(np.array(blocks))\n",
    "end_block=np.max(np.array(blocks))\n",
    "no_blocks=len(blocks)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total graph keys:36542\n",
      "Number of blocks to be processed:144\n",
      "---Sample graph keys---\n",
      "graph_snapshots/1587447789_connected/505149.gpickle\n",
      "---Sample channel opens---\n",
      "[(2378, 4223, {'capacity': 400000, 'open_fee': 4557, 'dec_id': 58766, 'channel_id': '508090x1515x1', 'no_channels': 0})]\n",
      "---Sample channel closures---\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Test: extracted formats\n",
    "print(\"Number of total graph keys:{}\".format(len(graph_keys)))\n",
    "print(\"Number of blocks to be processed:{}\".format(len(extract_keys)))\n",
    "print(\"---Sample graph keys---\")\n",
    "print(graph_keys[0])\n",
    "print(\"---Sample channel opens---\")\n",
    "print(channel_opens[508090])\n",
    "print(\"---Sample channel closures---\")\n",
    "print(channel_closures[508090])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of DataFrame in Memory:64821774\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>short_channel_id</th>\n",
       "      <th>open_block</th>\n",
       "      <th>open_transaction</th>\n",
       "      <th>address</th>\n",
       "      <th>close_block</th>\n",
       "      <th>close_transaction</th>\n",
       "      <th>node0</th>\n",
       "      <th>node1</th>\n",
       "      <th>satoshis</th>\n",
       "      <th>...</th>\n",
       "      <th>close_fee</th>\n",
       "      <th>last_update</th>\n",
       "      <th>close_type</th>\n",
       "      <th>close_htlc_count</th>\n",
       "      <th>close_balance_a</th>\n",
       "      <th>close_balance_b</th>\n",
       "      <th>dec_id</th>\n",
       "      <th>node0_id</th>\n",
       "      <th>node1_id</th>\n",
       "      <th>node_pair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70648</th>\n",
       "      <td>1027</td>\n",
       "      <td>535029x2012x1</td>\n",
       "      <td>535029</td>\n",
       "      <td>d01928d350e1ba04d7335a91e6dd54f5dbf94859e0c59b...</td>\n",
       "      <td>bc1qszamn0la3yqrqhjj8yepdxkl9qlr84zfwgg9zrkccl...</td>\n",
       "      <td>535029.0</td>\n",
       "      <td>d01928d350e1ba04d7335a91e6dd54f5dbf94859e0c59b...</td>\n",
       "      <td>022a7809052db05fde648391a53aba82286e4a517cff1d...</td>\n",
       "      <td>031b71cbad0cb4e22141e45f16c83c332f755e1ba68195...</td>\n",
       "      <td>462124</td>\n",
       "      <td>...</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unused</td>\n",
       "      <td>0.0</td>\n",
       "      <td>275630.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1027</td>\n",
       "      <td>4091</td>\n",
       "      <td>3578</td>\n",
       "      <td>14637598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70625</th>\n",
       "      <td>1045</td>\n",
       "      <td>535177x446x1</td>\n",
       "      <td>535177</td>\n",
       "      <td>7376d5bc0c18bbff8f644d0827e759a1518b38e1e95a08...</td>\n",
       "      <td>bc1qauzljedtlva73ngg7suqketlvn5gnnuemxpeuevcqt...</td>\n",
       "      <td>535177.0</td>\n",
       "      <td>7376d5bc0c18bbff8f644d0827e759a1518b38e1e95a08...</td>\n",
       "      <td>02272bd12e59324d0f2b231fb88f134b57eb26dd100d2c...</td>\n",
       "      <td>031b71cbad0cb4e22141e45f16c83c332f755e1ba68195...</td>\n",
       "      <td>257307</td>\n",
       "      <td>...</td>\n",
       "      <td>767.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unused</td>\n",
       "      <td>0.0</td>\n",
       "      <td>218405.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1045</td>\n",
       "      <td>3604</td>\n",
       "      <td>3578</td>\n",
       "      <td>12895112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68629</th>\n",
       "      <td>2745</td>\n",
       "      <td>549037x2738x0</td>\n",
       "      <td>549037</td>\n",
       "      <td>b7128bbbe422b4f18fad71b091eed1f9e4b0d231be8117...</td>\n",
       "      <td>bc1q95fytjzs8f7fma2nf66gcva7c3w7hnkdwrkef9pu33...</td>\n",
       "      <td>549037.0</td>\n",
       "      <td>b7128bbbe422b4f18fad71b091eed1f9e4b0d231be8117...</td>\n",
       "      <td>028b892b15f5cabcea5165b236db0e36dc06553c323c84...</td>\n",
       "      <td>038b36a43c38f75cd15bb25394f1cd162f717df0055852...</td>\n",
       "      <td>400000</td>\n",
       "      <td>...</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>1.547494e+09</td>\n",
       "      <td>unused</td>\n",
       "      <td>0.0</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2745</td>\n",
       "      <td>1781</td>\n",
       "      <td>4968</td>\n",
       "      <td>8848008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68636</th>\n",
       "      <td>2744</td>\n",
       "      <td>549037x2737x0</td>\n",
       "      <td>549037</td>\n",
       "      <td>0825da5e96cd45fced3233ebe615721b687285839d3036...</td>\n",
       "      <td>bc1q5mqzhw5e42rfqh250zalwu47ru8gvz4g4k968me0mg...</td>\n",
       "      <td>549037.0</td>\n",
       "      <td>0825da5e96cd45fced3233ebe615721b687285839d3036...</td>\n",
       "      <td>02b95713bbe4609a337f3ca5aab3a75674083ddf5331a4...</td>\n",
       "      <td>038b36a43c38f75cd15bb25394f1cd162f717df0055852...</td>\n",
       "      <td>400000</td>\n",
       "      <td>...</td>\n",
       "      <td>1992.0</td>\n",
       "      <td>1.547503e+09</td>\n",
       "      <td>unused</td>\n",
       "      <td>0.0</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2744</td>\n",
       "      <td>210</td>\n",
       "      <td>4968</td>\n",
       "      <td>1043280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68461</th>\n",
       "      <td>40227</td>\n",
       "      <td>549489x1194x1</td>\n",
       "      <td>549489</td>\n",
       "      <td>58dafe493648fbdd69143c26e0cf8a66ae11a272c2739d...</td>\n",
       "      <td>bc1q25j5l6crv4mrjkjjjw4rzyv890cwwnyyw9dezcqs5x...</td>\n",
       "      <td>549489.0</td>\n",
       "      <td>58dafe493648fbdd69143c26e0cf8a66ae11a272c2739d...</td>\n",
       "      <td>02574ffa55d394b9326f6e5c15992cc0516b0d6e6a79a1...</td>\n",
       "      <td>03a5927b64b1ea8657d5b770d61a3e2d0554fdb5d56877...</td>\n",
       "      <td>2500000</td>\n",
       "      <td>...</td>\n",
       "      <td>2363.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unused</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8974.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40227</td>\n",
       "      <td>7558</td>\n",
       "      <td>4495</td>\n",
       "      <td>33973210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0 short_channel_id  open_block  \\\n",
       "70648        1027    535029x2012x1      535029   \n",
       "70625        1045     535177x446x1      535177   \n",
       "68629        2745    549037x2738x0      549037   \n",
       "68636        2744    549037x2737x0      549037   \n",
       "68461       40227    549489x1194x1      549489   \n",
       "\n",
       "                                        open_transaction  \\\n",
       "70648  d01928d350e1ba04d7335a91e6dd54f5dbf94859e0c59b...   \n",
       "70625  7376d5bc0c18bbff8f644d0827e759a1518b38e1e95a08...   \n",
       "68629  b7128bbbe422b4f18fad71b091eed1f9e4b0d231be8117...   \n",
       "68636  0825da5e96cd45fced3233ebe615721b687285839d3036...   \n",
       "68461  58dafe493648fbdd69143c26e0cf8a66ae11a272c2739d...   \n",
       "\n",
       "                                                 address  close_block  \\\n",
       "70648  bc1qszamn0la3yqrqhjj8yepdxkl9qlr84zfwgg9zrkccl...     535029.0   \n",
       "70625  bc1qauzljedtlva73ngg7suqketlvn5gnnuemxpeuevcqt...     535177.0   \n",
       "68629  bc1q95fytjzs8f7fma2nf66gcva7c3w7hnkdwrkef9pu33...     549037.0   \n",
       "68636  bc1q5mqzhw5e42rfqh250zalwu47ru8gvz4g4k968me0mg...     549037.0   \n",
       "68461  bc1q25j5l6crv4mrjkjjjw4rzyv890cwwnyyw9dezcqs5x...     549489.0   \n",
       "\n",
       "                                       close_transaction  \\\n",
       "70648  d01928d350e1ba04d7335a91e6dd54f5dbf94859e0c59b...   \n",
       "70625  7376d5bc0c18bbff8f644d0827e759a1518b38e1e95a08...   \n",
       "68629  b7128bbbe422b4f18fad71b091eed1f9e4b0d231be8117...   \n",
       "68636  0825da5e96cd45fced3233ebe615721b687285839d3036...   \n",
       "68461  58dafe493648fbdd69143c26e0cf8a66ae11a272c2739d...   \n",
       "\n",
       "                                                   node0  \\\n",
       "70648  022a7809052db05fde648391a53aba82286e4a517cff1d...   \n",
       "70625  02272bd12e59324d0f2b231fb88f134b57eb26dd100d2c...   \n",
       "68629  028b892b15f5cabcea5165b236db0e36dc06553c323c84...   \n",
       "68636  02b95713bbe4609a337f3ca5aab3a75674083ddf5331a4...   \n",
       "68461  02574ffa55d394b9326f6e5c15992cc0516b0d6e6a79a1...   \n",
       "\n",
       "                                                   node1  satoshis  ...  \\\n",
       "70648  031b71cbad0cb4e22141e45f16c83c332f755e1ba68195...    462124  ...   \n",
       "70625  031b71cbad0cb4e22141e45f16c83c332f755e1ba68195...    257307  ...   \n",
       "68629  038b36a43c38f75cd15bb25394f1cd162f717df0055852...    400000  ...   \n",
       "68636  038b36a43c38f75cd15bb25394f1cd162f717df0055852...    400000  ...   \n",
       "68461  03a5927b64b1ea8657d5b770d61a3e2d0554fdb5d56877...   2500000  ...   \n",
       "\n",
       "      close_fee   last_update  close_type  close_htlc_count  close_balance_a  \\\n",
       "70648    1989.0           NaN      unused               0.0         275630.0   \n",
       "70625     767.0           NaN      unused               0.0         218405.0   \n",
       "68629    1991.0  1.547494e+09      unused               0.0         400000.0   \n",
       "68636    1992.0  1.547503e+09      unused               0.0         400000.0   \n",
       "68461    2363.0           NaN      unused               0.0           8974.0   \n",
       "\n",
       "       close_balance_b dec_id  node0_id  node1_id  node_pair  \n",
       "70648              0.0   1027      4091      3578   14637598  \n",
       "70625              0.0   1045      3604      3578   12895112  \n",
       "68629              0.0   2745      1781      4968    8848008  \n",
       "68636              0.0   2744       210      4968    1043280  \n",
       "68461              0.0  40227      7558      4495   33973210  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort and visualize DataFrame\n",
    "\n",
    "decisions_df.sort_values(by=['open_block'],inplace=True,ascending=True)\n",
    "print('Size of DataFrame in Memory:{}'.format(sys.getsizeof(decisions_df)))\n",
    "# Check specific channel id\n",
    "#decisions_df[decisions_df['short_channel_id']=='513675x2245x0'].head()\n",
    "\n",
    "decisions_df.sort_values(by=['close_block'],ascending=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TESTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Lazy Graph extract\n",
    "blocks_att=[]\n",
    "for i in range(len(graph_snapshots)):\n",
    "    graph_i=dask.compute(graph_snapshots[i])\n",
    "    block=graph_i.graph['block']\n",
    "    blocks_att.append(block)\n",
    "\n",
    "print(blocks_att)\n",
    "\n",
    "#graph_snapshots=dask.compute(*graph_snapshots)\n",
    "#block=graph_snapshots[0].graph['block']\n",
    "    \n",
    "#print(len(graph_snapshots[5]))\n",
    "#print(graph_snapshots[3].graph['block'])\n",
    "\n",
    "# Delayed testing\n",
    "#results = dask.compute(*futures)\n",
    "#graphs=dask.compute(*graph_snapshots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Comparative Analysis\n",
    "\n",
    "In order to understand the potential motivations behind each decision we analyze each decission (opening or closure of a channel) independently from the perspective of each of the participants in the decission, which we'll call the node under analysis. For each decission we extract or compute the following information: \n",
    "\n",
    "Betweenness centrality measures how central is a network to the flow of information in a network. In the case of the Lightning Network the higher the betweenness centrality of a node, the more transactions (messages) that are routed through it. In particular, we will use a measure of betweenness centrality defined in (Brandes and Fleischer 2005 - https://link.springer.com/chapter/10.1007/978-3-540-31856-9_44) that models infomation through a network, as electric current, efficiently and not only considering shortest path. This allows us to account for the fact that not all transactions travel through shortes path given that there are fee and capacity considerations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Measurments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Property measurement for a collection of graphs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEW\n",
    "# Function to calculate base measurement for every graph in snapshot\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "collection_measure\n",
    "    Iterates over graph snapshots and calculates measurement for every node in each of the graphs.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "g_snapshots : list\n",
    "    List of delayed nx graph elements contianing graph snapshots\n",
    "    \n",
    "\n",
    "measurement: string\n",
    "    Name of the type of measurment that will applied to the graph. (See graph_measurement function for options)\n",
    "\n",
    "\n",
    "\n",
    "Returns\n",
    "-------\n",
    "snapshot_mes_dic: dic\n",
    "    Dictionary with blocks as keys and dic of measurements as values\n",
    "\n",
    "\"\"\"\n",
    "def collection_measure(bucket,graph_keys,measurement):\n",
    "    \n",
    "    snapshot_mes_list=[]\n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    for key in graph_keys:\n",
    "      \n",
    "        \n",
    "        measurement_input=(key,measurement,'capacity',bucket)\n",
    "        \n",
    "        b_g_tuple=dask.delayed(graph_measurement)(measurement_input)\n",
    "        snapshot_mes_list.append(b_g_tuple)\n",
    "        \n",
    "\n",
    "    futures = dask.persist(*snapshot_mes_list)\n",
    "    \n",
    "    start=time.time()\n",
    "    snapshot_mes_list = dask.compute(*futures)\n",
    "    #snapshot_mes_list = dask.compute(*snapshot_mes_list)\n",
    "    snapshot_mes_dic={record[0]:record[1] for record in snapshot_mes_list}\n",
    "    end=time.time()\n",
    "    print('Compute in seconds: {}'.format(end-start))\n",
    "    \n",
    "    return snapshot_mes_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate base measurement for every graph in snapshot\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "collection_measure\n",
    "    Iterates over graph snapshots and calculates measurement for every node in each of the graphs.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "g_snapshots : list\n",
    "    List of delayed nx graph elements contianing graph snapshots\n",
    "    \n",
    "\n",
    "measurement: string\n",
    "    Name of the type of measurment that will applied to the graph. (See graph_measurement function for options)\n",
    "\n",
    "\n",
    "\n",
    "Returns\n",
    "-------\n",
    "snapshot_mes_dic: dic\n",
    "    Dictionary with blocks as keys and dic of measurements as values\n",
    "\n",
    "\n",
    "def collection_measure(g_snapshots,measurement):\n",
    "    \n",
    "    snapshot_mes_list=[]\n",
    "   \n",
    "    for g in graph_snapshots:\n",
    "      \n",
    "        \n",
    "        measurement_input=(g,measurement,'capacity')\n",
    "        \n",
    "        b_g_tuple=dask.delayed(graph_measurement)(measurement_input)\n",
    "        snapshot_mes_list.append(b_g_tuple)\n",
    "        \n",
    "\n",
    "    #futures = dask.persist(*snapshot_mes_list)\n",
    "    \n",
    "    start=time.time()\n",
    "    #snapshot_mes_list = dask.compute(*futures)\n",
    "    snapshot_mes_list = dask.compute(*snapshot_mes_list)\n",
    "    snapshot_mes_dic={record[0]:record[1] for record in snapshot_mes_list}\n",
    "    end=time.time()\n",
    "    print('Compute in seconds: {}'.format(end-start))\n",
    "    \n",
    "    return snapshot_mes_dic\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Property measurment for a single graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "###NEW\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "graph_measurement\n",
    "    Performs specific graph measurement \n",
    "\n",
    "Parameters\n",
    "----------\n",
    "measurment_input: tuple\n",
    "    g : nx graph\n",
    "        NetworkX graph object over which measurment will be calculated for each node\n",
    "\n",
    "    measurment: string\n",
    "        Type of measurement to be performed on graph\n",
    "        \n",
    "    weight: string\n",
    "        Edge attribute to be used as weight\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "g_dir: dir\n",
    "    Dictionary with measurment values for each node\n",
    "\"\"\"\n",
    "\n",
    "def graph_measurement(measurment_input):\n",
    "    \n",
    "    # Extract inputs\n",
    "    key=measurment_input[0]\n",
    "    measurement=measurment_input[1]\n",
    "    weight=measurment_input[2]\n",
    "    bucket=measurment_input[3]\n",
    "    \n",
    "    # Download graph\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.resource('s3')\n",
    "    response = s3.Object(bucket_name=bucket, key=key).get()\n",
    "    g=pickle.loads(response['Body'].read())\n",
    "    \n",
    "    \n",
    "    if measurement=='current_betweeness_full':\n",
    "        g_dir=nx.algorithms.centrality.current_flow_betweenness_centrality(g,weight=weight)\n",
    "    \n",
    "    elif measurement=='current_betweeness':\n",
    "        g_dir=nx.algorithms.centrality.approximate_current_flow_betweenness_centrality(g,weight=weight)\n",
    "    \n",
    "    elif measurement=='current_closeness':\n",
    "        g_dir=nx.algorithms.centrality.current_flow_closeness_centrality(g,weight=weight)\n",
    "    \n",
    "    elif measurement=='closeness':\n",
    "        g_dir=nx.closeness_centrality(g)\n",
    "        \n",
    "    elif measurement=='clustering':\n",
    "        g_dir=nx.clustering(g,weight=weight)\n",
    "        \n",
    "    elif measurement=='node_count':\n",
    "        g_dir=len(g.nodes())\n",
    "        \n",
    "    elif measurement=='channels':\n",
    "        g_dir=dict(list(g.degree(g.nodes())))\n",
    "    \n",
    "    elif measurement=='capacity':\n",
    "        g_dir=dict(list(g.degree(g.nodes(),weight=weight)))\n",
    "        \n",
    "    elif measurement=='age': \n",
    "        pass\n",
    "    elif measurement=='growth': \n",
    "        pass\n",
    "    \n",
    "    \n",
    "    # Safe graph processing to S3\n",
    "    \n",
    "    extraction_id=key.split('/')[1].split('_')[0]\n",
    "    block=key.split('/')[2].split('.')[0]\n",
    "    key_out='graph_snapshots/'+extraction_id+'_connected/.data_transformations/'+measurement+'/raw_score/'+block+'.pkl'\n",
    "    pickle_byte_obj = pickle.dumps(g_dir) \n",
    "    response=s3.Object(bucket,key_out).put(Body=pickle_byte_obj)['ResponseMetadata']['HTTPStatusCode']\n",
    "    \n",
    "    block=g.graph['block']\n",
    "    \n",
    "    return (block,response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph_snapshots/1587447789_connected/.data_transformations/current_betweeness_full/505149.pkl\n"
     ]
    }
   ],
   "source": [
    "key='graph_snapshots/1587447789_connected/505149.gpickle'\n",
    "extraction_id=key.split('/')[1].split('_')[0]\n",
    "block=key.split('/')[2].split('.')[0]\n",
    "measurement='current_betweeness_full'\n",
    "key_out='graph_snapshots/'+extraction_id+'_connected/.data_transformations/'+measurement+'/'+block+'.pkl'\n",
    "print (key_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "graph_measurement\n",
    "    Performs specific graph measurement \n",
    "\n",
    "Parameters\n",
    "----------\n",
    "measurment_input: tuple\n",
    "    g : nx graph\n",
    "        NetworkX graph object over which measurment will be calculated for each node\n",
    "\n",
    "    measurment: string\n",
    "        Type of measurement to be performed on graph\n",
    "        \n",
    "    weight: string\n",
    "        Edge attribute to be used as weight\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "g_dir: dir\n",
    "    Dictionary with measurment values for each node\n",
    "\n",
    "\n",
    "def graph_measurement(measurment_input):\n",
    "    \n",
    "    g=measurment_input[0]\n",
    "    measurement=measurment_input[1]\n",
    "    weight=measurment_input[2]\n",
    "    \n",
    "    if measurement=='current_betweeness':\n",
    "        g_dir=nx.algorithms.centrality.current_flow_betweenness_centrality(g,weight=weight)\n",
    "    \n",
    "    elif measurement=='current_closeness':\n",
    "        g_dir=nx.algorithms.centrality.current_flow_closeness_centrality(g,weight=weight)\n",
    "    \n",
    "    block=g.graph['block']\n",
    "    \n",
    "    return (block,g_dir)\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Property measurment for a couple of nodes in a graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "node_measurement\n",
    "    Performs selected graph measurment on specific nodes in graph\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "g : nx graph\n",
    "    NetworkX graph object over which the measurment will be performed\n",
    "\n",
    "measurement: string\n",
    "    Type of measurement to be performend in graph\n",
    "    \n",
    "node0: int\n",
    "    Node id for node 0\n",
    "\n",
    "node1: int\n",
    "    Node id for node 1\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "node_tuple: tuple\n",
    "    Tuple of the form (node0_mes,node1_mes)\n",
    "    \n",
    "    node0_mes: float\n",
    "        Graph measurement for node0\n",
    "    node1_mes: float\n",
    "        Graph measurement for node1\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "def node_measurement(g,measurement,node0,node1):\n",
    "    \n",
    "    measurement_input=(g,measurement,'capacity')\n",
    "    block,g_dir=graph_measurement(measurement_input)\n",
    "    \n",
    "    node0_mes=g_dir[node0]\n",
    "    node1_mes=g_dir[node1]\n",
    "        \n",
    "    # Update marginal values for node0 and node1\n",
    "        \n",
    "    if (g.has_node(node0)): #If connected component of marginal graph contains node0 find betweeness\n",
    "        node0_mes=g_dir[node0]\n",
    "    else: # else update with fixed value\n",
    "        node0_mes=0\n",
    "            \n",
    "    if (g.has_node(node1)): #If connected component of marginal graph contains node1 find betweeness\n",
    "        node1_mes=g_dir[node1]\n",
    "    else: # else update with fixed value\n",
    "        node1_mes=0\n",
    "    \n",
    "    return (node0_mes,node1_mes)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incremental_closeness(bucket,graph_keys,blocks,start_point):\n",
    "    \n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.resource('s3')\n",
    "    counter=0\n",
    "    extraction_id=graph_keys[0].split('/')[1].split('_')[0]\n",
    "    responses=[]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Channel closures\n",
    "    closure_file = s3.Object(bucket_name=bucket, key='channel_closures.p').get()\n",
    "    channel_closures = pickle.loads(closure_file['Body'].read())\n",
    "    \n",
    "    \n",
    "    # Channel openings \n",
    "    opens_file = s3.Object(bucket_name=bucket, key='channel_opens.p').get()\n",
    "    channel_opens = pickle.loads(opens_file['Body'].read())\n",
    "    snapshot_clo={}\n",
    "    \n",
    "    #Initialize graph with all nodes \n",
    "    lastgraph_block=blocks[-1]\n",
    "    last_key='graph_snapshots/'+str(extraction_id)+'/'+str(lastgraph_block)+'.gpickle'\n",
    "    response = s3.Object(bucket_name=bucket, key=last_key).get()\n",
    "    G_final=pickle.loads(response['Body'].read())\n",
    "    nodes_final=list(G_final.nodes())\n",
    "    G=nx.Graph()\n",
    "    G.add_nodes_from(nodes_final)\n",
    "    prev_clo=None\n",
    "    \n",
    "    if start_point>0:\n",
    "        # Get previous graph\n",
    "        inigraph_block=blocks[start_point-1]\n",
    "        ini_key='graph_snapshots/'+str(extraction_id)+'/'+str(inigraph_block)+'.gpickle'\n",
    "        response = s3.Object(bucket_name=bucket, key=ini_key).get()\n",
    "        G_ini=pickle.loads(response['Body'].read())\n",
    "        \n",
    "        # Get previous closeness centrality\n",
    "        measurement='incremental_closeness'\n",
    "        key_clo='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/raw_score/'+str(inigraph_block)+'.pkl'\n",
    "        response = s3.Object(bucket_name=bucket, key=key_clo).get()\n",
    "        prev_clo=pickle.loads(response['Body'].read())\n",
    "      \n",
    "        G.add_edges_from(list(G_ini.edges(data=True)))\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    with tqdm(total=len(range(start_point,len(graph_keys)))) as pbar:\n",
    "        for i  in range(start_point,len(graph_keys)):\n",
    "\n",
    "            \n",
    "            block=blocks[i]\n",
    "            new_edges=channel_opens[block]\n",
    "            closed_edges=channel_closures[block]\n",
    "        \n",
    "\n",
    "            # Incremental closeness calculation for OPENS\n",
    "\n",
    "            with tqdm(total=len(new_edges),disable=True) as pbar1:\n",
    "                for edge in new_edges:\n",
    "\n",
    "                    #If edge exists in previous graph closeness doesn't change\n",
    "                    if G.has_edge(edge[0],edge[1]):\n",
    "                        new_clo=prev_clo\n",
    "                        G.edges[edge[0],edge[1]]['no_channels']+=1\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        new_clo=nx.incremental_closeness_centrality(G,(edge[0],edge[1]),prev_cc=prev_clo,insertion=True)\n",
    "                        G.add_edges_from([edge])\n",
    "                \n",
    "\n",
    "                    prev_clo=new_clo\n",
    "                    pbar1.update(1)\n",
    "                \n",
    "            \n",
    "\n",
    "            # Incremental closeness calculation for CLOSES\n",
    "\n",
    "            with tqdm(total=len(closed_edges),disable=True) as pbar2:\n",
    "                for edge in closed_edges:\n",
    "\n",
    "                    # Verify if existing edges result from multiple channels, if so, only reduce capacity otherwise remove edge\n",
    "                    no_channels=G.edges[edge[0],edge[1]]['no_channels']\n",
    "\n",
    "                    #If edge exists in previous graph closeness doesn't change\n",
    "                    if no_channels>1:\n",
    "                        new_clo=prev_clo\n",
    "                        G.edges[edge[0],edge[1]]['no_channels']-=1\n",
    "\n",
    "                    else:                                    \n",
    "                        new_clo=nx.incremental_closeness_centrality(G,(edge[0],edge[1]),prev_cc=prev_clo,insertion=False)                   \n",
    "                        G.remove_edge(edge[0],edge[1])\n",
    "\n",
    "                    prev_clo=new_clo\n",
    "                    pbar2.update(1)\n",
    "                \n",
    "\n",
    "\n",
    "            # Safe outcome\n",
    "            g_dir=new_clo\n",
    "\n",
    "\n",
    "            # Safe graph processing to S3\n",
    "            \n",
    "            measurement='incremental_closeness'\n",
    "            key_out='graph_snapshots/'+extraction_id+'_connected/.data_transformations/'+measurement+'/raw_score/'+str(block)+'.pkl'\n",
    "            pickle_byte_obj = pickle.dumps(g_dir) \n",
    "            response=s3.Object(bucket,key_out).put(Body=pickle_byte_obj)['ResponseMetadata']['HTTPStatusCode']\n",
    "            #print((block,response))\n",
    "\n",
    "\n",
    "            # Loop updates\n",
    "            pbar1.close()\n",
    "            pbar2.close()\n",
    "            pbar.update(1)\n",
    "            responses.append(response)\n",
    "            \n",
    "        \n",
    "        output={b:res for b,res in zip(blocks,responses)}\n",
    "\n",
    "           \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_shortest_path(input_tuple):\n",
    "    G=input_tuple[0]\n",
    "    u=input_tuple[1]\n",
    "    v=input_tuple[2]\n",
    "    \n",
    "    sp=nx.shortest_path_length(G, source=u, target=v)\n",
    "    \n",
    "    return sp\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit sigmoid for p by looking at sample of graphs\n",
    "# estimate mid closeness by samplig closeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sp_closeness(G):\n",
    "    \n",
    "    nodes=list(G.nodes())\n",
    "    n=len(nodes)\n",
    "    sp_matrix=np.zeros((len(nodes),len(nodes)))\n",
    "    sp_matrix=sp_matrix.astype(object)\n",
    "    print(sp_matrix.dtype)\n",
    "    \n",
    "    dic_clo={}\n",
    "    \n",
    "    \n",
    "    for i in range(n):\n",
    "\n",
    "        for j in range(i,n):\n",
    "            # Calculate delayed shortest path\n",
    "            input_tuple=(G,nodes[i],nodes[j])\n",
    "            sp=dask.delayed(len_shortest_path)(input_tuple)\n",
    "            #print(type(sp))\n",
    "            sp_matrix[i][j]=sp\n",
    "            sp_matrix[j][i]=sp\n",
    "        \n",
    "        sp_sum=np.array(dask.compute(*sp_matrix[i].tolist())).sum()\n",
    "        clo_i=n-1/sp_sum\n",
    "        dic_clo[i]=clo_i\n",
    "    return dic_clo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collection_measure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-396-02a74f444044>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdic_clo_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msp_closeness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Compute in seconds: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-391-49ab4ce22c99>\u001b[0m in \u001b[0;36msp_closeness\u001b[0;34m(G)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0msp_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0msp_sum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msp_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mclo_i\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msp_sum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mdic_clo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclo_i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dask_keys__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0mpostcomputes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dask_postcompute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrepack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostcomputes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, dsk, keys, restrictions, loose_restrictions, resources, sync, asynchronous, direct, retries, priority, fifo_timeout, actors, **kwargs)\u001b[0m\n\u001b[1;32m   2594\u001b[0m                     \u001b[0mshould_rejoin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2595\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2596\u001b[0;31m                 \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masynchronous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2597\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2598\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(self, futures, errors, direct, asynchronous)\u001b[0m\n\u001b[1;32m   1892\u001b[0m                 \u001b[0mdirect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m                 \u001b[0mlocal_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1894\u001b[0;31m                 \u001b[0masynchronous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1895\u001b[0m             )\n\u001b[1;32m   1896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/distributed/client.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             return sync(\n\u001b[0;32m--> 778\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m             )\n\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n",
      "_GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=CancelledError()>\n",
      "concurrent.futures._base.CancelledError\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "start=time.time()\n",
    "dic_clo_test=sp_closeness(G_test)\n",
    "end=time.time()\n",
    "print(sp)\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print(list(dic_clo_test.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With aproximate betweeness: 3.782655715942383s \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nmeasurement_input_2=(extract_keys[10000],'current_betweeness_full','capacity',bucket)\\n\\nstart=time.time()\\ntest_full = graph_measurement(measurement_input_2)\\nend=time.time()\\n\\nprint('With full betweeness: {}s '.format(end-start))\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute in seconds: 10.438627481460571\n"
     ]
    }
   ],
   "source": [
    "snapshot_nodes=collection_measure(bucket,extract_keys,'node_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(snapshot_nodes.items())[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Baseline Current Betweeness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute in seconds: 8009.403836488724\n"
     ]
    }
   ],
   "source": [
    "# Calculate Baseline betweeness\n",
    "snapshot_bet=collection_measure(bucket,extract_keys,'current_betweeness')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save to S3 succesful\n"
     ]
    }
   ],
   "source": [
    "# Save baseline current betweeness to S3\n",
    "response=pickle_save_s3(snapshot_bet,blocks,extraction_id,'snapshot_bet')\n",
    "if response['HTTPStatusCode']==200:\n",
    "    print('Save to S3 succesful')\n",
    "else:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Baseline Current Closeness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Baseline current closeness\n",
    "#snapshot_clo=collection_measure(bucket,extract_keys,'current_closeness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate incremental closeness\n",
    "snapshot_clo=incremental_closeness(bucket,extract_keys,blocks,1425+3073)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graph_snapshots/1587447789_connected/535029.gpickle'"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_keys[1402]\n",
    "\n",
    "#channel_opens[1407]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "key='graph_snapshots/1587447789_connected/'+str(blocks[-100])+'.gpickle'\n",
    "response = s3.Object(bucket_name=bucket, key=key).get()\n",
    "G_test=pickle.loads(response['Body'].read())\n",
    "\n",
    "node1=random.choice(list(G.nodes()))\n",
    "node2=random.choice(list(G.nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6306, 2470)\n",
      "0\n",
      "Compute in seconds: 9.965896606445312e-05\n"
     ]
    }
   ],
   "source": [
    "print((node1,node2))\n",
    "#len(G_test.nodes())\n",
    "start=time.time()\n",
    "sp=nx.shortest_path_length(G_test, source=node1, target=node1)\n",
    "end=time.time()\n",
    "print(sp)\n",
    "print('Compute in seconds: {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0.2, 2: 0.2, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0}\n",
      "[(1, 2)]\n",
      "{1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2, 5: 0.0, 6: 0.0}\n",
      "[(1, 2), (3, 4)]\n",
      "{1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2, 5: 0.2, 6: 0.2}\n",
      "[(1, 2), (3, 4), (5, 6)]\n",
      "{1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2, 5: 0.0, 6: 0.0}\n",
      "[(1, 2), (3, 4)]\n"
     ]
    }
   ],
   "source": [
    "G=nx.Graph()\n",
    "G.add_nodes_from([1,2,3,4,5,6])\n",
    "#G.add_edge(2,1)\n",
    "new_clo=nx.incremental_closeness_centrality(G,(2,1),prev_cc=None,insertion=True)\n",
    "G.add_edge(2,1)\n",
    "print(new_clo)\n",
    "print(G.edges())\n",
    "\n",
    "\n",
    "new_clo=nx.incremental_closeness_centrality(G,(3,4),prev_cc=new_clo,insertion=True)\n",
    "G.add_edge(3,4)\n",
    "print(new_clo)\n",
    "print(G.edges())\n",
    "\n",
    "\n",
    "new_clo=nx.incremental_closeness_centrality(G,(5,6),prev_cc=new_clo,insertion=True)\n",
    "G.add_edge(5,6)\n",
    "print(new_clo)\n",
    "print(G.edges())\n",
    "\n",
    "\n",
    "new_clo=nx.incremental_closeness_centrality(G,(5,6),prev_cc=new_clo,insertion=False)\n",
    "G.remove_edge(5,6)\n",
    "\n",
    "print(new_clo)\n",
    "print(G.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save to S3 succesful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec1dca6cf3d443abad6f463e8fad8fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1400.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "% of nodes with positive closeness per Block : 0.9888914970559514\n",
      "Total Blocks: 1400\n"
     ]
    }
   ],
   "source": [
    "# TEST all saved correctly\n",
    "responses=np.array([r for b,r in snapshot_clo.items()])\n",
    "\n",
    "if np.amax(responses)==np.amin(responses): #and len(responses) == len(extract_keys):\n",
    "    print('Save to S3 succesful')\n",
    "else:\n",
    "    print('Error saving to S3')\n",
    "\n",
    "range_test=[]\n",
    "no_blocks=1400\n",
    "    \n",
    "with tqdm(total=no_blocks) as pbar:\n",
    "    for i in range(no_blocks):\n",
    "\n",
    "        measurement='incremental_closeness'    \n",
    "        #rand_block=str(random.choice(blocks[:1200]))    \n",
    "        rand_block=str(blocks[i])\n",
    "\n",
    "        test_key='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/raw_score/'+rand_block+'.pkl'\n",
    "        test_file = s3.Object(bucket_name=bucket, key=test_key).get()\n",
    "        test_object = pickle.loads(test_file['Body'].read())\n",
    "\n",
    "        graph_key='graph_snapshots/'+str(extraction_id)+'_connected/'+str(rand_block)+'.gpickle'\n",
    "        graph_response = s3.Object(bucket_name=bucket, key=graph_key).get()\n",
    "        G=pickle.loads(graph_response['Body'].read())\n",
    "\n",
    "\n",
    "        #print('Example of {} saved for block {}:'.format(measurement,rand_block))\n",
    "        #print(list(test_object.items())[:100]) \n",
    "        #print('Total nodes graph: {}'.format(len(G.nodes())))\n",
    "\n",
    "        connected_nodes=[(b,cent) for b,cent in list(test_object.items()) if cent>0]\n",
    "        #print('Total entries: {}'.format(len(connected_nodes)))\n",
    "\n",
    "        node_cons=[]\n",
    "        for node in list(G.nodes()):\n",
    "\n",
    "            node_con=test_object[node]\n",
    "            \n",
    "          \n",
    "            if node_con>0:\n",
    "                node_cons.append(1)\n",
    "            else:\n",
    "                node_cons.append(0)\n",
    "            \n",
    "            \n",
    "\n",
    "        block_test=np.array(node_cons).sum()/len(G.nodes)          \n",
    "\n",
    "        '''\n",
    "        if block_test==1:\n",
    "            test=1\n",
    "        else:\n",
    "            test=0\n",
    "        '''\n",
    "\n",
    "        range_test.append(block_test)\n",
    "        #range_test.append(test)\n",
    "        pbar.update(1)\n",
    "\n",
    "print('% of nodes with positive closeness per Block : {}'.format(np.array(range_test).mean()))\n",
    "#print('Blocks with closeness for all nodes: {}'.format(np.array(range_test).sum()))\n",
    "print('Total Blocks: {}'.format(no_blocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9950617283950617"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(range_test)[1399]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Baseline Channels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute in seconds: 186.33308386802673\n"
     ]
    }
   ],
   "source": [
    "# Calculate Baseline degree\n",
    "snapshot_channels=collection_measure(bucket,extract_keys,'channels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST all saved correctly\n",
    "responses=np.array([r for b,r in snapshot_channels.items()])\n",
    "\n",
    "if np.amax(responses)==np.amin(responses) and len(responses) == len(extract_keys):\n",
    "    print('Save to S3 succesful')\n",
    "else:\n",
    "    print('Error saving to S3')\n",
    "    \n",
    "measurement='channels'    \n",
    "rand_block=str(random.choice(blocks))    \n",
    "test_key='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/raw_score/'+rand_block+'.pkl'\n",
    "test_file = s3.Object(bucket_name=bucket, key=test_key).get()\n",
    "test_object = pickle.loads(test_file['Body'].read())\n",
    "print('Example of {} saved for block {}:'.format(measurement,rand_block))\n",
    "print(list(test_object.items())[:10])    \n",
    "print('Total entries: {}'.format(len(list(test_object.items()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Baseline Capacity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute in seconds: 205.32206177711487\n"
     ]
    }
   ],
   "source": [
    "# Calculate Baseline degree\n",
    "snapshot_capacity=collection_measure(bucket,extract_keys,'capacity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save to S3 succesful\n",
      "Example of capacity saved for block 586042:\n",
      "[(6038, 8878679), (5314, 14026340), (934, 1171934), (3023, 1111934), (3452, 2063908), (576, 40000), (3436, 6948131), (3310, 100000), (2378, 400000), (4223, 24298841)]\n",
      "Total entries: 4920\n"
     ]
    }
   ],
   "source": [
    "# TEST all saved correctly\n",
    "responses=np.array([r for b,r in snapshot_capacity.items()])\n",
    "\n",
    "if np.amax(responses)==np.amin(responses) and len(responses) == len(extract_keys):\n",
    "    print('Save to S3 succesful')\n",
    "else:\n",
    "    print('Error saving to S3')\n",
    "    \n",
    "measurement='capacity'    \n",
    "rand_block=str(random.choice(blocks))    \n",
    "test_key='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+measurement+'/raw_score/'+rand_block+'.pkl'\n",
    "test_file = s3.Object(bucket_name=bucket, key=test_key).get()\n",
    "test_object = pickle.loads(test_file['Body'].read())\n",
    "print('Example of {} saved for block {}:'.format(measurement,rand_block))\n",
    "print(list(test_object.items())[:10])\n",
    "print('Total entries: {}'.format(len(list(test_object.items()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparissons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Compare property changes for nodes in a collection of graphs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###NEW \n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "collection_compare\n",
    "    Iterates over blocks to calculate marginal change in metric for nodes that made decisions (opens/closures)\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "blocks : list\n",
    "    List of blocks extracted when reading graphs\n",
    "    \n",
    "    \n",
    "dec_dic: dic\n",
    "    Dictionary with channel decisions (open or closure) per block\n",
    "    \n",
    "    \n",
    "graph_snapshots: list\n",
    "    List of delayed dask objects each pointing to a graph snapshot to be loaded from S3\n",
    "\n",
    "snapshots_base: dic\n",
    "    Dictionary of dictionaries containing the base measurments per block per node\n",
    "\n",
    "measurement: string\n",
    "    Name of the type of measurment that will applied to the graph\n",
    "    \n",
    "type_dec: string\n",
    "    The type of decisions that will be analyzed 'opens' or 'closures'\n",
    "    \n",
    "\n",
    "Returns\n",
    "-------\n",
    "futures: list\n",
    "    List of tuples of the form (dic_node0,dic_node1) where dic_node0/1 is a dictionary containing the marginal changes for node0/1 \n",
    "    for every decision in a given block. The dictionaries are future dask objects that still need to be explicitly computed. \n",
    "\n",
    "\"\"\"\n",
    "def collection_compare(blocks,dec_dic,graph_keys,snapshots_base,measurement,type_dec):\n",
    "\n",
    "    futures_list=[] # list to populate with futures per block\n",
    "    \n",
    "    with tqdm(total=len(range(1,len(graph_keys)))) as pbar:\n",
    "        for i in range(1,len(graph_keys)): # iterate through blocks\n",
    "\n",
    "            # extract information from parameters and construct input tuple to delayed function\n",
    "            block=blocks[i]\n",
    "            block_prev=blocks[i-1]\n",
    "            block_dec=dec_dic[block]\n",
    "            key=graph_keys[i-1] # Pass previous graph\n",
    "            block_base=snapshots_base[block_prev]\n",
    "            block_res=snapshots_base[block]\n",
    "            input_tuple=(block,g,block_dec,block_base,measurement,type_dec,block_res)\n",
    "            \n",
    "            # submit to delayed function and append to list\n",
    "            output_tuple=dask.delayed(graph_compare)(input_tuple)\n",
    "            futures_list.append(output_tuple)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # persist to disk and return\n",
    "    futures = dask.persist(*futures_list)\n",
    "    return futures\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "collection_compare\n",
    "    Iterates over blocks to calculate marginal change in metric for nodes that made decisions (opens/closures)\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "blocks : list\n",
    "    List of blocks extracted when reading graphs\n",
    "    \n",
    "    \n",
    "dec_dic: dic\n",
    "    Dictionary with channel decisions (open or closure) per block\n",
    "    \n",
    "    \n",
    "graph_snapshots: list\n",
    "    List of delayed dask objects each pointing to a graph snapshot to be loaded from S3\n",
    "\n",
    "snapshots_base: dic\n",
    "    Dictionary of dictionaries containing the base measurments per block per node\n",
    "\n",
    "measurement: string\n",
    "    Name of the type of measurment that will applied to the graph\n",
    "    \n",
    "type_dec: string\n",
    "    The type of decisions that will be analyzed 'opens' or 'closures'\n",
    "    \n",
    "\n",
    "Returns\n",
    "-------\n",
    "futures: list\n",
    "    List of tuples of the form (dic_node0,dic_node1) where dic_node0/1 is a dictionary containing the marginal changes for node0/1 \n",
    "    for every decision in a given block. The dictionaries are future dask objects that still need to be explicitly computed. \n",
    "\n",
    "\n",
    "def collection_compare(blocks,dec_dic,graph_snapshots,snapshots_base,measurement,type_dec):\n",
    "\n",
    "    futures_list=[] # list to populate with futures per block\n",
    "    \n",
    "    with tqdm(total=len(range(1,len(graph_snapshots)))) as pbar:\n",
    "        for i in range(1,len(graph_snapshots)): # iterate through blocks\n",
    "\n",
    "            # extract information from parameters and construct input tuple to delayed function\n",
    "            block=blocks[i]\n",
    "            block_prev=blocks[i-1]\n",
    "            block_dec=dec_dic[block]\n",
    "            g=graph_snapshots[i-1] # Pass previous graph\n",
    "            block_base=snapshots_base[block_prev]\n",
    "            block_res=snapshots_base[block]\n",
    "            input_tuple=(block,g,block_dec,block_base,measurement,type_dec,block_res)\n",
    "            \n",
    "            # submit to delayed function and append to list\n",
    "            output_tuple=dask.delayed(graph_compare)(input_tuple)\n",
    "            futures_list.append(output_tuple)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # persist to disk and return\n",
    "    futures = dask.persist(*futures_list)\n",
    "    return futures\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION: Compare property changes for nodes in a graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "graph_compare\n",
    "    Calculates marginal change in metric for node0, node1 make decisions (open/close channels) in a single block\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "input_tuple : tuple\n",
    "    \n",
    "    block: int\n",
    "        Block number\n",
    "    g: nx_graph \n",
    "        Graph snapshot (as dask delayed object)\n",
    "    block_dec: list\n",
    "        List of tuples in nx edge format (u,v,att_dic) for all the decisions (channel opens/closures) made in that block  \n",
    "    block_base: dic\n",
    "        Dictionary of base measurements for each node in the graph snapshot corresponding to that block\n",
    "    measurement: string\n",
    "        Name of measurement to be computed\n",
    "    type_dec: string\n",
    "    The type of decisions that will be analyzed 'opens' or 'closures'\n",
    "    \n",
    "    block_res: dic\n",
    "        Dictionary of base measurements for each node in the graph snapshot corresponding to the next block\n",
    "    \n",
    "\n",
    "Returns\n",
    "-------\n",
    "nodes_mar_dic: tuple\n",
    "    Tuples of the form (mar_node0_dic_i,mar_node0_dic_i) where each element in the tuple is a dictionary containing the marginal changes for node0/1 \n",
    "    for every node0 and node1 involved in a decision (channel open/closures) in the block.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def graph_compare(input_tuple):\n",
    "    \n",
    "    block=input_tuple[0]\n",
    "    key=input_tuple[1]\n",
    "    block_dec=input_tuple[2]\n",
    "    block_base=input_tuple[3]\n",
    "    measurement=input_tuple[4]\n",
    "    type_dec=input_tuple[5]\n",
    "    block_res=input_tuple[6]\n",
    "   \n",
    "    mar_node0_dic_i={} # dictionary to story function output\n",
    "    mar_node1_dic_i={} \n",
    "    \n",
    "    \n",
    "    # Load data\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.resource('s3')\n",
    "    response = s3.Object(bucket_name=bucket, key=key).get()\n",
    "    G=pickle.loads(response['Body'].read())\n",
    "    \n",
    "    ###########################---------------------\n",
    "    ##if decisiono \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # For each decision calculate marginal change in measure for node0 and node1\n",
    "    for edge in block_dec:\n",
    "        \n",
    "        # Extract info about channel\n",
    "        \n",
    "        node0=edge[0]\n",
    "        node1=edge[1]\n",
    "        channel_id=edge[2]['channel_id']\n",
    "        capacity=edge[2]['capacity']\n",
    "\n",
    "        \n",
    "        # Copy original graph\n",
    "        g_mar=G.copy()   \n",
    "        old_nodes=False\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Retrieve base measurement before channel if nodes existed, else define base measure as 0\n",
    "        if (g_mar.has_node(node0)):\n",
    "            node0_base=block_base[node0]\n",
    "            old_nodes=True\n",
    "        else:\n",
    "            node0_base=0\n",
    "            \n",
    "        if (g_mar.has_node(node1)):\n",
    "            node1_base=block_base[node1]\n",
    "            old_nodes=True\n",
    "        else:\n",
    "            node1_base=0\n",
    "        \n",
    "            \n",
    "        if old_nodes: # If at least one node is old (part of the connected graph)\n",
    "            \n",
    "            if type_dec=='mar_opens': # marginal calculation for opens\n",
    "                \n",
    "                \n",
    "                # Define and add edges and calculate betweeness if at least one of the nodes is in graph \n",
    "                edge_list=[edge]\n",
    "                \n",
    "                \n",
    "                # If channel exists increase capacity\n",
    "                \n",
    "                if g_mar.has_edge(node0,node1):\n",
    "                   \n",
    "                    g_mar.edges[node0,node1]['capacity']+=capacity\n",
    "                    g_mar.edges[node0,node1]['no_channels']+=1\n",
    "\n",
    "                else:\n",
    "                    g_mar.add_edges_from(edge_list)\n",
    "                \n",
    "                \n",
    "                g_mar_mes=node_measurement(g_mar,measurement,node0,node1)\n",
    "                \n",
    "                # Update measurement values after marginal change\n",
    "                node0_new_mes=g_mar_mes[0]\n",
    "                node1_new_mes=g_mar_mes[1]\n",
    "            \n",
    "            elif type_dec=='mar_closures': # marginal calculation for closes\n",
    "                \n",
    "                # Define and remove edges, define new connected graph and calculate betweeness \n",
    "                edge_list=[(node0,node1)]\n",
    "                \n",
    "                \n",
    "                # If channel exists decrease capacity\n",
    "                if g_mar.edges[node0,node1]['no_channels']>1:\n",
    "                    g_mar.edges[node0,node1]['capacity']-=capacity\n",
    "                    g_mar.edges[node0,node1]['no_channels']-=1\n",
    "                \n",
    "                else: \n",
    "                    g_mar.remove_edges_from(edge_list) \n",
    "                    connected_components=[c for c in nx.algorithms.components.connected_components(g_mar)]\n",
    "                    g_mar=g_mar.subgraph(connected_components[0]).copy()\n",
    "                    \n",
    "                g_mar_mes=node_measurment(g_mar,measurement,node0,node1)\n",
    "                node0_new_mes=g_mar_mes[0]\n",
    "                node1_new_mes=g_mar_mes[1]\n",
    "                \n",
    "            elif type_dec=='actual': # actual calculation for both opens and closures\n",
    "                \n",
    "                # Check individualy if in the graph for the resulting block the node is present (in the connected component, \n",
    "                # if not assign measurment to 0. \n",
    "                \n",
    "                try:\n",
    "                    node0_new_mes=block_res[node0]\n",
    "\n",
    "                except KeyError:\n",
    "                    node0_new_mes=0\n",
    "\n",
    "                try:\n",
    "                    node1_new_mes=block_res[node1]\n",
    "\n",
    "                except KeyError:\n",
    "                    node1_new_mes=0\n",
    "\n",
    "                \n",
    "                   \n",
    "            node0_mar=(node0_new_mes-node0_base)\n",
    "            node1_mar=(node1_new_mes-node1_base) \n",
    "        \n",
    "        \n",
    "        else: # If both nodes are new (outside of connected graph) their marginal decision outcome is 0\n",
    "            node0_mar=0\n",
    "            node1_mar=0\n",
    "\n",
    "        \n",
    "        # Update dictionary - new betweenness\n",
    "        mar_node0_dic_i[channel_id]=node0_mar\n",
    "        mar_node1_dic_i[channel_id]=node1_mar\n",
    "        \n",
    "    \n",
    "    return (mar_node0_dic_i,mar_node1_dic_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MARGINAL - CURRENT BETWEENESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Marginal current betweeness for opens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute marginal betweeness for channel openings\n",
    "\n",
    "futures_bet_maropen=collection_compare(blocks,channel_opens,graph_snapshots,snapshot_bet,measurement='current_betweeness',type_dec='mar_opens')\n",
    "start=time.time()\n",
    "bet_maropen_diclist = dask.compute(*futures_bet_maropen)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(bet_maropen_diclist)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to marginal current betweeness for opens into decisions Dataframe\n",
    "\n",
    "bet_maropen_channels=add_columns(bet_maropen_diclist,decisions_df,'bet_maropen_node0','bet_maropen_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with marginal current betweeness for opens: {}'.format(len(bet_maropen_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(bet_maropen_channels)].head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Marginal current betweeness for closures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute marginal betweeness for channel closures\n",
    "\n",
    "futures_bet_marclose=collection_compare(blocks,channel_closures,graph_snapshots,snapshot_bet,measurement='current_betweeness',type_dec='mar_closures')\n",
    "start=time.time()\n",
    "bet_marclose_diclist = dask.compute(*futures_bet_marclose)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(bet_marclose_diclist)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to marginal current betweeness for closures into decisions Dataframe\n",
    "\n",
    "bet_marclose_channels=add_columns(bet_marclose_diclist,decisions_df,'bet_marclose_node0','bet_marclose_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with marginal current betweeness for closures: {}'.format(len(bet_marclose_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(bet_marclose_channels)].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MARGINAL - CURRENT CLOSENESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Marginal current closeness for opens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute marginal betweeness for channel openings\n",
    "\n",
    "futures_clo_maropen=collection_compare(blocks,channel_opens,graph_snapshots,snapshot_clo,measurement='current_closeness',type_dec='mar_opens')\n",
    "start=time.time()\n",
    "clo_maropen_diclist = dask.compute(*futures_clo_maropen)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(clo_maropen_diclist)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to marginal current betweeness for opens into decisions Dataframe\n",
    "\n",
    "clo_maropen_channels=add_columns(clo_maropen_diclist,decisions_df,'clo_maropen_node0','clo_maropen_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with marginal current closeness for opens: {}'.format(len(clo_maropen_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(clo_maropen_channels)].head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Marginal current closeness for closures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute marginal current closeness for channel closures\n",
    "\n",
    "futures_clo_marclose=collection_compare(blocks,channel_closures,graph_snapshots,snapshot_clo,measurement='current_closeness',type_dec='mar_closures')\n",
    "start=time.time()\n",
    "clo_marclose_diclist = dask.compute(*futures_clo_marclose)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(clo_marclose_diclist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to marginal current betweeness for opens into decisions Dataframe\n",
    "\n",
    "clo_marclose_channels=add_columns(clo_marclose_diclist,decisions_df,'clo_marclose_node0','clo_marclose_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with marginal current closeness for closures: {}'.format(len(clo_marclose_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(clo_marclose_channels)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACTUAL - CURRENT BETWEENESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Actual current betweeness for opens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute actual current betweeness for channel opens\n",
    "\n",
    "futures_bet_actopen=collection_compare(blocks,channel_opens,graph_snapshots,snapshot_bet,measurement='current_betweeness',type_dec='actual')\n",
    "start=time.time()\n",
    "bet_actopen_diclist = dask.compute(*futures_bet_actopen)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(bet_actopen_diclist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to marginal current betweeness for opens into decisions Dataframe\n",
    "\n",
    "bet_actopen_channels=add_columns(bet_actopen_diclist,decisions_df,'bet_actopen_node0','bet_actopen_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with actual current betweeness for opens: {}'.format(len(bet_actopen_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(bet_actopen_channels)].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Actual current betweeness for closures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute actual current betweeness for channel closures\n",
    "\n",
    "futures_bet_actclose=collection_compare(blocks,channel_closures,graph_snapshots,snapshot_bet,measurement='current_betweeness',type_dec='actual')\n",
    "start=time.time()\n",
    "bet_actclose_diclist = dask.compute(*futures_bet_actclose)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(bet_actclose_diclist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to marginal current betweeness for opens into decisions Dataframe\n",
    "\n",
    "bet_actclose_channels=add_columns(bet_actclose_diclist,decisions_df,'bet_actclose_node0','bet_actclose_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with actual current betweeness for closures: {}'.format(len(bet_actclose_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(bet_actclose_channels)].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACTUAL - CURRENT CLOSENESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Actual current closeness for opens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute actual current closeness for channel opens\n",
    "\n",
    "futures_clo_actopen=collection_compare(blocks,channel_opens,graph_snapshots,snapshot_bet,measurement='current_closeness',type_dec='actual')\n",
    "start=time.time()\n",
    "clo_actopen_diclist = dask.compute(*futures_clo_actopen)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(bet_actopen_diclist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to marginal current closeness for opens into decisions Dataframe\n",
    "\n",
    "clo_actopen_channels=add_columns(clo_actopen_diclist,decisions_df,'clo_actopen_node0','clo_actopen_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with actual current closeness for opens: {}'.format(len(clo_actopen_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(clo_actopen_channels)].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCRIPT: Actual current closeness for closures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute actual current closeness for channel closures\n",
    "\n",
    "futures_clo_actclose=collection_compare(blocks,channel_closures,graph_snapshots,snapshot_bet,measurement='current_closeness',type_dec='actual')\n",
    "start=time.time()\n",
    "clo_actclose_diclist = dask.compute(*futures_clo_actclose)\n",
    "end=time.time()\n",
    "print('Compute in seconds: {}'.format(end-start))\n",
    "print('Size in memory: {}'.format(sys.getsizeof(clo_actclose_diclist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to marginal current closeness for opens into decisions Dataframe\n",
    "\n",
    "clo_actclose_channels=add_columns(clo_actclose_diclist,decisions_df,'clo_actclose_node0','clo_actclose_node1')\n",
    "\n",
    "# Updated DataFrame\n",
    "\n",
    "print('Rows edited with actual current closeness for closures: {}'.format(len(clo_actclose_channels)))\n",
    "decisions_df[decisions_df['short_channel_id'].isin(clo_actclose_channels)].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise stability \n",
    "\n",
    "- **Marginal betweenness (bet_mar_nodei)**: The % change between the betweenness centrality, for the node under analysis, given the graph from the previous block and the betweenness centrality of the resulting graph after enacting the decission (adding or removing a channel). Weighted current betweenness centrality is used for this measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Marginal betweenness for opens** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Why is length of Dataframe longer than the number of snapshots extracted? Could it be that some channels appear more than once in dataframe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Marginal betweenness for closures** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Actual change in betweenness (bet_act_nodei)**: The % change between the betweenness centrality, for the node under analysis, given the graph from the previous block and the betweenness centrality of the resulting graph after enacting **all** the decissions (adding or removing a channels) in the current block. Weighted current betweenness centrality is used for this measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Marginal betweeness pairwise stability (bet_mar_pairst/open/close)**: Evaluates if given the marginal graph that results from just enacting this decission is consistent with pairwise stability, from a betweenness perspective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MARGINAL - Add column with check for pairwise stability compatability using marginal outcomes\n",
    "\n",
    "# OPEN - Channel is opened if both nodes gain\n",
    "\n",
    "# Define function \n",
    "def bet_pairst_maropen(row):\n",
    "    if not math.isnan(row['bet_mar_node0']):\n",
    "        pairst=(row['bet_maropen_node0']>=0 and row['bet_maropen_node1']>=0)\n",
    "    else:\n",
    "        pairst=row['bet_maropen_node0']\n",
    "    return pairst\n",
    "\n",
    "# Apply function\n",
    "decisions_df['bet_mar_pairstopen']=decisions_df.apply(bet_pairst_maropen,axis=1)\n",
    "\n",
    "# CLOSE - Channel is closed if at least one node is better off\n",
    "\n",
    "# Define function \n",
    "def bet_pairst_marclose(row):\n",
    "    if not math.isnan(row['bet_marclose_node0']):\n",
    "        pairst=(row['bet_marclose_node0']>0 or row['bet_marclose_node1']>0)\n",
    "    else:\n",
    "        pairst=row['bet_marclose_node0']\n",
    "    return pairst\n",
    "\n",
    "# Apply function\n",
    "decisions_df['bet_mar_pairstclose']=decisions_df.apply(bet_pairst_marclose,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MARGINAL OPEN\n",
    "decisions_df[decisions_df['bet_mar_node0'].notnull()][['bet_mar_node0','bet_mar_node1','bet_mar_pairstopen']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MARGINAL CLOSE\n",
    "decisions_df[decisions_df['bet_marclose_node0'].notnull()][['bet_marclose_node0','bet_marclose_node1','bet_mar_pairstclose']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Actual betweeness pairwise stability (bet_act_pairstopen/close)**: Evaluates if given the marginal graph that results from all the decisions in the block is consitend with pairwise stability, from a betweenness perspective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTUAL - Add column with check for pairwise stability compatability using marginal outcomes\n",
    "\n",
    "# OPEN - Channel is opened if both nodes gain\n",
    "\n",
    "# Define function \n",
    "def bet_pairst_actopen(row):\n",
    "    if not math.isnan(row['bet_actopen_node0']):\n",
    "        pairst=(row['bet_actopen_node0']>=0 and row['bet_actopen_node1']>=0)\n",
    "    else:\n",
    "        pairst=row['bet_actopen_node0']\n",
    "    return pairst\n",
    "\n",
    "# Apply function\n",
    "decisions_df['bet_act_pairstopen']=decisions_df.apply(bet_pairst_actopen,axis=1)\n",
    "\n",
    "# CLOSE - Channel is closed if at least one node is better off\n",
    "\n",
    "# Define function \n",
    "def bet_pairst_actclose(row):\n",
    "    if not math.isnan(row['bet_actclose_node0']):\n",
    "        pairst=(row['bet_actclose_node0']>0 or row['bet_actclose_node1']>0)\n",
    "    else:\n",
    "        pairst=row['bet_actclose_node0']\n",
    "    return pairst\n",
    "\n",
    "# Apply function\n",
    "decisions_df['bet_act_pairstclose']=decisions_df.apply(bet_pairst_actclose,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ACTUAL OPEN\n",
    "decisions_df[decisions_df['bet_actopen_node0'].notnull()][['bet_actopen_node0','bet_actopen_node1','bet_act_pairstopen']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ACTUAL CLOSE\n",
    "decisions_df[decisions_df['bet_actclose_node0'].notnull()][['bet_actclose_node0','bet_actclose_node1','bet_act_pairstclose']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Updated DataFrame to S3\n",
    "\n",
    "# Create S3 resource and define values\n",
    "session = boto3.session.Session()\n",
    "s3 = session.resource('s3')\n",
    "csv_buffer = io.StringIO()\n",
    "\n",
    "# File path and name ([extraction_id]snapshot_bet-[no_blocks]-[start_block]-[end_block])\n",
    "key_decisions_df='graph_snapshots/'+str(extraction_id)+'_connected/.data_transformations/'+str(extraction_id)+'decisions_df_bet-'+str(no_blocks)+'-'+str(start_block)+'-'+str(end_block)+'.csv'\n",
    "\n",
    "\n",
    "# Safe DataFrame\n",
    "decisions_df.to_csv(csv_buffer)\n",
    "s3.Object(bucket, key_decisions_df).put(Body=csv_buffer.getvalue())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Save\n",
    "decisions_df_test_load = s3.Object(bucket_name=bucket, key=key_decisions_df).get()\n",
    "decisions_df_test=pd.read_csv(io.BytesIO(decisions_df_test_load['Body'].read()),index_col=0)\n",
    "decisions_df_test==decisions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions_df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency\n",
    "- **Average betweeness per block (bet_effic)**: Average betweenness centrality for all the nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nash stability \n",
    "\n",
    "- **% Change with respect to not making decision (bet_binstat_deltai)**: The % change in betwewnness centrality, for the node under analysis, given the resulting graph after all of the decissions have been executed. \n",
    "- **Nash compatible - binary strategy (bet_binstat_nash)**: Returns true if given the other decissions enacted in the block not making decision would have NOT have resulted in higher betweenness centrality. This tells me if my strategy helped me be better off (took into account what others were doing)\n",
    "\n",
    "(Optional approaches - Check for tracktability)\n",
    "- **Nash compatible - close only strategy (bet_closestat_nash)**: Returns true if given the other decissions enacted in the block, closing any other channels would NOT have not resulted in higher betwneenness centrality. (NOTE: Check if there are combinatorial considerations, if so just look at closings up to x) \n",
    "- **Nash compatible - close/open (bet_allstat_nash)**: Returns true if given the other decissions enacted in the block, closing any other channels (with any node) or opening a channel with one of the round participants would NOT have not resulted in lower betwneenness centrality. (NOTE: To make it reasonable and constraint the strategy space only consider 'similar nodes' or with relationships in the past?).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Connectivity\n",
    "\n",
    "### Pairwise stability \n",
    "\n",
    "- **Marginal % change in connectivity (con_mar_deltai)**: The % change between the shortest path average, for the node under analysis, given the graph from the previous block and the shortest path average of the resulting graph after enacting the decission (adding or removing a channel). Weighted shortest path (_single_source_dijkstra_path_) is used for this measure.\n",
    "\n",
    "- **Actual % change in connectivity (con_act_deltai)**: The % change between the shortest path average, for the node under analysis, given the graph from the previous block and the shortest path average of the resulting graph after enacting **all** the decissions (adding or removing a channels) in the current block. Weighted shortest path (_single_source_dijkstra_path_) is used for this measure.\n",
    "\n",
    "- **Marginal connectivity pairwise stability (con_mar_pairstab)**: Evaluates if given the marginal graph that results from just enacting this decission is consistent with pairwise stability, from a connectivity perspective.\n",
    "\n",
    "- **Actual connectivity pairwise stability (con_act_pairstab)**: Evaluates if given the marginal graph that results from all the decisions in the block is consitend with pairwise stability, from a connectivity perspective.  \n",
    "\n",
    "\n",
    "\n",
    "### Nash stability \n",
    "\n",
    "- **% Change with respect to not making decision (con_binstat_deltai)**: The % change in shortest path average, for the node under analysis, given the resulting graph after all of the decissions have been executed. \n",
    "- **Nash compatible - binary strategy (con_binstat_nash)**: Returns true if given the other decissions enacted in the block not making decision would have NOT have resulted in higher shortest path average. NOTE: This indicates if the strategy selected made the node better off (took into account what others were doing)\n",
    "\n",
    "(Optional approaches - Check for tracktability)\n",
    "- **Nash compatible - close only strategy (con_closestat_nash)**: Returns true if given the other decissions enacted in the block, closing any other channels would NOT have not resulted in higher shortest path average. (NOTE: Check if there are combinatorial considerations, if so just look at closings up to x) \n",
    "- **Nash compatible - close/open (con_allstat_nash)**: Returns true if given the other decissions enacted in the block, closing any other channels (with any node) or opening a channel with one of the round participants would NOT have not resulted in lower shortest path average. (NOTE: To make it reasonable and constraint the strategy space only consider 'similar nodes' or with relationships in the past?).\n",
    "\n",
    "\n",
    "\n",
    "### Efficiency\n",
    "- **Average betweeness per block (bet_effic)**: Average shortest path average for all the nodes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
